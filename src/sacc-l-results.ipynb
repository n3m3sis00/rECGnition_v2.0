{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# !pip install tensorflow==2.8.0\n!pip install efficientnet\n!pip install --upgrade wandb\n!pip install boto3","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-10-28T09:03:27.993555Z","iopub.execute_input":"2023-10-28T09:03:27.993932Z","iopub.status.idle":"2023-10-28T09:04:16.107585Z","shell.execute_reply.started":"2023-10-28T09:03:27.993898Z","shell.execute_reply":"2023-10-28T09:04:16.106295Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting efficientnet\n  Downloading efficientnet-1.1.1-py3-none-any.whl (18 kB)\nCollecting keras-applications<=1.0.8,>=1.0.7 (from efficientnet)\n  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.7/50.7 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: scikit-image in /opt/conda/lib/python3.10/site-packages (from efficientnet) (0.21.0)\nRequirement already satisfied: numpy>=1.9.1 in /opt/conda/lib/python3.10/site-packages (from keras-applications<=1.0.8,>=1.0.7->efficientnet) (1.23.5)\nRequirement already satisfied: h5py in /opt/conda/lib/python3.10/site-packages (from keras-applications<=1.0.8,>=1.0.7->efficientnet) (3.9.0)\nRequirement already satisfied: scipy>=1.8 in /opt/conda/lib/python3.10/site-packages (from scikit-image->efficientnet) (1.11.2)\nRequirement already satisfied: networkx>=2.8 in /opt/conda/lib/python3.10/site-packages (from scikit-image->efficientnet) (3.1)\nRequirement already satisfied: pillow>=9.0.1 in /opt/conda/lib/python3.10/site-packages (from scikit-image->efficientnet) (9.5.0)\nRequirement already satisfied: imageio>=2.27 in /opt/conda/lib/python3.10/site-packages (from scikit-image->efficientnet) (2.31.1)\nRequirement already satisfied: tifffile>=2022.8.12 in /opt/conda/lib/python3.10/site-packages (from scikit-image->efficientnet) (2023.4.12)\nRequirement already satisfied: PyWavelets>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-image->efficientnet) (1.4.1)\nRequirement already satisfied: packaging>=21 in /opt/conda/lib/python3.10/site-packages (from scikit-image->efficientnet) (21.3)\nRequirement already satisfied: lazy_loader>=0.2 in /opt/conda/lib/python3.10/site-packages (from scikit-image->efficientnet) (0.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=21->scikit-image->efficientnet) (3.0.9)\nInstalling collected packages: keras-applications, efficientnet\nSuccessfully installed efficientnet-1.1.1 keras-applications-1.0.8\nRequirement already satisfied: wandb in /opt/conda/lib/python3.10/site-packages (0.15.9)\nCollecting wandb\n  Downloading wandb-0.15.12-py3-none-any.whl (2.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.1 in /opt/conda/lib/python3.10/site-packages (from wandb) (8.1.7)\nRequirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.1.31)\nRequirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (2.31.0)\nRequirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (5.9.3)\nRequirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (1.30.0)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (0.4.0)\nRequirement already satisfied: PyYAML in /opt/conda/lib/python3.10/site-packages (from wandb) (6.0)\nRequirement already satisfied: pathtools in /opt/conda/lib/python3.10/site-packages (from wandb) (0.1.2)\nRequirement already satisfied: setproctitle in /opt/conda/lib/python3.10/site-packages (from wandb) (1.3.2)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb) (68.0.0)\nRequirement already satisfied: appdirs>=1.4.3 in /opt/conda/lib/python3.10/site-packages (from wandb) (1.4.4)\nRequirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.20.3)\nRequirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.10)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2023.7.22)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.0)\nInstalling collected packages: wandb\n  Attempting uninstall: wandb\n    Found existing installation: wandb 0.15.9\n    Uninstalling wandb-0.15.9:\n      Successfully uninstalled wandb-0.15.9\nSuccessfully installed wandb-0.15.12\nRequirement already satisfied: boto3 in /opt/conda/lib/python3.10/site-packages (1.26.100)\nCollecting botocore<1.30.0,>=1.29.100 (from boto3)\n  Downloading botocore-1.29.165-py3-none-any.whl (11.0 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.0/11.0 MB\u001b[0m \u001b[31m57.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hRequirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from boto3) (1.0.1)\nRequirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from boto3) (0.6.2)\nRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.10/site-packages (from botocore<1.30.0,>=1.29.100->boto3) (2.8.2)\nRequirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/conda/lib/python3.10/site-packages (from botocore<1.30.0,>=1.29.100->boto3) (1.26.15)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.30.0,>=1.29.100->boto3) (1.16.0)\nInstalling collected packages: botocore\n  Attempting uninstall: botocore\n    Found existing installation: botocore 1.31.17\n    Uninstalling botocore-1.31.17:\n      Successfully uninstalled botocore-1.31.17\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\naiobotocore 2.5.4 requires botocore<1.31.18,>=1.31.17, but you have botocore 1.29.165 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed botocore-1.29.165\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport re\nimport random\nimport pandas as pd\nimport numpy as np\nimport json\nimport math\nimport string\nimport uuid\n\n\n### Tensorflow Imports\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import roc_auc_score,confusion_matrix\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras.layers import Input, Conv1D, Add, Activation, Layer, \\\n                        UpSampling1D, Input, DepthwiseConv2D, Conv2D, \\\n                        BatchNormalization, ReLU, AvgPool2D, Flatten, Dense\nfrom tensorflow.keras.applications import MobileNet\n\n\n### External models\nimport efficientnet.tfkeras as efn\n\n\n### Matplotlib Imports\nimport matplotlib\nimport matplotlib.pyplot as plt\n\n\n### import wandb\nimport wandb\nfrom wandb.keras import WandbCallback\n","metadata":{"execution":{"iopub.status.busy":"2023-10-28T09:04:16.109979Z","iopub.execute_input":"2023-10-28T09:04:16.110464Z","iopub.status.idle":"2023-10-28T09:04:27.573038Z","shell.execute_reply.started":"2023-10-28T09:04:16.110415Z","shell.execute_reply":"2023-10-28T09:04:27.571943Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import boto3\nimport os\nfrom botocore import UNSIGNED\nfrom botocore.config import Config\n\n\ndef download_files(bucket_name, s3_prefix, local_directory):\n    s3 = boto3.resource('s3', config=Config(signature_version=UNSIGNED))\n    bucket = s3.Bucket(bucket_name)\n\n    for obj in bucket.objects.filter(Prefix=s3_prefix):\n        local_file = os.path.join(local_directory, obj.key)\n\n        if not os.path.exists(os.path.dirname(local_file)):\n            os.makedirs(os.path.dirname(local_file))\n\n        bucket.download_file(obj.key, local_file)\n        print(f\"Downloaded {obj.key} to {local_file}\")\n\ndownload_files('mitdb128x128', 'train', '/content/input')","metadata":{"execution":{"iopub.status.busy":"2023-10-28T09:04:27.574856Z","iopub.execute_input":"2023-10-28T09:04:27.575395Z","iopub.status.idle":"2023-10-28T09:04:39.279053Z","shell.execute_reply.started":"2023-10-28T09:04:27.575341Z","shell.execute_reply":"2023-10-28T09:04:39.277771Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Downloaded trainfile_class10_fold0_6117.tfrec to /content/input/trainfile_class10_fold0_6117.tfrec\nDownloaded trainfile_class10_fold1_6116.tfrec to /content/input/trainfile_class10_fold1_6116.tfrec\nDownloaded trainfile_class10_fold2_6116.tfrec to /content/input/trainfile_class10_fold2_6116.tfrec\nDownloaded trainfile_class10_fold3_6116.tfrec to /content/input/trainfile_class10_fold3_6116.tfrec\nDownloaded trainfile_class10_fold4_6116.tfrec to /content/input/trainfile_class10_fold4_6116.tfrec\nDownloaded trainfile_class10_fold5_6116.tfrec to /content/input/trainfile_class10_fold5_6116.tfrec\nDownloaded trainfile_class10_fold6_6116.tfrec to /content/input/trainfile_class10_fold6_6116.tfrec\nDownloaded trainfile_class10_fold7_6117.tfrec to /content/input/trainfile_class10_fold7_6117.tfrec\nDownloaded trainfile_class10_fold8_6116.tfrec to /content/input/trainfile_class10_fold8_6116.tfrec\nDownloaded trainfile_class10_fold9_6116.tfrec to /content/input/trainfile_class10_fold9_6116.tfrec\n","output_type":"stream"}]},{"cell_type":"code","source":"!ls /content/input","metadata":{"execution":{"iopub.status.busy":"2023-10-28T09:04:39.281848Z","iopub.execute_input":"2023-10-28T09:04:39.282301Z","iopub.status.idle":"2023-10-28T09:04:40.309176Z","shell.execute_reply.started":"2023-10-28T09:04:39.282262Z","shell.execute_reply":"2023-10-28T09:04:40.307732Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"trainfile_class10_fold0_6117.tfrec  trainfile_class10_fold5_6116.tfrec\ntrainfile_class10_fold1_6116.tfrec  trainfile_class10_fold6_6116.tfrec\ntrainfile_class10_fold2_6116.tfrec  trainfile_class10_fold7_6117.tfrec\ntrainfile_class10_fold3_6116.tfrec  trainfile_class10_fold8_6116.tfrec\ntrainfile_class10_fold4_6116.tfrec  trainfile_class10_fold9_6116.tfrec\n","output_type":"stream"}]},{"cell_type":"code","source":"hparams = {\n    \"backbone\" : \"b0\",\n    \"batch_size\" : 32,\n    \"epochs\" : 40,\n    \"img_size\" : 256,\n    \"lr\" : 0.01,\n    \"optimizer\" : \"adam\",\n    \"seed\": 257,\n    \"notes\": \"MobileNet-base\"\n}","metadata":{"execution":{"iopub.status.busy":"2023-10-28T09:04:40.311019Z","iopub.execute_input":"2023-10-28T09:04:40.312050Z","iopub.status.idle":"2023-10-28T09:04:40.318354Z","shell.execute_reply.started":"2023-10-28T09:04:40.312008Z","shell.execute_reply":"2023-10-28T09:04:40.317110Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"class WandBConfigurations():\n    def __init__(self, exp_name = \"ECG_BEAT_CLASSIFICATION_PAPER_COMMENTS\"):\n        self.EXPERIMENT_NAME = exp_name\n        os.environ[\"WANDB_API_KEY\"] = \"221507f411c2ddcc0c17238e115a12c528a482f6\"\n        wandb.login()\n\nWB = WandBConfigurations()","metadata":{"execution":{"iopub.status.busy":"2023-10-28T09:04:44.064310Z","iopub.execute_input":"2023-10-28T09:04:44.064743Z","iopub.status.idle":"2023-10-28T09:04:47.030621Z","shell.execute_reply.started":"2023-10-28T09:04:44.064710Z","shell.execute_reply":"2023-10-28T09:04:47.029229Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mshreya-srivas02\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"}]},{"cell_type":"code","source":" class Utils():\n    def __init__(self):\n        self.seed_everything()\n\n    def id_generator(size=6):\n        return str(uuid.uuid4())[:size]\n\n    def setupTPU(self):\n\n        try:\n            tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n            print('Running on TPU ', tpu.cluster_spec().as_dict())\n        except ValueError:\n            tpu = None\n\n        if tpu:\n            tf.config.experimental_connect_to_cluster(tpu)\n            tf.tpu.experimental.initialize_tpu_system(tpu)\n            strategy = tf.distribute.TPUStrategy(tpu)\n            STRATEGY = strategy\n            BATCH_SIZE = 8 * strategy.num_replicas_in_sync\n            # wandb.config.hardware = 'TPU'\n        else:\n            strategy = tf.distribute.get_strategy()\n            \n        return strategy\n\n    def seed_everything(self):\n        np.random.seed(hparams['seed'])\n        tf.random.set_seed(hparams['seed'])\n        random.seed(a=hparams['seed'])\n        os.environ['PYTHONHASHSEED'] = str(hparams['seed'])\n\nUTILS = Utils()\nSTRATEGY = UTILS.setupTPU()","metadata":{"execution":{"iopub.status.busy":"2023-10-28T09:04:47.033455Z","iopub.execute_input":"2023-10-28T09:04:47.034975Z","iopub.status.idle":"2023-10-28T09:04:47.060677Z","shell.execute_reply.started":"2023-10-28T09:04:47.034925Z","shell.execute_reply":"2023-10-28T09:04:47.059681Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"STRATEGY","metadata":{"execution":{"iopub.status.busy":"2023-10-28T09:04:47.065221Z","iopub.execute_input":"2023-10-28T09:04:47.067308Z","iopub.status.idle":"2023-10-28T09:04:47.342562Z","shell.execute_reply.started":"2023-10-28T09:04:47.067270Z","shell.execute_reply":"2023-10-28T09:04:47.341582Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy at 0x7cbaba27ada0>"},"metadata":{}}]},{"cell_type":"code","source":"class Config():\n    def __init__(self):\n        self.DO_VAL_SPLIT = True\n        self.TRAIN_FILES = sorted(tf.io.gfile.glob('/content/input/train*.tfrec'))[:-1]\n        self.TOTAL_TRAIN_IMG = 48929\n        self.TOTAL_VAL_IMG = 6116\n        self.TOTAL_TEST_IMG = 6116\n        self.BACKBONE = hparams['backbone']\n        self.IMG_TRAIN_SHAPE = [hparams[\"img_size\"],hparams[\"img_size\"]]\n        self.DO_FINETUNE = True\n        self.BATCH_SIZE = hparams[\"batch_size\"] # 16\n        self.EPOCHES = hparams[\"epochs\"]\n        self.SEED = hparams[\"seed\"]\n        self.LOSS = tf.keras.losses.CategoricalCrossentropy()\n        self.OPTIMIZER = self.get_optimizer()\n        self.ACCURACY = []\n        self.CALLBACKS = []\n        self.STRATEGY = STRATEGY\n        self.FOLDS = 9\n        self.USE_LR_SCHEDULER = True\n        self.FOLD_NUMBER = 0\n        self.FOLDS_DICT = {}\n\n        if self.USE_LR_SCHEDULER:\n            lrfn = self.get_cosine_schedule_with_warmup(lr=hparams['lr'])\n            lr_schedule = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=0)\n            self.CALLBACKS.append(lr_schedule)\n\n    def get_optimizer(self):\n        if hparams['optimizer'] == 'adam':\n            return tf.keras.optimizers.Adam(learning_rate=hparams[\"lr\"])\n        if hparams['optimizer'] == 'rmsprop':\n            return tf.keras.optimizers.RMSprop(learning_rate=hparams[\"lr\"])\n        if hparams['optimizer'] == 'adagrad':\n            return tf.keras.optimizers.Adagrad(learning_rate=hparams[\"lr\"])\n        if hparams['optimizer'] == 'adadelta':\n            return tf.keras.optimizers.Adadelta(learning_rate=hparams[\"lr\"])\n\n        return tf.keras.optimizers.Adam(learning_rate=hparams[\"lr\"])\n\n    def get_cosine_schedule_with_warmup(\n        self,\n        lr = 0.00004,\n        num_warmup_steps = 0,\n        num_cycles=0.5):\n        num_training_steps = self.EPOCHES\n        def lrfn(epoch):\n            if epoch < num_warmup_steps:\n                return (float(epoch) / float(max(5, num_warmup_steps))) * lr\n            progress = float(epoch - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))\n            return max(0.0, 0.5 * (1.0 + math.cos(math.pi * float(num_cycles) * 2.0 * progress))) * lr\n\n        return lrfn\n\n\nCONFIG = Config()","metadata":{"execution":{"iopub.status.busy":"2023-10-28T09:04:47.345032Z","iopub.execute_input":"2023-10-28T09:04:47.345424Z","iopub.status.idle":"2023-10-28T09:04:47.459934Z","shell.execute_reply.started":"2023-10-28T09:04:47.345390Z","shell.execute_reply":"2023-10-28T09:04:47.458809Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"CONFIG.BATCH_SIZE","metadata":{"execution":{"iopub.status.busy":"2023-10-28T09:04:47.461682Z","iopub.execute_input":"2023-10-28T09:04:47.462376Z","iopub.status.idle":"2023-10-28T09:04:47.469220Z","shell.execute_reply.started":"2023-10-28T09:04:47.462333Z","shell.execute_reply":"2023-10-28T09:04:47.467560Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"32"},"metadata":{}}]},{"cell_type":"code","source":"class Data():\n    def __init__(self):\n        self.LABELED_TFREC_FORMAT = {\n            \"image_id\": tf.io.FixedLenFeature([], tf.string),\n            \"image\": tf.io.FixedLenFeature([], tf.string),\n            'target10': tf.io.FixedLenFeature([], tf.int64),\n            'gender' : tf.io.FixedLenFeature([], tf.int64),\n            'age_interval' : tf.io.FixedLenFeature([], tf.int64),\n        }\n\n    def process_training_data(self, data_file):\n        data = tf.io.parse_single_example(data_file, self.LABELED_TFREC_FORMAT)\n        img = tf.image.decode_jpeg(data['image'], channels=1)\n        img = tf.cast(img, tf.float32) / 255.0\n        img = tf.reshape(img, [*CONFIG.IMG_TRAIN_SHAPE, 1])\n\n        age = tf.cast(data['age_interval'], tf.float32) / 10.0\n        sex = tf.cast(data['gender'], tf.float32) / 1.0\n        tab_data = [tf.cast(tfeat, dtype = tf.float32) for tfeat in [age, sex]]\n        tabular_data = tf.stack(tab_data)\n\n        target10 = tf.one_hot(data['target10'], depth=10)\n\n        return {'inp1' : img, 'inp2' : tabular_data}, {\"target10\" : target10 }\n\n    def process_testing_data(self, data_file):\n        data = tf.io.parse_single_example(data_file, self.LABELED_TFREC_FORMAT)\n        img = tf.image.decode_jpeg(data['image'], channels=1)\n        img = tf.cast(img, tf.float32) / 255.0\n        img = tf.reshape(img, [*CONFIG.IMG_TRAIN_SHAPE, 1])\n\n        age = tf.cast(data['age_interval'], tf.float32) / 10.0\n        sex = tf.cast(data['gender'], tf.float32) / 1.0\n        tab_data = [tf.cast(tfeat, dtype = tf.float32) for tfeat in [age, sex]]\n        tabular_data = tf.stack(tab_data)\n\n        target10 = tf.one_hot(data['target10'], depth=10)\n        image_id = data[\"image_id\"]\n\n        return {'inp1' : img, 'inp2' : tabular_data}, {\"target10\" : target10, \"image_id\":  data['image_id']}\n\n    def val_dataset(self):\n        ignore_order = tf.data.Options()\n        val_dataset = (\n            tf.data.TFRecordDataset(\n                CONFIG.FOLDS_DICT[\"fold_{}\".format(CONFIG.FOLD_NUMBER)][\"valfiles\"],\n                num_parallel_reads=tf.data.experimental.AUTOTUNE\n            ).with_options(\n                ignore_order\n            ).map(\n                self.process_training_data,\n                num_parallel_calls=tf.data.experimental.AUTOTUNE\n            ).batch(\n                CONFIG.BATCH_SIZE\n            ).prefetch(\n                tf.data.experimental.AUTOTUNE\n            )\n        )\n\n        return val_dataset\n\n    def train_dataset(self):\n        ignore_order = tf.data.Options()\n        ignore_order.experimental_deterministic = False\n        train_dataset = (\n            tf.data.TFRecordDataset(\n                CONFIG.FOLDS_DICT[\"fold_{}\".format(fold_number)][\"trainfiles\"],\n                num_parallel_reads=tf.data.experimental.AUTOTUNE\n            ).with_options(\n                ignore_order\n            ).map(\n                self.process_training_data,\n                num_parallel_calls=tf.data.experimental.AUTOTUNE\n            ).repeat(\n            ).shuffle(\n                CONFIG.SEED\n            ).batch(\n                CONFIG.BATCH_SIZE\n            ).prefetch(\n                tf.data.experimental.AUTOTUNE\n            )\n        )\n\n        return train_dataset\n\n    def test_dataset(self):\n        ignore_order = tf.data.Options()\n        TEST_FILES = sorted(tf.io.gfile.glob('/content/input/train*.tfrec'))[-1]\n        test_dataset = (\n            tf.data.TFRecordDataset(\n                TEST_FILES,\n                num_parallel_reads=tf.data.experimental.AUTOTUNE\n            ).with_options(\n                ignore_order\n            ).map(\n                self.process_testing_data,\n                num_parallel_calls=tf.data.experimental.AUTOTUNE\n            ).batch(\n                CONFIG.BATCH_SIZE *  4\n            ).prefetch(\n                tf.data.experimental.AUTOTUNE\n            )\n        )\n        return test_dataset\n","metadata":{"execution":{"iopub.status.busy":"2023-10-28T09:04:47.638530Z","iopub.execute_input":"2023-10-28T09:04:47.639004Z","iopub.status.idle":"2023-10-28T09:04:47.659224Z","shell.execute_reply.started":"2023-10-28T09:04:47.638964Z","shell.execute_reply":"2023-10-28T09:04:47.657575Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"class SACCLayer(tf.keras.layers.Layer):\n    def __init__(self, output_dim, **kwargs):\n        self.output_dim = output_dim\n        super(SACCLayer, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        self.kernel_1 = self.add_weight(name='weights_ECG',\n                                        shape=(input_shape[0][-1], self.output_dim),\n                                        initializer='he_normal',\n                                        trainable=True)\n        self.kernel_2 = self.add_weight(name='weights_Patient_Metadata',\n                                        shape=(input_shape[1][-1], self.output_dim),\n                                        initializer='he_normal',\n                                        trainable=True)\n        self.attention_weights1 = self.add_weight(name='attention_weights_ECG',\n                                                 shape=(self.output_dim,),\n                                                 initializer='uniform',\n                                                 trainable=True)\n        self.attention_weights2 = self.add_weight(name='attention_weights_Patient_metadata',\n                                                 shape=(self.output_dim,),\n                                                 initializer='uniform',\n                                                 trainable=True)\n        self.dense_layer = tf.keras.layers.Dense(self.output_dim, activation='relu', name='cca_dense')\n        super(SACCLayer, self).build(input_shape)\n\n    def call(self, inputs):\n        proj_1 = K.dot(inputs[0], self.kernel_1)\n        proj_2 = K.dot(inputs[1], self.kernel_2)\n\n        # Apply non-linear transformation\n        proj_1 = tf.keras.activations.relu(proj_1)\n        proj_2 = tf.keras.activations.relu(proj_2)\n\n        # Attention mechanism\n        attention_scores1 = tf.nn.softmax(self.attention_weights1)\n        attention_scores2 = tf.nn.softmax(self.attention_weights2)\n        proj_1 = attention_scores1 * proj_1\n        proj_2 = attention_scores2 * proj_2\n\n        # Non-linear fusion\n        fused_representation = tf.keras.layers.concatenate([proj_1, proj_2])\n        fused_representation = self.dense_layer(fused_representation)\n\n        return fused_representation\n\n    def compute_output_shape(self, input_shape):\n        return (input_shape[0][0], self.output_dim)\n\n\ndef depthwise_separable_conv_with_residual_block(x, filters, stride):\n    # Depthwise Convolution\n    depthwise = DepthwiseConv2D((3, 3), strides=stride, padding='same')(x)\n    depthwise = BatchNormalization()(depthwise)\n    depthwise = ReLU()(depthwise)\n\n    # Pointwise Convolution\n    pointwise = Conv2D(filters, (1, 1), strides=(1, 1), padding='same')(depthwise)\n    pointwise = BatchNormalization()(pointwise)\n    pointwise = ReLU()(pointwise)\n\n\n    return pointwise\n\n\ndef DualPathwayModel(inp1):\n    # Initial Convolution Layers\n    conv1 = Conv2D(32, (3, 3), strides=(2, 2), padding='same')(inp1)\n    conv1 = BatchNormalization()(conv1)\n    conv1 = ReLU()(conv1)\n\n    conv2 = Conv2D(32, (7, 7), strides=(2, 2), padding='same')(inp1)\n    conv2 = BatchNormalization()(conv2)\n    conv2 = ReLU()(conv2)\n    \n    concatenated = tf.keras.layers.concatenate([conv1, conv2])\n\n    x = depthwise_separable_conv_with_residual_block(concatenated, 64, (1, 1))\n    x = depthwise_separable_conv_with_residual_block(x, 128, (1, 1))\n    x = tf.keras.layers.Dropout(0.2)(x)\n    x = depthwise_separable_conv_with_residual_block(x, 128, (2, 2))\n    x = depthwise_separable_conv_with_residual_block(x, 256, (1, 1))\n    x = tf.keras.layers.Dropout(0.2)(x)\n    x = depthwise_separable_conv_with_residual_block(x, 256, (2, 2))\n    x = depthwise_separable_conv_with_residual_block(x, 256, (4, 4))\n    x = tf.keras.layers.Dropout(0.2)(x)\n    x = depthwise_separable_conv_with_residual_block(x, 512, (1, 1)) \n    x = depthwise_separable_conv_with_residual_block(x, 512, (2, 2))\n    x = tf.keras.layers.Dropout(0.2)(x)\n    x = depthwise_separable_conv_with_residual_block(x, 1024, (1, 1)) \n    x = depthwise_separable_conv_with_residual_block(x, 1024, (2, 2))\n\n    return x\n\n    \ndef dpm_sacc():\n    inp1  = tf.keras.layers.Input(shape = (*CONFIG.IMG_TRAIN_SHAPE, 1), name='inp1')\n    inp2  = tf.keras.layers.Input(shape = (2,), name='inp2')\n    x1 = DualPathwayModel(inp1)\n\n    x1 = tf.keras.layers.GlobalMaxPooling2D()(x1) # AVG is not good\n    x1 = tf.keras.layers.Dropout(0.2)(x1)\n\n    x2 = tf.keras.layers.Dense(8, name='metadata_feature_dense_1', activation='relu')(inp2)\n    x2 = tf.keras.layers.concatenate([x2, inp2])\n\n    x = SACCLayer(output_dim=1024)([x1, x2])\n    \n    x = tf.keras.layers.Dense(512, name='combine_feature_dense_0', activation='relu')(x)\n    x = tf.keras.layers.Dropout(0.1)(x)\n    x = tf.keras.layers.Dense(128, name='combine_feature_dense_1', activation='relu')(x)\n    x = tf.keras.layers.Dense(64, name='combine_feature_dense_2', activation='relu')(x)\n    x = tf.keras.layers.Dropout(0.1)(x)\n\n    output10 = tf.keras.layers.Dense(10, activation='softmax', name='target10')(x)\n\n    model = tf.keras.models.Model(inputs = [inp1, inp2], outputs = [output10])\n\n    return model","metadata":{"execution":{"iopub.status.busy":"2023-10-28T09:09:15.921720Z","iopub.execute_input":"2023-10-28T09:09:15.922160Z","iopub.status.idle":"2023-10-28T09:09:15.946798Z","shell.execute_reply.started":"2023-10-28T09:09:15.922124Z","shell.execute_reply":"2023-10-28T09:09:15.945700Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"dpm_sacc().summary()","metadata":{"execution":{"iopub.status.busy":"2023-10-28T09:09:17.299081Z","iopub.execute_input":"2023-10-28T09:09:17.300232Z","iopub.status.idle":"2023-10-28T09:09:18.991461Z","shell.execute_reply.started":"2023-10-28T09:09:17.300168Z","shell.execute_reply":"2023-10-28T09:09:18.990219Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"Model: \"model_8\"\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to                     \n==================================================================================================\n inp1 (InputLayer)              [(None, 256, 256, 1  0           []                               \n                                )]                                                                \n                                                                                                  \n conv2d_92 (Conv2D)             (None, 128, 128, 32  320         ['inp1[0][0]']                   \n                                )                                                                 \n                                                                                                  \n conv2d_93 (Conv2D)             (None, 128, 128, 32  1600        ['inp1[0][0]']                   \n                                )                                                                 \n                                                                                                  \n batch_normalization_168 (Batch  (None, 128, 128, 32  128        ['conv2d_92[0][0]']              \n Normalization)                 )                                                                 \n                                                                                                  \n batch_normalization_169 (Batch  (None, 128, 128, 32  128        ['conv2d_93[0][0]']              \n Normalization)                 )                                                                 \n                                                                                                  \n re_lu_168 (ReLU)               (None, 128, 128, 32  0           ['batch_normalization_168[0][0]']\n                                )                                                                 \n                                                                                                  \n re_lu_169 (ReLU)               (None, 128, 128, 32  0           ['batch_normalization_169[0][0]']\n                                )                                                                 \n                                                                                                  \n concatenate_16 (Concatenate)   (None, 128, 128, 64  0           ['re_lu_168[0][0]',              \n                                )                                 're_lu_169[0][0]']              \n                                                                                                  \n depthwise_conv2d_76 (Depthwise  (None, 128, 128, 64  640        ['concatenate_16[0][0]']         \n Conv2D)                        )                                                                 \n                                                                                                  \n batch_normalization_170 (Batch  (None, 128, 128, 64  256        ['depthwise_conv2d_76[0][0]']    \n Normalization)                 )                                                                 \n                                                                                                  \n re_lu_170 (ReLU)               (None, 128, 128, 64  0           ['batch_normalization_170[0][0]']\n                                )                                                                 \n                                                                                                  \n conv2d_94 (Conv2D)             (None, 128, 128, 64  4160        ['re_lu_170[0][0]']              \n                                )                                                                 \n                                                                                                  \n batch_normalization_171 (Batch  (None, 128, 128, 64  256        ['conv2d_94[0][0]']              \n Normalization)                 )                                                                 \n                                                                                                  \n re_lu_171 (ReLU)               (None, 128, 128, 64  0           ['batch_normalization_171[0][0]']\n                                )                                                                 \n                                                                                                  \n depthwise_conv2d_77 (Depthwise  (None, 128, 128, 64  640        ['re_lu_171[0][0]']              \n Conv2D)                        )                                                                 \n                                                                                                  \n batch_normalization_172 (Batch  (None, 128, 128, 64  256        ['depthwise_conv2d_77[0][0]']    \n Normalization)                 )                                                                 \n                                                                                                  \n re_lu_172 (ReLU)               (None, 128, 128, 64  0           ['batch_normalization_172[0][0]']\n                                )                                                                 \n                                                                                                  \n conv2d_95 (Conv2D)             (None, 128, 128, 12  8320        ['re_lu_172[0][0]']              \n                                8)                                                                \n                                                                                                  \n batch_normalization_173 (Batch  (None, 128, 128, 12  512        ['conv2d_95[0][0]']              \n Normalization)                 8)                                                                \n                                                                                                  \n re_lu_173 (ReLU)               (None, 128, 128, 12  0           ['batch_normalization_173[0][0]']\n                                8)                                                                \n                                                                                                  \n dropout_52 (Dropout)           (None, 128, 128, 12  0           ['re_lu_173[0][0]']              \n                                8)                                                                \n                                                                                                  \n depthwise_conv2d_78 (Depthwise  (None, 64, 64, 128)  1280       ['dropout_52[0][0]']             \n Conv2D)                                                                                          \n                                                                                                  \n batch_normalization_174 (Batch  (None, 64, 64, 128)  512        ['depthwise_conv2d_78[0][0]']    \n Normalization)                                                                                   \n                                                                                                  \n re_lu_174 (ReLU)               (None, 64, 64, 128)  0           ['batch_normalization_174[0][0]']\n                                                                                                  \n conv2d_96 (Conv2D)             (None, 64, 64, 128)  16512       ['re_lu_174[0][0]']              \n                                                                                                  \n batch_normalization_175 (Batch  (None, 64, 64, 128)  512        ['conv2d_96[0][0]']              \n Normalization)                                                                                   \n                                                                                                  \n re_lu_175 (ReLU)               (None, 64, 64, 128)  0           ['batch_normalization_175[0][0]']\n                                                                                                  \n depthwise_conv2d_79 (Depthwise  (None, 64, 64, 128)  1280       ['re_lu_175[0][0]']              \n Conv2D)                                                                                          \n                                                                                                  \n batch_normalization_176 (Batch  (None, 64, 64, 128)  512        ['depthwise_conv2d_79[0][0]']    \n Normalization)                                                                                   \n                                                                                                  \n re_lu_176 (ReLU)               (None, 64, 64, 128)  0           ['batch_normalization_176[0][0]']\n                                                                                                  \n conv2d_97 (Conv2D)             (None, 64, 64, 256)  33024       ['re_lu_176[0][0]']              \n                                                                                                  \n batch_normalization_177 (Batch  (None, 64, 64, 256)  1024       ['conv2d_97[0][0]']              \n Normalization)                                                                                   \n                                                                                                  \n re_lu_177 (ReLU)               (None, 64, 64, 256)  0           ['batch_normalization_177[0][0]']\n                                                                                                  \n dropout_53 (Dropout)           (None, 64, 64, 256)  0           ['re_lu_177[0][0]']              \n                                                                                                  \n depthwise_conv2d_80 (Depthwise  (None, 32, 32, 256)  2560       ['dropout_53[0][0]']             \n Conv2D)                                                                                          \n                                                                                                  \n batch_normalization_178 (Batch  (None, 32, 32, 256)  1024       ['depthwise_conv2d_80[0][0]']    \n Normalization)                                                                                   \n                                                                                                  \n re_lu_178 (ReLU)               (None, 32, 32, 256)  0           ['batch_normalization_178[0][0]']\n                                                                                                  \n conv2d_98 (Conv2D)             (None, 32, 32, 256)  65792       ['re_lu_178[0][0]']              \n                                                                                                  \n batch_normalization_179 (Batch  (None, 32, 32, 256)  1024       ['conv2d_98[0][0]']              \n Normalization)                                                                                   \n                                                                                                  \n re_lu_179 (ReLU)               (None, 32, 32, 256)  0           ['batch_normalization_179[0][0]']\n                                                                                                  \n depthwise_conv2d_81 (Depthwise  (None, 8, 8, 256)   2560        ['re_lu_179[0][0]']              \n Conv2D)                                                                                          \n                                                                                                  \n batch_normalization_180 (Batch  (None, 8, 8, 256)   1024        ['depthwise_conv2d_81[0][0]']    \n Normalization)                                                                                   \n                                                                                                  \n re_lu_180 (ReLU)               (None, 8, 8, 256)    0           ['batch_normalization_180[0][0]']\n                                                                                                  \n conv2d_99 (Conv2D)             (None, 8, 8, 256)    65792       ['re_lu_180[0][0]']              \n                                                                                                  \n batch_normalization_181 (Batch  (None, 8, 8, 256)   1024        ['conv2d_99[0][0]']              \n Normalization)                                                                                   \n                                                                                                  \n re_lu_181 (ReLU)               (None, 8, 8, 256)    0           ['batch_normalization_181[0][0]']\n                                                                                                  \n dropout_54 (Dropout)           (None, 8, 8, 256)    0           ['re_lu_181[0][0]']              \n                                                                                                  \n depthwise_conv2d_82 (Depthwise  (None, 8, 8, 256)   2560        ['dropout_54[0][0]']             \n Conv2D)                                                                                          \n                                                                                                  \n batch_normalization_182 (Batch  (None, 8, 8, 256)   1024        ['depthwise_conv2d_82[0][0]']    \n Normalization)                                                                                   \n                                                                                                  \n re_lu_182 (ReLU)               (None, 8, 8, 256)    0           ['batch_normalization_182[0][0]']\n                                                                                                  \n conv2d_100 (Conv2D)            (None, 8, 8, 512)    131584      ['re_lu_182[0][0]']              \n                                                                                                  \n batch_normalization_183 (Batch  (None, 8, 8, 512)   2048        ['conv2d_100[0][0]']             \n Normalization)                                                                                   \n                                                                                                  \n re_lu_183 (ReLU)               (None, 8, 8, 512)    0           ['batch_normalization_183[0][0]']\n                                                                                                  \n depthwise_conv2d_83 (Depthwise  (None, 4, 4, 512)   5120        ['re_lu_183[0][0]']              \n Conv2D)                                                                                          \n                                                                                                  \n batch_normalization_184 (Batch  (None, 4, 4, 512)   2048        ['depthwise_conv2d_83[0][0]']    \n Normalization)                                                                                   \n                                                                                                  \n re_lu_184 (ReLU)               (None, 4, 4, 512)    0           ['batch_normalization_184[0][0]']\n                                                                                                  \n conv2d_101 (Conv2D)            (None, 4, 4, 512)    262656      ['re_lu_184[0][0]']              \n                                                                                                  \n batch_normalization_185 (Batch  (None, 4, 4, 512)   2048        ['conv2d_101[0][0]']             \n Normalization)                                                                                   \n                                                                                                  \n re_lu_185 (ReLU)               (None, 4, 4, 512)    0           ['batch_normalization_185[0][0]']\n                                                                                                  \n dropout_55 (Dropout)           (None, 4, 4, 512)    0           ['re_lu_185[0][0]']              \n                                                                                                  \n depthwise_conv2d_84 (Depthwise  (None, 4, 4, 512)   5120        ['dropout_55[0][0]']             \n Conv2D)                                                                                          \n                                                                                                  \n batch_normalization_186 (Batch  (None, 4, 4, 512)   2048        ['depthwise_conv2d_84[0][0]']    \n Normalization)                                                                                   \n                                                                                                  \n re_lu_186 (ReLU)               (None, 4, 4, 512)    0           ['batch_normalization_186[0][0]']\n                                                                                                  \n conv2d_102 (Conv2D)            (None, 4, 4, 1024)   525312      ['re_lu_186[0][0]']              \n                                                                                                  \n batch_normalization_187 (Batch  (None, 4, 4, 1024)  4096        ['conv2d_102[0][0]']             \n Normalization)                                                                                   \n                                                                                                  \n re_lu_187 (ReLU)               (None, 4, 4, 1024)   0           ['batch_normalization_187[0][0]']\n                                                                                                  \n depthwise_conv2d_85 (Depthwise  (None, 2, 2, 1024)  10240       ['re_lu_187[0][0]']              \n Conv2D)                                                                                          \n                                                                                                  \n batch_normalization_188 (Batch  (None, 2, 2, 1024)  4096        ['depthwise_conv2d_85[0][0]']    \n Normalization)                                                                                   \n                                                                                                  \n re_lu_188 (ReLU)               (None, 2, 2, 1024)   0           ['batch_normalization_188[0][0]']\n                                                                                                  \n conv2d_103 (Conv2D)            (None, 2, 2, 1024)   1049600     ['re_lu_188[0][0]']              \n                                                                                                  \n batch_normalization_189 (Batch  (None, 2, 2, 1024)  4096        ['conv2d_103[0][0]']             \n Normalization)                                                                                   \n                                                                                                  \n re_lu_189 (ReLU)               (None, 2, 2, 1024)   0           ['batch_normalization_189[0][0]']\n                                                                                                  \n inp2 (InputLayer)              [(None, 2)]          0           []                               \n                                                                                                  \n global_max_pooling2d_8 (Global  (None, 1024)        0           ['re_lu_189[0][0]']              \n MaxPooling2D)                                                                                    \n                                                                                                  \n metadata_feature_dense_1 (Dens  (None, 8)           24          ['inp2[0][0]']                   \n e)                                                                                               \n                                                                                                  \n dropout_56 (Dropout)           (None, 1024)         0           ['global_max_pooling2d_8[0][0]'] \n                                                                                                  \n concatenate_17 (Concatenate)   (None, 10)           0           ['metadata_feature_dense_1[0][0]'\n                                                                 , 'inp2[0][0]']                  \n                                                                                                  \n sacc_layer_8 (SACCLayer)       (None, 1024)         3159040     ['dropout_56[0][0]',             \n                                                                  'concatenate_17[0][0]']         \n                                                                                                  \n combine_feature_dense_0 (Dense  (None, 512)         524800      ['sacc_layer_8[0][0]']           \n )                                                                                                \n                                                                                                  \n dropout_57 (Dropout)           (None, 512)          0           ['combine_feature_dense_0[0][0]']\n                                                                                                  \n combine_feature_dense_1 (Dense  (None, 128)         65664       ['dropout_57[0][0]']             \n )                                                                                                \n                                                                                                  \n combine_feature_dense_2 (Dense  (None, 64)          8256        ['combine_feature_dense_1[0][0]']\n )                                                                                                \n                                                                                                  \n dropout_58 (Dropout)           (None, 64)           0           ['combine_feature_dense_2[0][0]']\n                                                                                                  \n target10 (Dense)               (None, 10)           650         ['dropout_58[0][0]']             \n                                                                                                  \n==================================================================================================\nTotal params: 5,984,802\nTrainable params: 5,969,954\nNon-trainable params: 14,848\n__________________________________________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"def fitengine(model, traindataset, valdataset = None, istraining = True):\n    model.compile(\n        optimizer   =  CONFIG.OPTIMIZER,\n        loss        =  CONFIG.LOSS,\n        metrics     =  CONFIG.ACCURACY\n    )\n\n    history = model.fit(\n                traindataset,\n                epochs            =   CONFIG.EPOCHES,\n                steps_per_epoch   =   CONFIG.TOTAL_TRAIN_IMG//CONFIG.BATCH_SIZE,\n                callbacks         =   CONFIG.CALLBACKS,\n                validation_data   =   valdataset,\n                validation_steps = (CONFIG.TOTAL_VAL_IMG)//(CONFIG.BATCH_SIZE) + 1,\n                verbose           =   1\n            )\n\n    return history\n\nskf = KFold(n_splits=CONFIG.FOLDS,shuffle=True,random_state=CONFIG.SEED)\nfor fold_number,(idxT,idxV) in enumerate(skf.split(np.arange(len(CONFIG.TRAIN_FILES)))):\n    CONFIG.FOLDS_DICT['fold_{}'.format(fold_number)] = {\n                                            \"trainfiles\" : [CONFIG.TRAIN_FILES[x] for x in idxT],\n                                            \"valfiles\"   : [CONFIG.TRAIN_FILES[x] for x in idxV]\n                                            }\n\nfold_number = CONFIG.FOLD_NUMBER\nprint(CONFIG.FOLDS_DICT['fold_{}'.format(fold_number)]['trainfiles'])\nprint(CONFIG.FOLDS_DICT['fold_{}'.format(fold_number)]['valfiles'])\n\nrun_ = wandb.init(\n    project= WB.EXPERIMENT_NAME,\n    reinit=True,\n    dir = \"/root\",\n    allow_val_change = True,\n    config = hparams\n)\n\nif CONFIG.STRATEGY is not None:\n    with CONFIG.STRATEGY.scope():\n        x2 = tf.keras.metrics.Precision(name='precision')\n        x3 = tf.keras.metrics.BinaryAccuracy(name='accuracy')\n        x4 = tf.keras.metrics.Recall(name='sensitivity')\n\n        CONFIG.ACCURACY.append(x2)\n        CONFIG.ACCURACY.append(x3)\n        CONFIG.ACCURACY.append(x4)\n\n        model = dpm_sacc()\n#         CONFIG.CALLBACKS.append(InLayerLossCallback())\nelse:\n    x2 = tf.keras.metrics.Precision(name='precision')\n    x3 = tf.keras.metrics.BinaryAccuracy(name='accuracy')\n    x4 = tf.keras.metrics.Recall(name='sensitivity')\n    x4 = tf.keras.metrics.Recall(name='sensitivity')\n\n    CONFIG.ACCURACY.append(x2)\n    CONFIG.ACCURACY.append(x3)\n    CONFIG.ACCURACY.append(x4)\n\n    model = dpm_sacc()\n\nCONFIG.CALLBACKS.append(tf.keras.callbacks.ModelCheckpoint(\n                                'model-%s.h5'%(fold_number), monitor='val_loss', verbose=1, save_best_only=True,\n                                save_weights_only=True, mode='min', save_freq='epoch'))\n\nCONFIG.CALLBACKS.append(WandbCallback(save_weights_only=True,\n                                            log_weights=True,\n                                            log_evaluation=True))\n\n\n\n\nDATA = Data()\n\nprint(\"##\"*30)\n\nhistory = fitengine(model, DATA.train_dataset(), valdataset=DATA.val_dataset()) #training model\n\nprint('##'*30)","metadata":{"execution":{"iopub.status.busy":"2023-10-28T08:55:28.700481Z","iopub.execute_input":"2023-10-28T08:55:28.700863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save('model_last_epoch.h5')\nmodel_till_last_epoch = model\nSAVED_MODEL_LOC = \"model-0.h5\"\nmodel = dpm_sacc()\nmodel.load_weights(SAVED_MODEL_LOC)","metadata":{"execution":{"iopub.status.busy":"2023-10-28T08:50:38.609151Z","iopub.status.idle":"2023-10-28T08:50:38.609508Z","shell.execute_reply.started":"2023-10-28T08:50:38.609335Z","shell.execute_reply":"2023-10-28T08:50:38.609352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"NAME = ['/', \"A\",  'F', 'L', 'N', 'R', 'V', 'a', 'f', 'j']\nfor model_type, trained_model in zip(['best_epoch', 'last_epoch'], [model, model_till_last_epoch]):\n    test_imgs = DATA.test_dataset().map(lambda data, ids: data)\n#     img_labels_ds = DATA.test_dataset().map(lambda data, ids: ids).unbatch()\n\n    STEPS = (CONFIG.TOTAL_TEST_IMG)//(CONFIG.BATCH_SIZE*4) + 1\n\n    y_pred = trained_model.predict(test_imgs,steps = int(STEPS), verbose=1)\n    test_labels = next(iter(img_labels_ds.batch(int(CONFIG.TOTAL_TEST_IMG) + 1)))\n    y_true = test_labels[\"target10\"].numpy()\n    pd.DataFrame({\n            'image_id'  : test_labels[\"image_id\"].numpy(),\n            'actual'  : np.argmax(y_true, axis=1),\n            'predicted'      : np.argmax(y_pred, axis=1)\n            }).to_csv('prediction_{}.csv'.format(model_type), index=False)\n\n    df = pd.read_csv(\"prediction_{}.csv\".format(model_type))\n\n    run_.log({f\"{model_type}_pr\": wandb.plot.pr_curve(np.argmax(y_true, axis=1), y_pred, labels=NAME)})\n    run_.log({f\"{model_type}_roc\": wandb.plot.roc_curve(np.argmax(y_true, axis=1), y_pred, labels=NAME)})\n\n    cm = wandb.plot.confusion_matrix(\n                    y_true=np.argmax(y_true, axis=1),\n                    preds=np.argmax(y_pred, axis=1),\n                    class_names=NAME)\n\n    run_.log({f\"{model_type}_conf_mat\": cm})\n\n    harvest = confusion_matrix(df['actual'], df['predicted'])\n    fig, ax = plt.subplots(figsize=(8,8))\n    im = ax.imshow(harvest)\n    ax.set_xticks(np.arange(len(NAME)))\n    ax.set_yticks(np.arange(len(NAME)))\n    ax.set_xticklabels(NAME)\n    ax.set_yticklabels(NAME)\n\n    # Rotate the tick labels and set their alignment.\n    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n            rotation_mode=\"anchor\")\n\n    for i in range(len(NAME)):\n        for j in range(len(NAME)):\n            text = ax.text(j, i, harvest[i, j],\n                        ha=\"center\", va=\"center\", color=\"w\")\n\n    fig.tight_layout()\n    run_.log({f\"{model_type}_cm\": plt})\n\n    from sklearn.metrics import classification_report\n    target_names = NAME\n    x_ = classification_report(df['actual'], df['predicted'], target_names=target_names, digits=4)\n    x2 = classification_report(df['actual'], df['predicted'], target_names=target_names, digits=4, output_dict=True)\n    print(x_)\n    run_.log({f\"{model_type}_CR\": x2})\n\n\n## log more\nartifact = wandb.Artifact(\"Full_Logs\", type=\"logs\")\nartifact.add_dir(\"/kaggle/working/\")\nwandb.log_artifact(artifact)","metadata":{"execution":{"iopub.status.busy":"2023-10-28T08:50:38.611684Z","iopub.status.idle":"2023-10-28T08:50:38.612224Z","shell.execute_reply.started":"2023-10-28T08:50:38.611935Z","shell.execute_reply":"2023-10-28T08:50:38.611961Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.python.framework.convert_to_constants import  convert_variables_to_constants_v2_as_graph\n\n\ndef get_flops(model):\n    concrete = tf.function(lambda inputs: model(inputs))\n    concrete_func = concrete.get_concrete_function(\n        [tf.TensorSpec([1, *inputs.shape[1:]]) for inputs in model.inputs])\n    frozen_func, graph_def = convert_variables_to_constants_v2_as_graph(concrete_func)\n    with tf.Graph().as_default() as graph:\n        tf.graph_util.import_graph_def(graph_def, name='')\n        run_meta = tf.compat.v1.RunMetadata()\n        opts = tf.compat.v1.profiler.ProfileOptionBuilder.float_operation()\n        flops = tf.compat.v1.profiler.profile(graph=graph, run_meta=run_meta, cmd=\"op\", options=opts)\n        return flops.total_float_ops\n\nmodel = dpm_sacc()\n\nprint(\"The FLOPs is:{}\".format(get_flops(model)) ,flush=True )\n\nrun_.finish()","metadata":{"execution":{"iopub.status.busy":"2023-10-28T09:09:28.858661Z","iopub.execute_input":"2023-10-28T09:09:28.859508Z","iopub.status.idle":"2023-10-28T09:09:31.019164Z","shell.execute_reply.started":"2023-10-28T09:09:28.859461Z","shell.execute_reply":"2023-10-28T09:09:31.017502Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"\n=========================Options=============================\n-max_depth                  10000\n-min_bytes                  0\n-min_peak_bytes             0\n-min_residual_bytes         0\n-min_output_bytes           0\n-min_micros                 0\n-min_accelerator_micros     0\n-min_cpu_micros             0\n-min_params                 0\n-min_float_ops              1\n-min_occurrence             0\n-step                       -1\n-order_by                   float_ops\n-account_type_regexes       .*\n-start_name_regexes         .*\n-trim_name_regexes          \n-show_name_regexes          .*\n-hide_name_regexes          \n-account_displayed_op_only  true\n-select                     float_ops\n-output                     stdout:\n\n==================Model Analysis Report======================\n\nDoc:\nop: The nodes are operation kernel type, such as MatMul, Conv2D. Graph nodes belonging to the same type are aggregated together.\nflops: Number of float operations. Note: Please read the implementation for the math behind it.\n\nProfile:\nnode name | # float_ops\nConv2D                   1.06b float_ops (100.00%, 93.03%)\nDepthwiseConv2dNative    62.30m float_ops (6.97%, 5.47%)\nBiasAdd                  9.57m float_ops (1.50%, 0.84%)\nMatMul                   7.51m float_ops (0.66%, 0.66%)\nSoftmax                  10.29k float_ops (0.00%, 0.00%)\nMul                      2.05k float_ops (0.00%, 0.00%)\n\n======================End of Report==========================\nThe FLOPs is:1138453540\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[27], line 21\u001b[0m\n\u001b[1;32m     17\u001b[0m model \u001b[38;5;241m=\u001b[39m dpm_sacc()\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe FLOPs is:\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(get_flops(model)) ,flush\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m )\n\u001b[0;32m---> 21\u001b[0m \u001b[43mrun_\u001b[49m\u001b[38;5;241m.\u001b[39mfinish()\n","\u001b[0;31mNameError\u001b[0m: name 'run_' is not defined"],"ename":"NameError","evalue":"name 'run_' is not defined","output_type":"error"}]},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2023-10-28T08:50:38.616095Z","iopub.status.idle":"2023-10-28T08:50:38.616536Z","shell.execute_reply.started":"2023-10-28T08:50:38.616313Z","shell.execute_reply":"2023-10-28T08:50:38.616333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.keras.utils.plot_model(\n    dpm_sacc(),\n    to_file='model.png',\n    show_shapes=True,\n    show_dtype=False,\n    show_layer_names=False,\n    rankdir='TB',\n    expand_nested=False,\n    dpi=96,\n    layer_range=None,\n    show_layer_activations=True,\n    show_trainable=True\n)","metadata":{"execution":{"iopub.status.busy":"2023-10-28T08:50:38.618106Z","iopub.status.idle":"2023-10-28T08:50:38.618547Z","shell.execute_reply.started":"2023-10-28T08:50:38.618319Z","shell.execute_reply":"2023-10-28T08:50:38.618341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}