{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]}},"nbformat_minor":0,"nbformat":4,"cells":[{"cell_type":"code","source":["!pip install --upgrade wandb"],"metadata":{"execution":{"iopub.status.busy":"2023-11-03T23:17:45.233390Z","iopub.execute_input":"2023-11-03T23:17:45.233747Z","iopub.status.idle":"2023-11-03T23:17:59.454394Z","shell.execute_reply.started":"2023-11-03T23:17:45.233719Z","shell.execute_reply":"2023-11-03T23:17:59.453574Z"},"trusted":true,"id":"XgaqH3DDDkag","outputId":"f64443b5-c763-478e-f783-5b1f69226dca"},"execution_count":null,"outputs":[{"name":"stdout","text":"Requirement already satisfied: wandb in /opt/conda/lib/python3.10/site-packages (0.15.9)\nCollecting wandb\n  Downloading wandb-0.15.12-py3-none-any.whl (2.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.1 in /opt/conda/lib/python3.10/site-packages (from wandb) (8.1.7)\nRequirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.1.31)\nRequirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (2.31.0)\nRequirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (5.9.3)\nRequirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (1.30.0)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (0.4.0)\nRequirement already satisfied: PyYAML in /opt/conda/lib/python3.10/site-packages (from wandb) (6.0)\nRequirement already satisfied: pathtools in /opt/conda/lib/python3.10/site-packages (from wandb) (0.1.2)\nRequirement already satisfied: setproctitle in /opt/conda/lib/python3.10/site-packages (from wandb) (1.3.2)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb) (68.0.0)\nRequirement already satisfied: appdirs>=1.4.3 in /opt/conda/lib/python3.10/site-packages (from wandb) (1.4.4)\nRequirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.20.3)\nRequirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.10)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2023.7.22)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.0)\nInstalling collected packages: wandb\n  Attempting uninstall: wandb\n    Found existing installation: wandb 0.15.9\n    Uninstalling wandb-0.15.9:\n      Successfully uninstalled wandb-0.15.9\nSuccessfully installed wandb-0.15.12\n","output_type":"stream"}]},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow.keras import backend as K\n","import wandb\n","import os\n","\n","os.environ[\"WANDB_API_KEY\"] = \"221507f411c2ddcc0c17238e115a12c528a482f6\"\n","wandb.login()"],"metadata":{"execution":{"iopub.status.busy":"2023-11-03T23:17:59.457689Z","iopub.execute_input":"2023-11-03T23:17:59.458040Z","iopub.status.idle":"2023-11-03T23:18:10.623754Z","shell.execute_reply.started":"2023-11-03T23:17:59.458012Z","shell.execute_reply":"2023-11-03T23:18:10.622769Z"},"trusted":true,"id":"dQqusRTLDkaj","outputId":"7bcd283a-20b6-4239-c9e8-b23cd401e6a7"},"execution_count":null,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mshreya-srivas02\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":["run = wandb.init()"],"metadata":{"execution":{"iopub.status.busy":"2023-11-03T23:18:10.624905Z","iopub.execute_input":"2023-11-03T23:18:10.625564Z","iopub.status.idle":"2023-11-03T23:18:41.646448Z","shell.execute_reply.started":"2023-11-03T23:18:10.625529Z","shell.execute_reply":"2023-11-03T23:18:41.645534Z"},"trusted":true,"id":"dm6WHM_3Dkak","outputId":"b9e7a283-996d-48e0-ed4e-a6bd28458d76"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.15.12"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20231103_231810-wle0wy4u</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/shreya-srivas02/uncategorized/runs/wle0wy4u' target=\"_blank\">hearty-deluge-54</a></strong> to <a href='https://wandb.ai/shreya-srivas02/uncategorized' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/shreya-srivas02/uncategorized' target=\"_blank\">https://wandb.ai/shreya-srivas02/uncategorized</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/shreya-srivas02/uncategorized/runs/wle0wy4u' target=\"_blank\">https://wandb.ai/shreya-srivas02/uncategorized/runs/wle0wy4u</a>"},"metadata":{}}]},{"cell_type":"code","source":["def get_model_memory_usage(batch_size, model):\n","    import numpy as np\n","    try:\n","        from keras import backend as K\n","    except:\n","        from tensorflow.keras import backend as K\n","\n","    shapes_mem_count = 0\n","    internal_model_mem_count = 0\n","    for l in model.layers:\n","        layer_type = l.__class__.__name__\n","        if isinstance(l, tf.keras.Model):\n","            internal_model_mem_count += get_model_memory_usage(batch_size, l)\n","        single_layer_mem = 1\n","        out_shape = l.output_shape\n","        if type(out_shape) is list:\n","            out_shape = out_shape[0]\n","        for s in out_shape:\n","            if s is None:\n","                continue\n","            single_layer_mem *= s\n","        shapes_mem_count += single_layer_mem\n","\n","    trainable_count = np.sum([K.count_params(p) for p in model.trainable_weights])\n","    non_trainable_count = np.sum([K.count_params(p) for p in model.non_trainable_weights])\n","\n","    number_size = 4.0\n","    if K.floatx() == 'float16':\n","        number_size = 2.0\n","    if K.floatx() == 'float64':\n","        number_size = 8.0\n","\n","    total_memory = number_size * (batch_size * shapes_mem_count + trainable_count + non_trainable_count)\n","    gbytes = total_memory / (1024.0 ** 3) + internal_model_mem_count\n","    return gbytes\n","\n","def get_flops(model):\n","    concrete = tf.function(lambda inputs: model(inputs))\n","    concrete_func = concrete.get_concrete_function(\n","        [tf.TensorSpec([1, *inputs.shape[1:]]) for inputs in model.inputs])\n","    frozen_func, graph_def = convert_variables_to_constants_v2_as_graph(concrete_func)\n","    with tf.Graph().as_default() as graph:\n","        tf.graph_util.import_graph_def(graph_def, name='')\n","        run_meta = tf.compat.v1.RunMetadata()\n","        opts = tf.compat.v1.profiler.ProfileOptionBuilder.float_operation()\n","        flops = tf.compat.v1.profiler.profile(graph=graph, run_meta=run_meta, cmd=\"op\", options=opts)\n","        return flops.total_float_ops\n"],"metadata":{"execution":{"iopub.status.busy":"2023-11-03T23:34:23.419289Z","iopub.execute_input":"2023-11-03T23:34:23.420213Z","iopub.status.idle":"2023-11-03T23:34:23.433245Z","shell.execute_reply.started":"2023-11-03T23:34:23.420181Z","shell.execute_reply":"2023-11-03T23:34:23.431808Z"},"trusted":true,"id":"SMeO1ee4Dkak"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"gQfCGqO_Dkal"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## ResNet50"],"metadata":{"id":"loRWZ4x9Dkal"}},{"cell_type":"code","source":["from tensorflow.python.framework.convert_to_constants import  convert_variables_to_constants_v2_as_graph\n","\n","artifact = run.use_artifact('shreya-srivas02/ECG_BEAT_CLASSIFICATION_PAPER_COMMENTS/model-lively-armadillo-164:v6', type='model')\n","artifact_dir = artifact.download()\n","\n","model = tf.keras.models.load_model(\"/kaggle/working/artifacts/model-lively-armadillo-164:v6\")\n","\n","print(\"The FLOPs is:{}\".format(get_flops(model)) ,flush=True )\n","print(\"Memory Training 32 batch size is:{}\".format(get_model_memory_usage(32, model)) ,flush=True )\n","print(\"Memory Training 1 image is:{}\".format(get_model_memory_usage(1, model)) ,flush=True )\n","model.summary()"],"metadata":{"execution":{"iopub.status.busy":"2023-11-03T23:34:27.865337Z","iopub.execute_input":"2023-11-03T23:34:27.865678Z","iopub.status.idle":"2023-11-03T23:34:40.972027Z","shell.execute_reply.started":"2023-11-03T23:34:27.865655Z","shell.execute_reply":"2023-11-03T23:34:40.970554Z"},"trusted":true,"id":"n79U4vWGDkam","outputId":"16ab835e-c4da-481c-b2f9-7fac8ba671f2"},"execution_count":null,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m:   5 of 5 files downloaded.  \n","output_type":"stream"},{"name":"stdout","text":"\n=========================Options=============================\n-max_depth                  10000\n-min_bytes                  0\n-min_peak_bytes             0\n-min_residual_bytes         0\n-min_output_bytes           0\n-min_micros                 0\n-min_accelerator_micros     0\n-min_cpu_micros             0\n-min_params                 0\n-min_float_ops              1\n-min_occurrence             0\n-step                       -1\n-order_by                   float_ops\n-account_type_regexes       .*\n-start_name_regexes         .*\n-trim_name_regexes          \n-show_name_regexes          .*\n-hide_name_regexes          \n-account_displayed_op_only  true\n-select                     float_ops\n-output                     stdout:\n\n==================Model Analysis Report======================\n\nDoc:\nop: The nodes are operation kernel type, such as MatMul, Conv2D. Graph nodes belonging to the same type are aggregated together.\nflops: Number of float operations. Note: Please read the implementation for the math behind it.\n\nProfile:\nnode name | # float_ops\nConv2D                   2.47b float_ops (100.00%, 99.83%)\nBiasAdd                  3.46m float_ops (0.17%, 0.14%)\nMaxPool                  589.82k float_ops (0.03%, 0.02%)\nMatMul                   40.96k float_ops (0.00%, 0.00%)\nMean                     32.77k float_ops (0.00%, 0.00%)\nSoftmax                     50 float_ops (0.00%, 0.00%)\n\n======================End of Report==========================\nThe FLOPs is:2470895676\nMemory Training 32 batch size is:1.6195946112275124\nMemory Training 1 image is:0.22089121490716934\nModel: \"model_1\"\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to                     \n==================================================================================================\n inp1 (InputLayer)              [(None, 128, 128, 1  0           []                               \n                                )]                                                                \n                                                                                                  \n resnet50 (Functional)          (None, 4, 4, 2048)   23581440    ['inp1[0][0]']                   \n                                                                                                  \n global_average_pooling2d_1 (Gl  (None, 2048)        0           ['resnet50[0][0]']               \n obalAveragePooling2D)                                                                            \n                                                                                                  \n inp2 (InputLayer)              [(None, 2)]          0           []                               \n                                                                                                  \n target10 (Dense)               (None, 10)           20490       ['global_average_pooling2d_1[0][0\n                                                                 ]']                              \n                                                                                                  \n==================================================================================================\nTotal params: 23,601,930\nTrainable params: 23,548,810\nNon-trainable params: 53,120\n__________________________________________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"markdown","source":["## SACC-s"],"metadata":{"id":"aRldY5iQDkam"}},{"cell_type":"code","source":["artifact = run.use_artifact('shreya-srivas02/ECG_BEAT_CLASSIFICATION_PAPER_COMMENTS/model-worthy-feather-190:v0', type='model')\n","artifact_dir = artifact.download()\n","\n","model = tf.keras.models.load_model(\"/kaggle/working/artifacts/model-worthy-feather-190:v0\")\n","\n","print(\"The FLOPs is:{}\".format(get_flops(model)) ,flush=True )\n","print(\"Memory Training 32 batch size is:{}\".format(get_model_memory_usage(32, model)) ,flush=True )\n","print(\"Memory Training 1 image is:{}\".format(get_model_memory_usage(1, model)) ,flush=True )\n","model.summary()"],"metadata":{"execution":{"iopub.status.busy":"2023-11-03T23:35:34.579245Z","iopub.execute_input":"2023-11-03T23:35:34.579624Z","iopub.status.idle":"2023-11-03T23:35:38.315271Z","shell.execute_reply.started":"2023-11-03T23:35:34.579599Z","shell.execute_reply":"2023-11-03T23:35:38.314144Z"},"trusted":true,"id":"xEGQ12MVDkam","outputId":"a10bae7a-4ccf-4dc5-d651-33c72fe2488c"},"execution_count":null,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m:   5 of 5 files downloaded.  \n","output_type":"stream"},{"name":"stdout","text":"\n=========================Options=============================\n-max_depth                  10000\n-min_bytes                  0\n-min_peak_bytes             0\n-min_residual_bytes         0\n-min_output_bytes           0\n-min_micros                 0\n-min_accelerator_micros     0\n-min_cpu_micros             0\n-min_params                 0\n-min_float_ops              1\n-min_occurrence             0\n-step                       -1\n-order_by                   float_ops\n-account_type_regexes       .*\n-start_name_regexes         .*\n-trim_name_regexes          \n-show_name_regexes          .*\n-hide_name_regexes          \n-account_displayed_op_only  true\n-select                     float_ops\n-output                     stdout:\n\n==================Model Analysis Report======================\n\nDoc:\nop: The nodes are operation kernel type, such as MatMul, Conv2D. Graph nodes belonging to the same type are aggregated together.\nflops: Number of float operations. Note: Please read the implementation for the math behind it.\n\nProfile:\nnode name | # float_ops\nConv2D                   74.45m float_ops (100.00%, 89.97%)\nDepthwiseConv2dNative    6.73m float_ops (10.03%, 8.13%)\nBiasAdd                  1.08m float_ops (1.90%, 1.31%)\nMatMul                   481.57k float_ops (0.59%, 0.58%)\nSoftmax                  2.61k float_ops (0.00%, 0.00%)\nMul                        512 float_ops (0.00%, 0.00%)\n\n======================End of Report==========================\nThe FLOPs is:82745124\nMemory Training 32 batch size is:0.44004810601472855\nMemory Training 1 image is:0.015388727188110352\nModel: \"model_43\"\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to                     \n==================================================================================================\n inp1 (InputLayer)              [(None, 128, 128, 1  0           []                               \n                                )]                                                                \n                                                                                                  \n conv2d_354 (Conv2D)            (None, 64, 64, 32)   320         ['inp1[0][0]']                   \n                                                                                                  \n conv2d_355 (Conv2D)            (None, 64, 64, 32)   832         ['inp1[0][0]']                   \n                                                                                                  \n batch_normalization_618 (Batch  (None, 64, 64, 32)  128         ['conv2d_354[0][0]']             \n Normalization)                                                                                   \n                                                                                                  \n batch_normalization_619 (Batch  (None, 64, 64, 32)  128         ['conv2d_355[0][0]']             \n Normalization)                                                                                   \n                                                                                                  \n re_lu_618 (ReLU)               (None, 64, 64, 32)   0           ['batch_normalization_618[0][0]']\n                                                                                                  \n re_lu_619 (ReLU)               (None, 64, 64, 32)   0           ['batch_normalization_619[0][0]']\n                                                                                                  \n concatenate_89 (Concatenate)   (None, 64, 64, 64)   0           ['re_lu_618[0][0]',              \n                                                                  're_lu_619[0][0]']              \n                                                                                                  \n depthwise_conv2d_264 (Depthwis  (None, 64, 64, 64)  640         ['concatenate_89[0][0]']         \n eConv2D)                                                                                         \n                                                                                                  \n batch_normalization_620 (Batch  (None, 64, 64, 64)  256         ['depthwise_conv2d_264[0][0]']   \n Normalization)                                                                                   \n                                                                                                  \n re_lu_620 (ReLU)               (None, 64, 64, 64)   0           ['batch_normalization_620[0][0]']\n                                                                                                  \n conv2d_356 (Conv2D)            (None, 64, 64, 64)   4160        ['re_lu_620[0][0]']              \n                                                                                                  \n batch_normalization_621 (Batch  (None, 64, 64, 64)  256         ['conv2d_356[0][0]']             \n Normalization)                                                                                   \n                                                                                                  \n re_lu_621 (ReLU)               (None, 64, 64, 64)   0           ['batch_normalization_621[0][0]']\n                                                                                                  \n depthwise_conv2d_265 (Depthwis  (None, 32, 32, 64)  640         ['re_lu_621[0][0]']              \n eConv2D)                                                                                         \n                                                                                                  \n batch_normalization_622 (Batch  (None, 32, 32, 64)  256         ['depthwise_conv2d_265[0][0]']   \n Normalization)                                                                                   \n                                                                                                  \n re_lu_622 (ReLU)               (None, 32, 32, 64)   0           ['batch_normalization_622[0][0]']\n                                                                                                  \n conv2d_357 (Conv2D)            (None, 32, 32, 128)  8320        ['re_lu_622[0][0]']              \n                                                                                                  \n batch_normalization_623 (Batch  (None, 32, 32, 128)  512        ['conv2d_357[0][0]']             \n Normalization)                                                                                   \n                                                                                                  \n re_lu_623 (ReLU)               (None, 32, 32, 128)  0           ['batch_normalization_623[0][0]']\n                                                                                                  \n dropout_175 (Dropout)          (None, 32, 32, 128)  0           ['re_lu_623[0][0]']              \n                                                                                                  \n depthwise_conv2d_266 (Depthwis  (None, 16, 16, 128)  1280       ['dropout_175[0][0]']            \n eConv2D)                                                                                         \n                                                                                                  \n batch_normalization_624 (Batch  (None, 16, 16, 128)  512        ['depthwise_conv2d_266[0][0]']   \n Normalization)                                                                                   \n                                                                                                  \n re_lu_624 (ReLU)               (None, 16, 16, 128)  0           ['batch_normalization_624[0][0]']\n                                                                                                  \n conv2d_358 (Conv2D)            (None, 16, 16, 128)  16512       ['re_lu_624[0][0]']              \n                                                                                                  \n batch_normalization_625 (Batch  (None, 16, 16, 128)  512        ['conv2d_358[0][0]']             \n Normalization)                                                                                   \n                                                                                                  \n re_lu_625 (ReLU)               (None, 16, 16, 128)  0           ['batch_normalization_625[0][0]']\n                                                                                                  \n depthwise_conv2d_267 (Depthwis  (None, 8, 8, 128)   1280        ['re_lu_625[0][0]']              \n eConv2D)                                                                                         \n                                                                                                  \n batch_normalization_626 (Batch  (None, 8, 8, 128)   512         ['depthwise_conv2d_267[0][0]']   \n Normalization)                                                                                   \n                                                                                                  \n re_lu_626 (ReLU)               (None, 8, 8, 128)    0           ['batch_normalization_626[0][0]']\n                                                                                                  \n conv2d_359 (Conv2D)            (None, 8, 8, 256)    33024       ['re_lu_626[0][0]']              \n                                                                                                  \n batch_normalization_627 (Batch  (None, 8, 8, 256)   1024        ['conv2d_359[0][0]']             \n Normalization)                                                                                   \n                                                                                                  \n re_lu_627 (ReLU)               (None, 8, 8, 256)    0           ['batch_normalization_627[0][0]']\n                                                                                                  \n dropout_176 (Dropout)          (None, 8, 8, 256)    0           ['re_lu_627[0][0]']              \n                                                                                                  \n depthwise_conv2d_268 (Depthwis  (None, 4, 4, 256)   2560        ['dropout_176[0][0]']            \n eConv2D)                                                                                         \n                                                                                                  \n batch_normalization_628 (Batch  (None, 4, 4, 256)   1024        ['depthwise_conv2d_268[0][0]']   \n Normalization)                                                                                   \n                                                                                                  \n re_lu_628 (ReLU)               (None, 4, 4, 256)    0           ['batch_normalization_628[0][0]']\n                                                                                                  \n conv2d_360 (Conv2D)            (None, 4, 4, 256)    65792       ['re_lu_628[0][0]']              \n                                                                                                  \n batch_normalization_629 (Batch  (None, 4, 4, 256)   1024        ['conv2d_360[0][0]']             \n Normalization)                                                                                   \n                                                                                                  \n re_lu_629 (ReLU)               (None, 4, 4, 256)    0           ['batch_normalization_629[0][0]']\n                                                                                                  \n depthwise_conv2d_269 (Depthwis  (None, 2, 2, 256)   2560        ['re_lu_629[0][0]']              \n eConv2D)                                                                                         \n                                                                                                  \n batch_normalization_630 (Batch  (None, 2, 2, 256)   1024        ['depthwise_conv2d_269[0][0]']   \n Normalization)                                                                                   \n                                                                                                  \n re_lu_630 (ReLU)               (None, 2, 2, 256)    0           ['batch_normalization_630[0][0]']\n                                                                                                  \n conv2d_361 (Conv2D)            (None, 2, 2, 256)    65792       ['re_lu_630[0][0]']              \n                                                                                                  \n batch_normalization_631 (Batch  (None, 2, 2, 256)   1024        ['conv2d_361[0][0]']             \n Normalization)                                                                                   \n                                                                                                  \n re_lu_631 (ReLU)               (None, 2, 2, 256)    0           ['batch_normalization_631[0][0]']\n                                                                                                  \n inp2 (InputLayer)              [(None, 2)]          0           []                               \n                                                                                                  \n global_max_pooling2d_43 (Globa  (None, 256)         0           ['re_lu_631[0][0]']              \n lMaxPooling2D)                                                                                   \n                                                                                                  \n metadata_feature_dense_1 (Dens  (None, 8)           24          ['inp2[0][0]']                   \n e)                                                                                               \n                                                                                                  \n dropout_177 (Dropout)          (None, 256)          0           ['global_max_pooling2d_43[0][0]']\n                                                                                                  \n concatenate_90 (Concatenate)   (None, 10)           0           ['metadata_feature_dense_1[0][0]'\n                                                                 , 'inp2[0][0]']                  \n                                                                                                  \n sacc_layer_43 (SACCLayer)      (None, 256)          199936      ['dropout_177[0][0]',            \n                                                                  'concatenate_90[0][0]']         \n                                                                                                  \n combine_feature_dense_1 (Dense  (None, 128)         32896       ['sacc_layer_43[0][0]']          \n )                                                                                                \n                                                                                                  \n combine_feature_dense_2 (Dense  (None, 64)          8256        ['combine_feature_dense_1[0][0]']\n )                                                                                                \n                                                                                                  \n dropout_178 (Dropout)          (None, 64)           0           ['combine_feature_dense_2[0][0]']\n                                                                                                  \n target10 (Dense)               (None, 10)           650         ['dropout_178[0][0]']            \n                                                                                                  \n==================================================================================================\nTotal params: 453,666\nTrainable params: 449,570\nNon-trainable params: 4,096\n__________________________________________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"markdown","source":["## EfficientNetB0"],"metadata":{"id":"h4CsS3ixDkan"}},{"cell_type":"code","source":["artifact = run.use_artifact('shreya-srivas02/ECG_BEAT_CLASSIFICATION_PAPER_COMMENTS/model-absurd-donkey-159:v0', type='model')\n","artifact_dir = artifact.download()\n","\n","model = tf.keras.models.load_model(\"/kaggle/working/artifacts/model-absurd-donkey-159:v0\")\n","\n","print(\"The FLOPs is:{}\".format(get_flops(model)) ,flush=True )\n","print(\"Memory Training 32 batch size is:{}\".format(get_model_memory_usage(32, model)) ,flush=True )\n","print(\"Memory Training 1 image is:{}\".format(get_model_memory_usage(1, model)) ,flush=True )\n","model.summary()"],"metadata":{"execution":{"iopub.status.busy":"2023-11-03T23:34:44.271401Z","iopub.execute_input":"2023-11-03T23:34:44.271657Z","iopub.status.idle":"2023-11-03T23:34:59.841599Z","shell.execute_reply.started":"2023-11-03T23:34:44.271635Z","shell.execute_reply":"2023-11-03T23:34:59.840222Z"},"trusted":true,"id":"Y5pq1xW2Dkan","outputId":"f8c8c21f-d257-4105-c55c-0da0d729354a"},"execution_count":null,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m:   5 of 5 files downloaded.  \n","output_type":"stream"},{"name":"stdout","text":"\n=========================Options=============================\n-max_depth                  10000\n-min_bytes                  0\n-min_peak_bytes             0\n-min_residual_bytes         0\n-min_output_bytes           0\n-min_micros                 0\n-min_accelerator_micros     0\n-min_cpu_micros             0\n-min_params                 0\n-min_float_ops              1\n-min_occurrence             0\n-step                       -1\n-order_by                   float_ops\n-account_type_regexes       .*\n-start_name_regexes         .*\n-trim_name_regexes          \n-show_name_regexes          .*\n-hide_name_regexes          \n-account_displayed_op_only  true\n-select                     float_ops\n-output                     stdout:\n\n==================Model Analysis Report======================\n\nDoc:\nop: The nodes are operation kernel type, such as MatMul, Conv2D. Graph nodes belonging to the same type are aggregated together.\nflops: Number of float operations. Note: Please read the implementation for the math behind it.\n\nProfile:\nnode name | # float_ops\nConv2D                   224.70m float_ops (100.00%, 88.87%)\nDepthwiseConv2dNative    22.55m float_ops (11.13%, 8.92%)\nMul                      4.78m float_ops (2.21%, 1.89%)\nMean                     773.63k float_ops (0.32%, 0.31%)\nMatMul                   25.60k float_ops (0.01%, 0.01%)\nBiasAdd                  9.35k float_ops (0.00%, 0.00%)\nSoftmax                     50 float_ops (0.00%, 0.00%)\n\n======================End of Report==========================\nThe FLOPs is:252834736\nMemory Training 32 batch size is:0.9103276804089546\nMemory Training 1 image is:0.05771855264902115\nModel: \"model_1\"\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to                     \n==================================================================================================\n inp1 (InputLayer)              [(None, 128, 128, 1  0           []                               \n                                )]                                                                \n                                                                                                  \n efficientnet-b0 (Functional)   (None, 4, 4, 1280)   4048988     ['inp1[0][0]']                   \n                                                                                                  \n global_average_pooling2d_1 (Gl  (None, 1280)        0           ['efficientnet-b0[0][0]']        \n obalAveragePooling2D)                                                                            \n                                                                                                  \n inp2 (InputLayer)              [(None, 2)]          0           []                               \n                                                                                                  \n target10 (Dense)               (None, 10)           12810       ['global_average_pooling2d_1[0][0\n                                                                 ]']                              \n                                                                                                  \n==================================================================================================\nTotal params: 4,061,798\nTrainable params: 4,019,782\nNon-trainable params: 42,016\n__________________________________________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"markdown","source":["## MobileNet"],"metadata":{"id":"oInfDrodDkao"}},{"cell_type":"code","source":["artifact = run.use_artifact('shreya-srivas02/ECG_BEAT_CLASSIFICATION_PAPER_COMMENTS/model-logical-donkey-163:v0', type='model')\n","artifact_dir = artifact.download()\n","\n","model = tf.keras.models.load_model(\"/kaggle/working/artifacts/model-logical-donkey-163:v0\")\n","\n","print(\"The FLOPs is:{}\".format(get_flops(model)) ,flush=True )\n","print(\"Memory Training 32 batch size is:{}\".format(get_model_memory_usage(32, model)) ,flush=True )\n","print(\"Memory Training 1 image is:{}\".format(get_model_memory_usage(1, model)) ,flush=True )\n","model.summary()"],"metadata":{"execution":{"iopub.status.busy":"2023-11-03T23:35:04.563072Z","iopub.execute_input":"2023-11-03T23:35:04.563307Z","iopub.status.idle":"2023-11-03T23:35:11.151549Z","shell.execute_reply.started":"2023-11-03T23:35:04.563285Z","shell.execute_reply":"2023-11-03T23:35:11.150264Z"},"trusted":true,"id":"7g_xHxyIDkao","outputId":"f18858e0-2839-4dc7-f35b-feb49c41883a"},"execution_count":null,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m:   5 of 5 files downloaded.  \n","output_type":"stream"},{"name":"stdout","text":"\n=========================Options=============================\n-max_depth                  10000\n-min_bytes                  0\n-min_peak_bytes             0\n-min_residual_bytes         0\n-min_output_bytes           0\n-min_micros                 0\n-min_accelerator_micros     0\n-min_cpu_micros             0\n-min_params                 0\n-min_float_ops              1\n-min_occurrence             0\n-step                       -1\n-order_by                   float_ops\n-account_type_regexes       .*\n-start_name_regexes         .*\n-trim_name_regexes          \n-show_name_regexes          .*\n-hide_name_regexes          \n-account_displayed_op_only  true\n-select                     float_ops\n-output                     stdout:\n\n==================Model Analysis Report======================\n\nDoc:\nop: The nodes are operation kernel type, such as MatMul, Conv2D. Graph nodes belonging to the same type are aggregated together.\nflops: Number of float operations. Note: Please read the implementation for the math behind it.\n\nProfile:\nnode name | # float_ops\nConv2D                   354.68m float_ops (100.00%, 96.89%)\nDepthwiseConv2dNative    11.35m float_ops (3.11%, 3.10%)\nMatMul                   20.48k float_ops (0.01%, 0.01%)\nMean                     16.38k float_ops (0.00%, 0.00%)\nSoftmax                     50 float_ops (0.00%, 0.00%)\nBiasAdd                     10 float_ops (0.00%, 0.00%)\n\n======================End of Report==========================\nThe FLOPs is:366071868\nMemory Training 32 batch size is:0.6815553084015846\nMemory Training 1 image is:0.044636569917201996\nModel: \"model_1\"\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to                     \n==================================================================================================\n inp1 (InputLayer)              [(None, 128, 128, 1  0           []                               \n                                )]                                                                \n                                                                                                  \n mobilenet_1.00_128 (Functional  (None, 4, 4, 1024)  3228288     ['inp1[0][0]']                   \n )                                                                                                \n                                                                                                  \n global_average_pooling2d_1 (Gl  (None, 1024)        0           ['mobilenet_1.00_128[0][0]']     \n obalAveragePooling2D)                                                                            \n                                                                                                  \n inp2 (InputLayer)              [(None, 2)]          0           []                               \n                                                                                                  \n target10 (Dense)               (None, 10)           10250       ['global_average_pooling2d_1[0][0\n                                                                 ]']                              \n                                                                                                  \n==================================================================================================\nTotal params: 3,238,538\nTrainable params: 3,216,650\nNon-trainable params: 21,888\n__________________________________________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":[],"metadata":{"id":"wGdnIhg6Dkao"},"execution_count":null,"outputs":[]}]}