{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# !pip install tensorflow==2.8.0\n!pip install efficientnet\n!pip install --upgrade wandb\n!pip install boto3","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-10-28T16:50:44.939586Z","iopub.execute_input":"2023-10-28T16:50:44.939838Z","iopub.status.idle":"2023-10-28T16:51:28.122444Z","shell.execute_reply.started":"2023-10-28T16:50:44.939814Z","shell.execute_reply":"2023-10-28T16:51:28.121328Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting efficientnet\n  Downloading efficientnet-1.1.1-py3-none-any.whl (18 kB)\nCollecting keras-applications<=1.0.8,>=1.0.7 (from efficientnet)\n  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.7/50.7 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: scikit-image in /opt/conda/lib/python3.10/site-packages (from efficientnet) (0.21.0)\nRequirement already satisfied: numpy>=1.9.1 in /opt/conda/lib/python3.10/site-packages (from keras-applications<=1.0.8,>=1.0.7->efficientnet) (1.23.5)\nRequirement already satisfied: h5py in /opt/conda/lib/python3.10/site-packages (from keras-applications<=1.0.8,>=1.0.7->efficientnet) (3.9.0)\nRequirement already satisfied: scipy>=1.8 in /opt/conda/lib/python3.10/site-packages (from scikit-image->efficientnet) (1.11.2)\nRequirement already satisfied: networkx>=2.8 in /opt/conda/lib/python3.10/site-packages (from scikit-image->efficientnet) (3.1)\nRequirement already satisfied: pillow>=9.0.1 in /opt/conda/lib/python3.10/site-packages (from scikit-image->efficientnet) (9.5.0)\nRequirement already satisfied: imageio>=2.27 in /opt/conda/lib/python3.10/site-packages (from scikit-image->efficientnet) (2.31.1)\nRequirement already satisfied: tifffile>=2022.8.12 in /opt/conda/lib/python3.10/site-packages (from scikit-image->efficientnet) (2023.4.12)\nRequirement already satisfied: PyWavelets>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-image->efficientnet) (1.4.1)\nRequirement already satisfied: packaging>=21 in /opt/conda/lib/python3.10/site-packages (from scikit-image->efficientnet) (21.3)\nRequirement already satisfied: lazy_loader>=0.2 in /opt/conda/lib/python3.10/site-packages (from scikit-image->efficientnet) (0.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=21->scikit-image->efficientnet) (3.0.9)\nInstalling collected packages: keras-applications, efficientnet\nSuccessfully installed efficientnet-1.1.1 keras-applications-1.0.8\nRequirement already satisfied: wandb in /opt/conda/lib/python3.10/site-packages (0.15.9)\nCollecting wandb\n  Downloading wandb-0.15.12-py3-none-any.whl (2.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.1 in /opt/conda/lib/python3.10/site-packages (from wandb) (8.1.7)\nRequirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.1.31)\nRequirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (2.31.0)\nRequirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (5.9.3)\nRequirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (1.30.0)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (0.4.0)\nRequirement already satisfied: PyYAML in /opt/conda/lib/python3.10/site-packages (from wandb) (6.0)\nRequirement already satisfied: pathtools in /opt/conda/lib/python3.10/site-packages (from wandb) (0.1.2)\nRequirement already satisfied: setproctitle in /opt/conda/lib/python3.10/site-packages (from wandb) (1.3.2)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb) (68.0.0)\nRequirement already satisfied: appdirs>=1.4.3 in /opt/conda/lib/python3.10/site-packages (from wandb) (1.4.4)\nRequirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.20.3)\nRequirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.10)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2023.7.22)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.0)\nInstalling collected packages: wandb\n  Attempting uninstall: wandb\n    Found existing installation: wandb 0.15.9\n    Uninstalling wandb-0.15.9:\n      Successfully uninstalled wandb-0.15.9\nSuccessfully installed wandb-0.15.12\nRequirement already satisfied: boto3 in /opt/conda/lib/python3.10/site-packages (1.26.100)\nCollecting botocore<1.30.0,>=1.29.100 (from boto3)\n  Downloading botocore-1.29.165-py3-none-any.whl (11.0 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.0/11.0 MB\u001b[0m \u001b[31m74.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hRequirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from boto3) (1.0.1)\nRequirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from boto3) (0.6.2)\nRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.10/site-packages (from botocore<1.30.0,>=1.29.100->boto3) (2.8.2)\nRequirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/conda/lib/python3.10/site-packages (from botocore<1.30.0,>=1.29.100->boto3) (1.26.15)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.30.0,>=1.29.100->boto3) (1.16.0)\nInstalling collected packages: botocore\n  Attempting uninstall: botocore\n    Found existing installation: botocore 1.31.17\n    Uninstalling botocore-1.31.17:\n      Successfully uninstalled botocore-1.31.17\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\naiobotocore 2.5.4 requires botocore<1.31.18,>=1.31.17, but you have botocore 1.29.165 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed botocore-1.29.165\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport re\nimport random\nimport pandas as pd\nimport numpy as np\nimport json\nimport math\nimport string\nimport uuid\n\n\n### Tensorflow Imports\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import roc_auc_score,confusion_matrix\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras.layers import Input, Conv1D, Add, Activation, Layer, \\\n                        UpSampling1D, Input, DepthwiseConv2D, Conv2D, \\\n                        BatchNormalization, ReLU, AvgPool2D, Flatten, Dense\nfrom tensorflow.keras.applications import MobileNet\n\n\n### External models\nimport efficientnet.tfkeras as efn\n\n\n### Matplotlib Imports\nimport matplotlib\nimport matplotlib.pyplot as plt\n\n\n### import wandb\nimport wandb\nfrom wandb.keras import WandbCallback\n","metadata":{"execution":{"iopub.status.busy":"2023-10-28T16:56:25.363339Z","iopub.execute_input":"2023-10-28T16:56:25.363680Z","iopub.status.idle":"2023-10-28T16:56:34.529355Z","shell.execute_reply.started":"2023-10-28T16:56:25.363650Z","shell.execute_reply":"2023-10-28T16:56:34.528391Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"import boto3\nimport os\nfrom botocore import UNSIGNED\nfrom botocore.config import Config\n\n\ndef download_files(bucket_name, s3_prefix, local_directory):\n    s3 = boto3.resource('s3', config=Config(signature_version=UNSIGNED))\n    bucket = s3.Bucket(bucket_name)\n\n    for obj in bucket.objects.filter(Prefix=s3_prefix):\n        local_file = os.path.join(local_directory, obj.key)\n\n        if not os.path.exists(os.path.dirname(local_file)):\n            os.makedirs(os.path.dirname(local_file))\n\n        bucket.download_file(obj.key, local_file)\n        print(f\"Downloaded {obj.key} to {local_file}\")\n\ndownload_files('incartdb128x128', 'test', '/content/input')","metadata":{"execution":{"iopub.status.busy":"2023-10-28T17:17:06.729065Z","iopub.execute_input":"2023-10-28T17:17:06.729819Z","iopub.status.idle":"2023-10-28T17:17:13.396684Z","shell.execute_reply.started":"2023-10-28T17:17:06.729782Z","shell.execute_reply":"2023-10-28T17:17:13.395736Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"Downloaded testfiles_class5_fold0_6282.tfrec to /content/input/testfiles_class5_fold0_6282.tfrec\nDownloaded testfiles_class5_fold1_6283.tfrec to /content/input/testfiles_class5_fold1_6283.tfrec\nDownloaded testfiles_class5_fold2_6282.tfrec to /content/input/testfiles_class5_fold2_6282.tfrec\nDownloaded testfiles_class5_fold3_6282.tfrec to /content/input/testfiles_class5_fold3_6282.tfrec\nDownloaded testfiles_class5_fold4_6282.tfrec to /content/input/testfiles_class5_fold4_6282.tfrec\nDownloaded testfiles_class5_fold5_6282.tfrec to /content/input/testfiles_class5_fold5_6282.tfrec\nDownloaded testfiles_class5_fold6_6282.tfrec to /content/input/testfiles_class5_fold6_6282.tfrec\nDownloaded testfiles_class5_fold7_6283.tfrec to /content/input/testfiles_class5_fold7_6283.tfrec\nDownloaded testfiles_class5_fold8_6282.tfrec to /content/input/testfiles_class5_fold8_6282.tfrec\nDownloaded testfiles_class5_fold9_6282.tfrec to /content/input/testfiles_class5_fold9_6282.tfrec\n","output_type":"stream"}]},{"cell_type":"code","source":"!ls /content/input","metadata":{"execution":{"iopub.status.busy":"2023-10-28T17:16:41.729293Z","iopub.execute_input":"2023-10-28T17:16:41.730449Z","iopub.status.idle":"2023-10-28T17:16:42.757045Z","shell.execute_reply.started":"2023-10-28T17:16:41.730403Z","shell.execute_reply":"2023-10-28T17:16:42.755918Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"hparams = {\n    \"backbone\" : \"b0\",\n    \"batch_size\" : 32,\n    \"epochs\" : 40,\n    \"img_size\" : 128,\n    \"lr\" : 0.01,\n    \"optimizer\" : \"adam\",\n    \"seed\": 257,\n    \"notes\": \"SACC-s-dropout-changes\"\n}","metadata":{"execution":{"iopub.status.busy":"2023-10-28T16:56:48.627945Z","iopub.execute_input":"2023-10-28T16:56:48.628829Z","iopub.status.idle":"2023-10-28T16:56:48.634856Z","shell.execute_reply.started":"2023-10-28T16:56:48.628790Z","shell.execute_reply":"2023-10-28T16:56:48.633983Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"class WandBConfigurations():\n    def __init__(self, exp_name = \"ECG_BEAT_CLASSIFICATION_PAPER_COMMENTS\"):\n        self.EXPERIMENT_NAME = exp_name\n        os.environ[\"WANDB_API_KEY\"] = \"221507f411c2ddcc0c17238e115a12c528a482f6\"\n        wandb.login()\n\nWB = WandBConfigurations()","metadata":{"execution":{"iopub.status.busy":"2023-10-28T16:56:50.502352Z","iopub.execute_input":"2023-10-28T16:56:50.503005Z","iopub.status.idle":"2023-10-28T16:56:52.724281Z","shell.execute_reply.started":"2023-10-28T16:56:50.502972Z","shell.execute_reply":"2023-10-28T16:56:52.723319Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mshreya-srivas02\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"}]},{"cell_type":"code","source":" class Utils():\n    def __init__(self):\n        self.seed_everything()\n\n    def id_generator(size=6):\n        return str(uuid.uuid4())[:size]\n\n    def setupTPU(self):\n\n        try:\n            tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n            print('Running on TPU ', tpu.cluster_spec().as_dict())\n        except ValueError:\n            tpu = None\n\n        if tpu:\n            tf.config.experimental_connect_to_cluster(tpu)\n            tf.tpu.experimental.initialize_tpu_system(tpu)\n            strategy = tf.distribute.TPUStrategy(tpu)\n            STRATEGY = strategy\n            BATCH_SIZE = 8 * strategy.num_replicas_in_sync\n            # wandb.config.hardware = 'TPU'\n        else:\n            strategy = tf.distribute.get_strategy()\n            \n        return strategy\n\n    def seed_everything(self):\n        np.random.seed(hparams['seed'])\n        tf.random.set_seed(hparams['seed'])\n        random.seed(a=hparams['seed'])\n        os.environ['PYTHONHASHSEED'] = str(hparams['seed'])\n\nUTILS = Utils()\nSTRATEGY = UTILS.setupTPU()","metadata":{"execution":{"iopub.status.busy":"2023-10-28T17:19:24.736304Z","iopub.execute_input":"2023-10-28T17:19:24.737311Z","iopub.status.idle":"2023-10-28T17:19:24.820144Z","shell.execute_reply.started":"2023-10-28T17:19:24.737265Z","shell.execute_reply":"2023-10-28T17:19:24.819059Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"STRATEGY","metadata":{"execution":{"iopub.status.busy":"2023-10-28T17:19:28.215373Z","iopub.execute_input":"2023-10-28T17:19:28.216259Z","iopub.status.idle":"2023-10-28T17:19:28.223819Z","shell.execute_reply.started":"2023-10-28T17:19:28.216214Z","shell.execute_reply":"2023-10-28T17:19:28.222039Z"},"trusted":true},"execution_count":28,"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy at 0x7e797684eec0>"},"metadata":{}}]},{"cell_type":"code","source":"class Config():\n    def __init__(self):\n        self.DO_VAL_SPLIT = True\n        self.TRAIN_FILES = sorted(tf.io.gfile.glob('/content/input/test*.tfrec'))[:-1]\n        self.TOTAL_TRAIN_IMG = 50258\n        self.TOTAL_VAL_IMG = 6282\n        self.TOTAL_TEST_IMG = 6282\n        self.BACKBONE = hparams['backbone']\n        self.IMG_TRAIN_SHAPE = [hparams[\"img_size\"],hparams[\"img_size\"]]\n        self.DO_FINETUNE = True\n        self.BATCH_SIZE = hparams[\"batch_size\"] # 16\n        self.EPOCHES = hparams[\"epochs\"]\n        self.SEED = hparams[\"seed\"]\n        self.LOSS = tf.keras.losses.CategoricalCrossentropy()\n        self.OPTIMIZER = self.get_optimizer()\n        self.ACCURACY = []\n        self.CALLBACKS = []\n        self.STRATEGY = STRATEGY\n        self.FOLDS = 9\n        self.USE_LR_SCHEDULER = True\n        self.FOLD_NUMBER = 0\n        self.FOLDS_DICT = {}\n\n        if self.USE_LR_SCHEDULER:\n            lrfn = self.get_cosine_schedule_with_warmup(lr=hparams['lr'])\n            lr_schedule = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=0)\n            self.CALLBACKS.append(lr_schedule)\n\n    def get_optimizer(self):\n        if hparams['optimizer'] == 'adam':\n            return tf.keras.optimizers.Adam(learning_rate=hparams[\"lr\"])\n        if hparams['optimizer'] == 'rmsprop':\n            return tf.keras.optimizers.RMSprop(learning_rate=hparams[\"lr\"])\n        if hparams['optimizer'] == 'adagrad':\n            return tf.keras.optimizers.Adagrad(learning_rate=hparams[\"lr\"])\n        if hparams['optimizer'] == 'adadelta':\n            return tf.keras.optimizers.Adadelta(learning_rate=hparams[\"lr\"])\n\n        return tf.keras.optimizers.Adam(learning_rate=hparams[\"lr\"])\n\n    def get_cosine_schedule_with_warmup(\n        self,\n        lr = 0.00004,\n        num_warmup_steps = 0,\n        num_cycles=0.5):\n        num_training_steps = self.EPOCHES\n        def lrfn(epoch):\n            if epoch < num_warmup_steps:\n                return (float(epoch) / float(max(5, num_warmup_steps))) * lr\n            progress = float(epoch - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))\n            return max(0.0, 0.5 * (1.0 + math.cos(math.pi * float(num_cycles) * 2.0 * progress))) * lr\n\n        return lrfn\n\n\nCONFIG = Config()","metadata":{"execution":{"iopub.status.busy":"2023-10-28T17:20:10.551269Z","iopub.execute_input":"2023-10-28T17:20:10.552039Z","iopub.status.idle":"2023-10-28T17:20:10.569358Z","shell.execute_reply.started":"2023-10-28T17:20:10.552004Z","shell.execute_reply":"2023-10-28T17:20:10.568478Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"CONFIG.BATCH_SIZE","metadata":{"execution":{"iopub.status.busy":"2023-10-28T17:20:11.335864Z","iopub.execute_input":"2023-10-28T17:20:11.336231Z","iopub.status.idle":"2023-10-28T17:20:11.341964Z","shell.execute_reply.started":"2023-10-28T17:20:11.336202Z","shell.execute_reply":"2023-10-28T17:20:11.341063Z"},"trusted":true},"execution_count":36,"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"32"},"metadata":{}}]},{"cell_type":"code","source":"class Data():\n    def __init__(self):\n        self.LABELED_TFREC_FORMAT = {\n            \"image_id\": tf.io.FixedLenFeature([], tf.string),\n            \"image\": tf.io.FixedLenFeature([], tf.string),\n            'target5': tf.io.FixedLenFeature([], tf.int64),\n            'gender' : tf.io.FixedLenFeature([], tf.int64),\n            'age_interval' : tf.io.FixedLenFeature([], tf.int64),\n        }\n\n    def process_training_data(self, data_file):\n        data = tf.io.parse_single_example(data_file, self.LABELED_TFREC_FORMAT)\n        img = tf.image.decode_jpeg(data['image'], channels=1)\n        img = tf.cast(img, tf.float32) / 255.0\n        img = tf.reshape(img, [*CONFIG.IMG_TRAIN_SHAPE, 1])\n\n        age = tf.cast(data['age_interval'], tf.float32) / 10.0\n        sex = tf.cast(data['gender'], tf.float32) / 1.0\n        tab_data = [tf.cast(tfeat, dtype = tf.float32) for tfeat in [age, sex]]\n        tabular_data = tf.stack(tab_data)\n\n        target10 = tf.one_hot(data['target5'], depth=3)\n\n        return {'inp1' : img, 'inp2' : tabular_data}, {\"target10\" : target10 }\n\n    def process_testing_data(self, data_file):\n        data = tf.io.parse_single_example(data_file, self.LABELED_TFREC_FORMAT)\n        img = tf.image.decode_jpeg(data['image'], channels=1)\n        img = tf.cast(img, tf.float32) / 255.0\n        img = tf.reshape(img, [*CONFIG.IMG_TRAIN_SHAPE, 1])\n\n        age = tf.cast(data['age_interval'], tf.float32) / 10.0\n        sex = tf.cast(data['gender'], tf.float32) / 1.0\n        tab_data = [tf.cast(tfeat, dtype = tf.float32) for tfeat in [age, sex]]\n        tabular_data = tf.stack(tab_data)\n\n        target10 = tf.one_hot(data['target5'], depth=3)\n        image_id = data[\"image_id\"]\n\n        return {'inp1' : img, 'inp2' : tabular_data}, {\"target10\" : target10, \"image_id\":  data['image_id']}\n\n    def val_dataset(self):\n        ignore_order = tf.data.Options()\n        val_dataset = (\n            tf.data.TFRecordDataset(\n                CONFIG.FOLDS_DICT[\"fold_{}\".format(CONFIG.FOLD_NUMBER)][\"valfiles\"],\n                num_parallel_reads=tf.data.experimental.AUTOTUNE\n            ).with_options(\n                ignore_order\n            ).map(\n                self.process_training_data,\n                num_parallel_calls=tf.data.experimental.AUTOTUNE\n            ).batch(\n                CONFIG.BATCH_SIZE\n            ).prefetch(\n                tf.data.experimental.AUTOTUNE\n            )\n        )\n\n        return val_dataset\n\n    def train_dataset(self):\n        ignore_order = tf.data.Options()\n        ignore_order.experimental_deterministic = False\n        train_dataset = (\n            tf.data.TFRecordDataset(\n                CONFIG.FOLDS_DICT[\"fold_{}\".format(fold_number)][\"trainfiles\"],\n                num_parallel_reads=tf.data.experimental.AUTOTUNE\n            ).with_options(\n                ignore_order\n            ).map(\n                self.process_training_data,\n                num_parallel_calls=tf.data.experimental.AUTOTUNE\n            ).repeat(\n            ).shuffle(\n                CONFIG.SEED\n            ).batch(\n                CONFIG.BATCH_SIZE\n            ).prefetch(\n                tf.data.experimental.AUTOTUNE\n            )\n        )\n\n        return train_dataset\n\n    def test_dataset(self):\n        ignore_order = tf.data.Options()\n        TEST_FILES = sorted(tf.io.gfile.glob('/content/input/test*.tfrec'))[-1]\n        test_dataset = (\n            tf.data.TFRecordDataset(\n                TEST_FILES,\n                num_parallel_reads=tf.data.experimental.AUTOTUNE\n            ).with_options(\n                ignore_order\n            ).map(\n                self.process_testing_data,\n                num_parallel_calls=tf.data.experimental.AUTOTUNE\n            ).batch(\n                CONFIG.BATCH_SIZE *  4\n            ).prefetch(\n                tf.data.experimental.AUTOTUNE\n            )\n        )\n        return test_dataset\n","metadata":{"execution":{"iopub.status.busy":"2023-10-28T17:20:11.619185Z","iopub.execute_input":"2023-10-28T17:20:11.619529Z","iopub.status.idle":"2023-10-28T17:20:11.639364Z","shell.execute_reply.started":"2023-10-28T17:20:11.619501Z","shell.execute_reply":"2023-10-28T17:20:11.638413Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"class SACCLayer(tf.keras.layers.Layer):\n    def __init__(self, output_dim, **kwargs):\n        self.output_dim = output_dim\n        super(SACCLayer, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        self.kernel_1 = self.add_weight(name='weights_ECG',\n                                        shape=(input_shape[0][-1], self.output_dim),\n                                        initializer='he_normal',\n                                        trainable=True)\n        self.kernel_2 = self.add_weight(name='weights_Patient_Metadata',\n                                        shape=(input_shape[1][-1], self.output_dim),\n                                        initializer='he_normal',\n                                        trainable=True)\n        self.attention_weights1 = self.add_weight(name='attention_weights_ECG',\n                                                 shape=(self.output_dim,),\n                                                 initializer='uniform',\n                                                 trainable=True)\n        self.attention_weights2 = self.add_weight(name='attention_weights_Patient_metadata',\n                                                 shape=(self.output_dim,),\n                                                 initializer='uniform',\n                                                 trainable=True)\n        self.dense_layer = tf.keras.layers.Dense(self.output_dim, activation='relu', name='cca_dense')\n        super(SACCLayer, self).build(input_shape)\n\n    def call(self, inputs):\n        proj_1 = K.dot(inputs[0], self.kernel_1)\n        proj_2 = K.dot(inputs[1], self.kernel_2)\n\n        # Apply non-linear transformation\n        proj_1 = tf.keras.activations.relu(proj_1)\n        proj_2 = tf.keras.activations.relu(proj_2)\n\n        # Attention mechanism\n        attention_scores1 = tf.nn.softmax(self.attention_weights1)\n        attention_scores2 = tf.nn.softmax(self.attention_weights2)\n        proj_1 = attention_scores1 * proj_1\n        proj_2 = attention_scores2 * proj_2\n\n        # Non-linear fusion\n        fused_representation = tf.keras.layers.concatenate([proj_1, proj_2])\n        fused_representation = self.dense_layer(fused_representation)\n\n        return fused_representation\n\n    def compute_output_shape(self, input_shape):\n        return (input_shape[0][0], self.output_dim)\n\n\ndef depthwise_separable_conv_with_residual_block(x, filters, stride):\n    # Depthwise Convolution\n    depthwise = DepthwiseConv2D((3, 3), strides=stride, padding='same')(x)\n    depthwise = BatchNormalization()(depthwise)\n    depthwise = ReLU()(depthwise)\n\n    # Pointwise Convolution\n    pointwise = Conv2D(filters, (1, 1), strides=(1, 1), padding='same')(depthwise)\n    pointwise = BatchNormalization()(pointwise)\n    pointwise = ReLU()(pointwise)\n\n\n    return pointwise\n\n\ndef DualPathwayModel(inp1):\n    # Initial Convolution Layers\n    conv1 = Conv2D(32, (3, 3), strides=(2, 2), padding='same')(inp1)\n    conv1 = BatchNormalization()(conv1)\n    conv1 = ReLU()(conv1)\n\n    conv2 = Conv2D(32, (5, 5), strides=(2, 2), padding='same')(inp1)\n    conv2 = BatchNormalization()(conv2)\n    conv2 = ReLU()(conv2)\n    \n\n    concatenated = tf.keras.layers.concatenate([conv1, conv2])\n\n    x = depthwise_separable_conv_with_residual_block(concatenated, 64, (1, 1))\n    x = depthwise_separable_conv_with_residual_block(x, 128, (2, 2))\n    x = tf.keras.layers.Dropout(0.25)(x)\n    x = depthwise_separable_conv_with_residual_block(x, 128, (2, 2))\n    x = depthwise_separable_conv_with_residual_block(x, 256, (2, 2))\n    x = tf.keras.layers.Dropout(0.25)(x)\n    x = depthwise_separable_conv_with_residual_block(x, 256, (2, 2))\n    x = depthwise_separable_conv_with_residual_block(x, 256, (2, 2)) #<--- Can be removed\n\n    return x\n\n    \ndef dpm_sacc():\n    inp1  = tf.keras.layers.Input(shape = (*CONFIG.IMG_TRAIN_SHAPE, 1), name='inp1')\n    inp2  = tf.keras.layers.Input(shape = (2,), name='inp2')\n    x1 = DualPathwayModel(inp1)\n\n    x1 = tf.keras.layers.GlobalMaxPooling2D()(x1) # AVG is not good\n    x1 = tf.keras.layers.Dropout(0.2)(x1)\n\n    x2 = tf.keras.layers.Dense(8, name='metadata_feature_dense_1', activation='relu')(inp2)\n    x2 = tf.keras.layers.concatenate([x2, inp2])\n\n    x = SACCLayer(output_dim=256)([x1, x2])\n    \n    \n    x = tf.keras.layers.Dense(128, name='combine_feature_dense_1', activation='relu')(x)\n    x = tf.keras.layers.Dense(64, name='combine_feature_dense_2', activation='relu')(x)\n    x = tf.keras.layers.Dropout(0.1)(x)\n\n    output10 = tf.keras.layers.Dense(3, activation='softmax', name='target10')(x)\n\n    model = tf.keras.models.Model(inputs = [inp1, inp2], outputs = [output10])\n\n    return model","metadata":{"execution":{"iopub.status.busy":"2023-10-28T17:20:12.295248Z","iopub.execute_input":"2023-10-28T17:20:12.295584Z","iopub.status.idle":"2023-10-28T17:20:12.321383Z","shell.execute_reply.started":"2023-10-28T17:20:12.295557Z","shell.execute_reply":"2023-10-28T17:20:12.320562Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"dpm_sacc().summary()","metadata":{"execution":{"iopub.status.busy":"2023-10-28T17:20:12.917763Z","iopub.execute_input":"2023-10-28T17:20:12.918146Z","iopub.status.idle":"2023-10-28T17:20:13.505758Z","shell.execute_reply.started":"2023-10-28T17:20:12.918113Z","shell.execute_reply":"2023-10-28T17:20:13.504804Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stdout","text":"Model: \"model_5\"\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to                     \n==================================================================================================\n inp1 (InputLayer)              [(None, 128, 128, 1  0           []                               \n                                )]                                                                \n                                                                                                  \n conv2d_40 (Conv2D)             (None, 64, 64, 32)   320         ['inp1[0][0]']                   \n                                                                                                  \n conv2d_41 (Conv2D)             (None, 64, 64, 32)   832         ['inp1[0][0]']                   \n                                                                                                  \n batch_normalization_70 (BatchN  (None, 64, 64, 32)  128         ['conv2d_40[0][0]']              \n ormalization)                                                                                    \n                                                                                                  \n batch_normalization_71 (BatchN  (None, 64, 64, 32)  128         ['conv2d_41[0][0]']              \n ormalization)                                                                                    \n                                                                                                  \n re_lu_70 (ReLU)                (None, 64, 64, 32)   0           ['batch_normalization_70[0][0]'] \n                                                                                                  \n re_lu_71 (ReLU)                (None, 64, 64, 32)   0           ['batch_normalization_71[0][0]'] \n                                                                                                  \n concatenate_10 (Concatenate)   (None, 64, 64, 64)   0           ['re_lu_70[0][0]',               \n                                                                  're_lu_71[0][0]']               \n                                                                                                  \n depthwise_conv2d_30 (Depthwise  (None, 64, 64, 64)  640         ['concatenate_10[0][0]']         \n Conv2D)                                                                                          \n                                                                                                  \n batch_normalization_72 (BatchN  (None, 64, 64, 64)  256         ['depthwise_conv2d_30[0][0]']    \n ormalization)                                                                                    \n                                                                                                  \n re_lu_72 (ReLU)                (None, 64, 64, 64)   0           ['batch_normalization_72[0][0]'] \n                                                                                                  \n conv2d_42 (Conv2D)             (None, 64, 64, 64)   4160        ['re_lu_72[0][0]']               \n                                                                                                  \n batch_normalization_73 (BatchN  (None, 64, 64, 64)  256         ['conv2d_42[0][0]']              \n ormalization)                                                                                    \n                                                                                                  \n re_lu_73 (ReLU)                (None, 64, 64, 64)   0           ['batch_normalization_73[0][0]'] \n                                                                                                  \n depthwise_conv2d_31 (Depthwise  (None, 32, 32, 64)  640         ['re_lu_73[0][0]']               \n Conv2D)                                                                                          \n                                                                                                  \n batch_normalization_74 (BatchN  (None, 32, 32, 64)  256         ['depthwise_conv2d_31[0][0]']    \n ormalization)                                                                                    \n                                                                                                  \n re_lu_74 (ReLU)                (None, 32, 32, 64)   0           ['batch_normalization_74[0][0]'] \n                                                                                                  \n conv2d_43 (Conv2D)             (None, 32, 32, 128)  8320        ['re_lu_74[0][0]']               \n                                                                                                  \n batch_normalization_75 (BatchN  (None, 32, 32, 128)  512        ['conv2d_43[0][0]']              \n ormalization)                                                                                    \n                                                                                                  \n re_lu_75 (ReLU)                (None, 32, 32, 128)  0           ['batch_normalization_75[0][0]'] \n                                                                                                  \n dropout_20 (Dropout)           (None, 32, 32, 128)  0           ['re_lu_75[0][0]']               \n                                                                                                  \n depthwise_conv2d_32 (Depthwise  (None, 16, 16, 128)  1280       ['dropout_20[0][0]']             \n Conv2D)                                                                                          \n                                                                                                  \n batch_normalization_76 (BatchN  (None, 16, 16, 128)  512        ['depthwise_conv2d_32[0][0]']    \n ormalization)                                                                                    \n                                                                                                  \n re_lu_76 (ReLU)                (None, 16, 16, 128)  0           ['batch_normalization_76[0][0]'] \n                                                                                                  \n conv2d_44 (Conv2D)             (None, 16, 16, 128)  16512       ['re_lu_76[0][0]']               \n                                                                                                  \n batch_normalization_77 (BatchN  (None, 16, 16, 128)  512        ['conv2d_44[0][0]']              \n ormalization)                                                                                    \n                                                                                                  \n re_lu_77 (ReLU)                (None, 16, 16, 128)  0           ['batch_normalization_77[0][0]'] \n                                                                                                  \n depthwise_conv2d_33 (Depthwise  (None, 8, 8, 128)   1280        ['re_lu_77[0][0]']               \n Conv2D)                                                                                          \n                                                                                                  \n batch_normalization_78 (BatchN  (None, 8, 8, 128)   512         ['depthwise_conv2d_33[0][0]']    \n ormalization)                                                                                    \n                                                                                                  \n re_lu_78 (ReLU)                (None, 8, 8, 128)    0           ['batch_normalization_78[0][0]'] \n                                                                                                  \n conv2d_45 (Conv2D)             (None, 8, 8, 256)    33024       ['re_lu_78[0][0]']               \n                                                                                                  \n batch_normalization_79 (BatchN  (None, 8, 8, 256)   1024        ['conv2d_45[0][0]']              \n ormalization)                                                                                    \n                                                                                                  \n re_lu_79 (ReLU)                (None, 8, 8, 256)    0           ['batch_normalization_79[0][0]'] \n                                                                                                  \n dropout_21 (Dropout)           (None, 8, 8, 256)    0           ['re_lu_79[0][0]']               \n                                                                                                  \n depthwise_conv2d_34 (Depthwise  (None, 4, 4, 256)   2560        ['dropout_21[0][0]']             \n Conv2D)                                                                                          \n                                                                                                  \n batch_normalization_80 (BatchN  (None, 4, 4, 256)   1024        ['depthwise_conv2d_34[0][0]']    \n ormalization)                                                                                    \n                                                                                                  \n re_lu_80 (ReLU)                (None, 4, 4, 256)    0           ['batch_normalization_80[0][0]'] \n                                                                                                  \n conv2d_46 (Conv2D)             (None, 4, 4, 256)    65792       ['re_lu_80[0][0]']               \n                                                                                                  \n batch_normalization_81 (BatchN  (None, 4, 4, 256)   1024        ['conv2d_46[0][0]']              \n ormalization)                                                                                    \n                                                                                                  \n re_lu_81 (ReLU)                (None, 4, 4, 256)    0           ['batch_normalization_81[0][0]'] \n                                                                                                  \n depthwise_conv2d_35 (Depthwise  (None, 2, 2, 256)   2560        ['re_lu_81[0][0]']               \n Conv2D)                                                                                          \n                                                                                                  \n batch_normalization_82 (BatchN  (None, 2, 2, 256)   1024        ['depthwise_conv2d_35[0][0]']    \n ormalization)                                                                                    \n                                                                                                  \n re_lu_82 (ReLU)                (None, 2, 2, 256)    0           ['batch_normalization_82[0][0]'] \n                                                                                                  \n conv2d_47 (Conv2D)             (None, 2, 2, 256)    65792       ['re_lu_82[0][0]']               \n                                                                                                  \n batch_normalization_83 (BatchN  (None, 2, 2, 256)   1024        ['conv2d_47[0][0]']              \n ormalization)                                                                                    \n                                                                                                  \n re_lu_83 (ReLU)                (None, 2, 2, 256)    0           ['batch_normalization_83[0][0]'] \n                                                                                                  \n inp2 (InputLayer)              [(None, 2)]          0           []                               \n                                                                                                  \n global_max_pooling2d_5 (Global  (None, 256)         0           ['re_lu_83[0][0]']               \n MaxPooling2D)                                                                                    \n                                                                                                  \n metadata_feature_dense_1 (Dens  (None, 8)           24          ['inp2[0][0]']                   \n e)                                                                                               \n                                                                                                  \n dropout_22 (Dropout)           (None, 256)          0           ['global_max_pooling2d_5[0][0]'] \n                                                                                                  \n concatenate_11 (Concatenate)   (None, 10)           0           ['metadata_feature_dense_1[0][0]'\n                                                                 , 'inp2[0][0]']                  \n                                                                                                  \n sacc_layer_5 (SACCLayer)       (None, 256)          199936      ['dropout_22[0][0]',             \n                                                                  'concatenate_11[0][0]']         \n                                                                                                  \n combine_feature_dense_1 (Dense  (None, 128)         32896       ['sacc_layer_5[0][0]']           \n )                                                                                                \n                                                                                                  \n combine_feature_dense_2 (Dense  (None, 64)          8256        ['combine_feature_dense_1[0][0]']\n )                                                                                                \n                                                                                                  \n dropout_23 (Dropout)           (None, 64)           0           ['combine_feature_dense_2[0][0]']\n                                                                                                  \n target10 (Dense)               (None, 3)            195         ['dropout_23[0][0]']             \n                                                                                                  \n==================================================================================================\nTotal params: 453,211\nTrainable params: 449,115\nNon-trainable params: 4,096\n__________________________________________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"def fitengine(model, traindataset, valdataset = None, istraining = True):\n    model.compile(\n        optimizer   =  CONFIG.OPTIMIZER,\n        loss        =  CONFIG.LOSS,\n        metrics     =  CONFIG.ACCURACY\n    )\n\n    history = model.fit(\n                traindataset,\n                epochs            =   CONFIG.EPOCHES,\n                steps_per_epoch   =   CONFIG.TOTAL_TRAIN_IMG//CONFIG.BATCH_SIZE,\n                callbacks         =   CONFIG.CALLBACKS,\n                validation_data   =   valdataset,\n                validation_steps = (CONFIG.TOTAL_VAL_IMG)//(CONFIG.BATCH_SIZE) + 1,\n                verbose           =   1\n            )\n\n    return history\n\nskf = KFold(n_splits=CONFIG.FOLDS,shuffle=True,random_state=CONFIG.SEED)\nfor fold_number,(idxT,idxV) in enumerate(skf.split(np.arange(len(CONFIG.TRAIN_FILES)))):\n    CONFIG.FOLDS_DICT['fold_{}'.format(fold_number)] = {\n                                            \"trainfiles\" : [CONFIG.TRAIN_FILES[x] for x in idxT],\n                                            \"valfiles\"   : [CONFIG.TRAIN_FILES[x] for x in idxV]\n                                            }\n\nfold_number = CONFIG.FOLD_NUMBER\nprint(CONFIG.FOLDS_DICT['fold_{}'.format(fold_number)]['trainfiles'])\nprint(CONFIG.FOLDS_DICT['fold_{}'.format(fold_number)]['valfiles'])\n\nrun_ = wandb.init(\n    project= WB.EXPERIMENT_NAME,\n    reinit=True,\n    dir = \"/root\",\n    allow_val_change = True,\n    config = hparams\n)\n\nif CONFIG.STRATEGY is not None:\n    with CONFIG.STRATEGY.scope():\n        x2 = tf.keras.metrics.Precision(name='precision')\n        x3 = tf.keras.metrics.BinaryAccuracy(name='accuracy')\n        x4 = tf.keras.metrics.Recall(name='sensitivity')\n\n        CONFIG.ACCURACY.append(x2)\n        CONFIG.ACCURACY.append(x3)\n        CONFIG.ACCURACY.append(x4)\n\n        model = dpm_sacc()\n#         CONFIG.CALLBACKS.append(InLayerLossCallback())\nelse:\n    x2 = tf.keras.metrics.Precision(name='precision')\n    x3 = tf.keras.metrics.BinaryAccuracy(name='accuracy')\n    x4 = tf.keras.metrics.Recall(name='sensitivity')\n    x4 = tf.keras.metrics.Recall(name='sensitivity')\n\n    CONFIG.ACCURACY.append(x2)\n    CONFIG.ACCURACY.append(x3)\n    CONFIG.ACCURACY.append(x4)\n\n    model = dpm_sacc()\n\nCONFIG.CALLBACKS.append(tf.keras.callbacks.ModelCheckpoint(\n                                'model-%s.h5'%(fold_number), monitor='val_loss', verbose=1, save_best_only=True,\n                                save_weights_only=True, mode='min', save_freq='epoch'))\n\nCONFIG.CALLBACKS.append(WandbCallback(save_weights_only=True,\n                                            log_weights=True,\n                                            log_evaluation=True))\n\n\n\n\nDATA = Data()\n\nprint(\"##\"*30)\n\nhistory = fitengine(model, DATA.train_dataset(), valdataset=DATA.val_dataset()) #training model\n\nprint('##'*30)","metadata":{"execution":{"iopub.status.busy":"2023-10-28T17:20:13.622539Z","iopub.execute_input":"2023-10-28T17:20:13.623039Z","iopub.status.idle":"2023-10-28T18:06:11.852272Z","shell.execute_reply.started":"2023-10-28T17:20:13.623012Z","shell.execute_reply":"2023-10-28T18:06:11.851307Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stdout","text":"['/content/input/testfiles_class5_fold0_6282.tfrec', '/content/input/testfiles_class5_fold1_6283.tfrec', '/content/input/testfiles_class5_fold2_6282.tfrec', '/content/input/testfiles_class5_fold4_6282.tfrec', '/content/input/testfiles_class5_fold5_6282.tfrec', '/content/input/testfiles_class5_fold6_6282.tfrec', '/content/input/testfiles_class5_fold7_6283.tfrec', '/content/input/testfiles_class5_fold8_6282.tfrec']\n['/content/input/testfiles_class5_fold3_6282.tfrec']\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Finishing last run (ID:66i5lr3c) before initializing another..."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>▁</td></tr><tr><td>epoch</td><td>▁</td></tr><tr><td>loss</td><td>▁</td></tr><tr><td>lr</td><td>▁</td></tr><tr><td>precision</td><td>▁</td></tr><tr><td>sensitivity</td><td>▁</td></tr><tr><td>val_accuracy</td><td>▁</td></tr><tr><td>val_loss</td><td>▁</td></tr><tr><td>val_precision</td><td>▁</td></tr><tr><td>val_sensitivity</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>0.71352</td></tr><tr><td>best_epoch</td><td>0</td></tr><tr><td>best_val_loss</td><td>1169788370944.0</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>260500455424.0</td></tr><tr><td>lr</td><td>0.01</td></tr><tr><td>precision</td><td>0.53204</td></tr><tr><td>sensitivity</td><td>0.57505</td></tr><tr><td>val_accuracy</td><td>0.43705</td></tr><tr><td>val_loss</td><td>1169788370944.0</td></tr><tr><td>val_precision</td><td>0.11903</td></tr><tr><td>val_sensitivity</td><td>0.12842</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">graceful-wave-194</strong> at: <a href='https://wandb.ai/shreya-srivas02/ECG_BEAT_CLASSIFICATION_PAPER_COMMENTS/runs/66i5lr3c' target=\"_blank\">https://wandb.ai/shreya-srivas02/ECG_BEAT_CLASSIFICATION_PAPER_COMMENTS/runs/66i5lr3c</a><br/>Synced 6 W&B file(s), 1 media file(s), 5 artifact file(s) and 1 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>/root/wandb/run-20231028_170123-66i5lr3c/logs</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Successfully finished last run (ID:66i5lr3c). Initializing new run:<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.15.12"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/root/wandb/run-20231028_172013-9vnf4cg5</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/shreya-srivas02/ECG_BEAT_CLASSIFICATION_PAPER_COMMENTS/runs/9vnf4cg5' target=\"_blank\">logical-sponge-195</a></strong> to <a href='https://wandb.ai/shreya-srivas02/ECG_BEAT_CLASSIFICATION_PAPER_COMMENTS' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/shreya-srivas02/ECG_BEAT_CLASSIFICATION_PAPER_COMMENTS' target=\"_blank\">https://wandb.ai/shreya-srivas02/ECG_BEAT_CLASSIFICATION_PAPER_COMMENTS</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/shreya-srivas02/ECG_BEAT_CLASSIFICATION_PAPER_COMMENTS/runs/9vnf4cg5' target=\"_blank\">https://wandb.ai/shreya-srivas02/ECG_BEAT_CLASSIFICATION_PAPER_COMMENTS/runs/9vnf4cg5</a>"},"metadata":{}},{"name":"stdout","text":"############################################################\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m WandbCallback is unable to log validation data. When using a generator for validation_data, you must pass validation_steps\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/40\n","output_type":"stream"},{"name":"stderr","text":"2023-10-28 17:21:18.754081: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape inmodel_6/dropout_24/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n","output_type":"stream"},{"name":"stdout","text":"1569/1570 [============================>.] - ETA: 0s - loss: 0.1743 - precision: 0.9577 - accuracy: 0.9681 - sensitivity: 0.9460\nEpoch 1: val_loss improved from inf to 2.61897, saving model to model-0.h5\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/root/wandb/run-20231028_172013-9vnf4cg5/files/model-best)... Done. 0.0s\n","output_type":"stream"},{"name":"stdout","text":"1570/1570 [==============================] - 86s 48ms/step - loss: 0.1743 - precision: 0.9577 - accuracy: 0.9681 - sensitivity: 0.9460 - val_loss: 2.6190 - val_precision: 0.3356 - val_accuracy: 0.5587 - val_sensitivity: 0.3305 - lr: 0.0100\nEpoch 2/40\n1570/1570 [==============================] - ETA: 0s - loss: 0.0988 - precision: 0.9771 - accuracy: 0.9836 - sensitivity: 0.9737\nEpoch 2: val_loss improved from 2.61897 to 0.07203, saving model to model-0.h5\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/root/wandb/run-20231028_172013-9vnf4cg5/files/model-best)... Done. 0.0s\n","output_type":"stream"},{"name":"stdout","text":"1570/1570 [==============================] - 74s 47ms/step - loss: 0.0988 - precision: 0.9771 - accuracy: 0.9836 - sensitivity: 0.9737 - val_loss: 0.0720 - val_precision: 0.9802 - val_accuracy: 0.9862 - val_sensitivity: 0.9784 - lr: 0.0100\nEpoch 3/40\n1570/1570 [==============================] - ETA: 0s - loss: 0.0825 - precision: 0.9795 - accuracy: 0.9857 - sensitivity: 0.9777\nEpoch 3: val_loss did not improve from 0.07203\n1570/1570 [==============================] - 66s 42ms/step - loss: 0.0825 - precision: 0.9795 - accuracy: 0.9857 - sensitivity: 0.9777 - val_loss: 0.0767 - val_precision: 0.9831 - val_accuracy: 0.9884 - val_sensitivity: 0.9820 - lr: 0.0099\nEpoch 4/40\n1569/1570 [============================>.] - ETA: 0s - loss: 0.0744 - precision: 0.9819 - accuracy: 0.9874 - sensitivity: 0.9801\nEpoch 4: val_loss improved from 0.07203 to 0.04853, saving model to model-0.h5\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/root/wandb/run-20231028_172013-9vnf4cg5/files/model-best)... Done. 0.0s\n","output_type":"stream"},{"name":"stdout","text":"1570/1570 [==============================] - 74s 47ms/step - loss: 0.0744 - precision: 0.9820 - accuracy: 0.9874 - sensitivity: 0.9801 - val_loss: 0.0485 - val_precision: 0.9873 - val_accuracy: 0.9913 - val_sensitivity: 0.9866 - lr: 0.0099\nEpoch 5/40\n1569/1570 [============================>.] - ETA: 0s - loss: 0.0602 - precision: 0.9841 - accuracy: 0.9890 - sensitivity: 0.9830\nEpoch 5: val_loss did not improve from 0.04853\n1570/1570 [==============================] - 66s 42ms/step - loss: 0.0602 - precision: 0.9841 - accuracy: 0.9890 - sensitivity: 0.9830 - val_loss: 0.1156 - val_precision: 0.9619 - val_accuracy: 0.9691 - val_sensitivity: 0.9446 - lr: 0.0098\nEpoch 6/40\n1569/1570 [============================>.] - ETA: 0s - loss: 0.0572 - precision: 0.9859 - accuracy: 0.9901 - sensitivity: 0.9844\nEpoch 6: val_loss did not improve from 0.04853\n1570/1570 [==============================] - 67s 42ms/step - loss: 0.0572 - precision: 0.9859 - accuracy: 0.9901 - sensitivity: 0.9844 - val_loss: 0.0733 - val_precision: 0.9796 - val_accuracy: 0.9862 - val_sensitivity: 0.9790 - lr: 0.0096\nEpoch 7/40\n1570/1570 [==============================] - ETA: 0s - loss: 0.0545 - precision: 0.9859 - accuracy: 0.9904 - sensitivity: 0.9854\nEpoch 7: val_loss did not improve from 0.04853\n1570/1570 [==============================] - 66s 42ms/step - loss: 0.0545 - precision: 0.9859 - accuracy: 0.9904 - sensitivity: 0.9854 - val_loss: 0.1488 - val_precision: 0.9439 - val_accuracy: 0.9617 - val_sensitivity: 0.9409 - lr: 0.0095\nEpoch 8/40\n1569/1570 [============================>.] - ETA: 0s - loss: 0.0489 - precision: 0.9878 - accuracy: 0.9917 - sensitivity: 0.9871\nEpoch 8: val_loss did not improve from 0.04853\n1570/1570 [==============================] - 66s 42ms/step - loss: 0.0489 - precision: 0.9878 - accuracy: 0.9917 - sensitivity: 0.9871 - val_loss: 0.0844 - val_precision: 0.9777 - val_accuracy: 0.9848 - val_sensitivity: 0.9766 - lr: 0.0093\nEpoch 9/40\n1570/1570 [==============================] - ETA: 0s - loss: 0.0469 - precision: 0.9876 - accuracy: 0.9916 - sensitivity: 0.9871\nEpoch 9: val_loss improved from 0.04853 to 0.03871, saving model to model-0.h5\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/root/wandb/run-20231028_172013-9vnf4cg5/files/model-best)... Done. 0.0s\n","output_type":"stream"},{"name":"stdout","text":"1570/1570 [==============================] - 74s 47ms/step - loss: 0.0469 - precision: 0.9876 - accuracy: 0.9916 - sensitivity: 0.9871 - val_loss: 0.0387 - val_precision: 0.9896 - val_accuracy: 0.9928 - val_sensitivity: 0.9889 - lr: 0.0090\nEpoch 10/40\n1570/1570 [==============================] - ETA: 0s - loss: 0.0424 - precision: 0.9894 - accuracy: 0.9927 - sensitivity: 0.9888\nEpoch 10: val_loss did not improve from 0.03871\n1570/1570 [==============================] - 66s 42ms/step - loss: 0.0424 - precision: 0.9894 - accuracy: 0.9927 - sensitivity: 0.9888 - val_loss: 0.0436 - val_precision: 0.9900 - val_accuracy: 0.9932 - val_sensitivity: 0.9895 - lr: 0.0088\nEpoch 11/40\n1570/1570 [==============================] - ETA: 0s - loss: 0.0412 - precision: 0.9894 - accuracy: 0.9927 - sensitivity: 0.9889\nEpoch 11: val_loss did not improve from 0.03871\n1570/1570 [==============================] - 66s 42ms/step - loss: 0.0412 - precision: 0.9894 - accuracy: 0.9927 - sensitivity: 0.9889 - val_loss: 0.0499 - val_precision: 0.9854 - val_accuracy: 0.9901 - val_sensitivity: 0.9850 - lr: 0.0085\nEpoch 12/40\n1570/1570 [==============================] - ETA: 0s - loss: 0.0385 - precision: 0.9900 - accuracy: 0.9932 - sensitivity: 0.9895\nEpoch 12: val_loss did not improve from 0.03871\n1570/1570 [==============================] - 66s 42ms/step - loss: 0.0385 - precision: 0.9900 - accuracy: 0.9932 - sensitivity: 0.9895 - val_loss: 0.0664 - val_precision: 0.9852 - val_accuracy: 0.9898 - val_sensitivity: 0.9841 - lr: 0.0082\nEpoch 13/40\n1570/1570 [==============================] - ETA: 0s - loss: 0.0388 - precision: 0.9904 - accuracy: 0.9934 - sensitivity: 0.9899\nEpoch 13: val_loss did not improve from 0.03871\n1570/1570 [==============================] - 66s 42ms/step - loss: 0.0388 - precision: 0.9904 - accuracy: 0.9934 - sensitivity: 0.9899 - val_loss: 0.0433 - val_precision: 0.9895 - val_accuracy: 0.9928 - val_sensitivity: 0.9889 - lr: 0.0079\nEpoch 14/40\n1569/1570 [============================>.] - ETA: 0s - loss: 0.0336 - precision: 0.9915 - accuracy: 0.9942 - sensitivity: 0.9911\nEpoch 14: val_loss did not improve from 0.03871\n1570/1570 [==============================] - 66s 42ms/step - loss: 0.0336 - precision: 0.9915 - accuracy: 0.9942 - sensitivity: 0.9911 - val_loss: 0.0493 - val_precision: 0.9887 - val_accuracy: 0.9921 - val_sensitivity: 0.9876 - lr: 0.0076\nEpoch 15/40\n1570/1570 [==============================] - ETA: 0s - loss: 0.0326 - precision: 0.9916 - accuracy: 0.9943 - sensitivity: 0.9914\nEpoch 15: val_loss did not improve from 0.03871\n1570/1570 [==============================] - 66s 42ms/step - loss: 0.0326 - precision: 0.9916 - accuracy: 0.9943 - sensitivity: 0.9914 - val_loss: 0.0508 - val_precision: 0.9863 - val_accuracy: 0.9908 - val_sensitivity: 0.9862 - lr: 0.0073\nEpoch 16/40\n1569/1570 [============================>.] - ETA: 0s - loss: 0.0305 - precision: 0.9919 - accuracy: 0.9945 - sensitivity: 0.9917\nEpoch 16: val_loss did not improve from 0.03871\n1570/1570 [==============================] - 66s 42ms/step - loss: 0.0305 - precision: 0.9919 - accuracy: 0.9945 - sensitivity: 0.9917 - val_loss: 0.0727 - val_precision: 0.9822 - val_accuracy: 0.9878 - val_sensitivity: 0.9814 - lr: 0.0069\nEpoch 17/40\n1570/1570 [==============================] - ETA: 0s - loss: 0.0304 - precision: 0.9919 - accuracy: 0.9946 - sensitivity: 0.9918\nEpoch 17: val_loss did not improve from 0.03871\n1570/1570 [==============================] - 66s 42ms/step - loss: 0.0304 - precision: 0.9919 - accuracy: 0.9946 - sensitivity: 0.9918 - val_loss: 0.0557 - val_precision: 0.9834 - val_accuracy: 0.9889 - val_sensitivity: 0.9833 - lr: 0.0065\nEpoch 18/40\n1570/1570 [==============================] - ETA: 0s - loss: 0.0304 - precision: 0.9926 - accuracy: 0.9950 - sensitivity: 0.9923\nEpoch 18: val_loss did not improve from 0.03871\n1570/1570 [==============================] - 66s 42ms/step - loss: 0.0304 - precision: 0.9926 - accuracy: 0.9950 - sensitivity: 0.9923 - val_loss: 0.0412 - val_precision: 0.9876 - val_accuracy: 0.9915 - val_sensitivity: 0.9869 - lr: 0.0062\nEpoch 19/40\n1569/1570 [============================>.] - ETA: 0s - loss: 0.0262 - precision: 0.9935 - accuracy: 0.9956 - sensitivity: 0.9933\nEpoch 19: val_loss improved from 0.03871 to 0.03301, saving model to model-0.h5\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/root/wandb/run-20231028_172013-9vnf4cg5/files/model-best)... Done. 0.0s\n","output_type":"stream"},{"name":"stdout","text":"1570/1570 [==============================] - 74s 47ms/step - loss: 0.0262 - precision: 0.9935 - accuracy: 0.9956 - sensitivity: 0.9933 - val_loss: 0.0330 - val_precision: 0.9930 - val_accuracy: 0.9953 - val_sensitivity: 0.9930 - lr: 0.0058\nEpoch 20/40\n1569/1570 [============================>.] - ETA: 0s - loss: 0.0245 - precision: 0.9941 - accuracy: 0.9960 - sensitivity: 0.9939\nEpoch 20: val_loss improved from 0.03301 to 0.02791, saving model to model-0.h5\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/root/wandb/run-20231028_172013-9vnf4cg5/files/model-best)... Done. 0.0s\n","output_type":"stream"},{"name":"stdout","text":"1570/1570 [==============================] - 74s 47ms/step - loss: 0.0245 - precision: 0.9941 - accuracy: 0.9960 - sensitivity: 0.9939 - val_loss: 0.0279 - val_precision: 0.9932 - val_accuracy: 0.9954 - val_sensitivity: 0.9930 - lr: 0.0054\nEpoch 21/40\n1570/1570 [==============================] - ETA: 0s - loss: 0.0243 - precision: 0.9936 - accuracy: 0.9957 - sensitivity: 0.9936\nEpoch 21: val_loss did not improve from 0.02791\n1570/1570 [==============================] - 66s 42ms/step - loss: 0.0243 - precision: 0.9936 - accuracy: 0.9957 - sensitivity: 0.9936 - val_loss: 0.0369 - val_precision: 0.9925 - val_accuracy: 0.9950 - val_sensitivity: 0.9925 - lr: 0.0050\nEpoch 22/40\n1569/1570 [============================>.] - ETA: 0s - loss: 0.0222 - precision: 0.9945 - accuracy: 0.9963 - sensitivity: 0.9944\nEpoch 22: val_loss did not improve from 0.02791\n1570/1570 [==============================] - 66s 42ms/step - loss: 0.0222 - precision: 0.9945 - accuracy: 0.9963 - sensitivity: 0.9944 - val_loss: 0.0288 - val_precision: 0.9933 - val_accuracy: 0.9955 - val_sensitivity: 0.9933 - lr: 0.0046\nEpoch 23/40\n1569/1570 [============================>.] - ETA: 0s - loss: 0.0205 - precision: 0.9946 - accuracy: 0.9964 - sensitivity: 0.9946\nEpoch 23: val_loss did not improve from 0.02791\n1570/1570 [==============================] - 66s 42ms/step - loss: 0.0205 - precision: 0.9946 - accuracy: 0.9964 - sensitivity: 0.9946 - val_loss: 0.0336 - val_precision: 0.9927 - val_accuracy: 0.9951 - val_sensitivity: 0.9927 - lr: 0.0042\nEpoch 24/40\n1570/1570 [==============================] - ETA: 0s - loss: 0.0211 - precision: 0.9947 - accuracy: 0.9964 - sensitivity: 0.9945\nEpoch 24: val_loss did not improve from 0.02791\n1570/1570 [==============================] - 66s 42ms/step - loss: 0.0211 - precision: 0.9947 - accuracy: 0.9964 - sensitivity: 0.9945 - val_loss: 0.0328 - val_precision: 0.9932 - val_accuracy: 0.9954 - val_sensitivity: 0.9932 - lr: 0.0038\nEpoch 25/40\n1570/1570 [==============================] - ETA: 0s - loss: 0.0203 - precision: 0.9953 - accuracy: 0.9968 - sensitivity: 0.9950\nEpoch 25: val_loss did not improve from 0.02791\n1570/1570 [==============================] - 67s 43ms/step - loss: 0.0203 - precision: 0.9953 - accuracy: 0.9968 - sensitivity: 0.9950 - val_loss: 0.0348 - val_precision: 0.9925 - val_accuracy: 0.9950 - val_sensitivity: 0.9924 - lr: 0.0035\nEpoch 26/40\n1570/1570 [==============================] - ETA: 0s - loss: 0.0171 - precision: 0.9957 - accuracy: 0.9971 - sensitivity: 0.9956\nEpoch 26: val_loss did not improve from 0.02791\n1570/1570 [==============================] - 66s 42ms/step - loss: 0.0171 - precision: 0.9957 - accuracy: 0.9971 - sensitivity: 0.9956 - val_loss: 0.0353 - val_precision: 0.9936 - val_accuracy: 0.9957 - val_sensitivity: 0.9935 - lr: 0.0031\nEpoch 27/40\n1569/1570 [============================>.] - ETA: 0s - loss: 0.0175 - precision: 0.9959 - accuracy: 0.9972 - sensitivity: 0.9957\nEpoch 27: val_loss did not improve from 0.02791\n1570/1570 [==============================] - 66s 42ms/step - loss: 0.0175 - precision: 0.9959 - accuracy: 0.9972 - sensitivity: 0.9957 - val_loss: 0.0332 - val_precision: 0.9911 - val_accuracy: 0.9941 - val_sensitivity: 0.9911 - lr: 0.0027\nEpoch 28/40\n1569/1570 [============================>.] - ETA: 0s - loss: 0.0156 - precision: 0.9962 - accuracy: 0.9974 - sensitivity: 0.9961\nEpoch 28: val_loss did not improve from 0.02791\n1570/1570 [==============================] - 66s 42ms/step - loss: 0.0155 - precision: 0.9962 - accuracy: 0.9974 - sensitivity: 0.9961 - val_loss: 0.0333 - val_precision: 0.9935 - val_accuracy: 0.9956 - val_sensitivity: 0.9935 - lr: 0.0024\nEpoch 29/40\n1570/1570 [==============================] - ETA: 0s - loss: 0.0164 - precision: 0.9961 - accuracy: 0.9974 - sensitivity: 0.9960\nEpoch 29: val_loss did not improve from 0.02791\n1570/1570 [==============================] - 66s 42ms/step - loss: 0.0164 - precision: 0.9961 - accuracy: 0.9974 - sensitivity: 0.9960 - val_loss: 0.0284 - val_precision: 0.9940 - val_accuracy: 0.9960 - val_sensitivity: 0.9940 - lr: 0.0021\nEpoch 30/40\n1569/1570 [============================>.] - ETA: 0s - loss: 0.0136 - precision: 0.9966 - accuracy: 0.9977 - sensitivity: 0.9966\nEpoch 30: val_loss did not improve from 0.02791\n1570/1570 [==============================] - 66s 42ms/step - loss: 0.0135 - precision: 0.9966 - accuracy: 0.9977 - sensitivity: 0.9966 - val_loss: 0.0334 - val_precision: 0.9941 - val_accuracy: 0.9961 - val_sensitivity: 0.9941 - lr: 0.0018\nEpoch 31/40\n1570/1570 [==============================] - ETA: 0s - loss: 0.0136 - precision: 0.9966 - accuracy: 0.9977 - sensitivity: 0.9965\nEpoch 31: val_loss did not improve from 0.02791\n1570/1570 [==============================] - 66s 42ms/step - loss: 0.0136 - precision: 0.9966 - accuracy: 0.9977 - sensitivity: 0.9965 - val_loss: 0.0393 - val_precision: 0.9917 - val_accuracy: 0.9944 - val_sensitivity: 0.9914 - lr: 0.0015\nEpoch 32/40\n1569/1570 [============================>.] - ETA: 0s - loss: 0.0118 - precision: 0.9969 - accuracy: 0.9979 - sensitivity: 0.9968\nEpoch 32: val_loss did not improve from 0.02791\n1570/1570 [==============================] - 66s 42ms/step - loss: 0.0118 - precision: 0.9968 - accuracy: 0.9979 - sensitivity: 0.9968 - val_loss: 0.0325 - val_precision: 0.9938 - val_accuracy: 0.9959 - val_sensitivity: 0.9938 - lr: 0.0012\nEpoch 33/40\n1570/1570 [==============================] - ETA: 0s - loss: 0.0116 - precision: 0.9973 - accuracy: 0.9981 - sensitivity: 0.9972\nEpoch 33: val_loss did not improve from 0.02791\n1570/1570 [==============================] - 66s 42ms/step - loss: 0.0116 - precision: 0.9973 - accuracy: 0.9981 - sensitivity: 0.9972 - val_loss: 0.0411 - val_precision: 0.9940 - val_accuracy: 0.9959 - val_sensitivity: 0.9938 - lr: 9.5492e-04\nEpoch 34/40\n1570/1570 [==============================] - ETA: 0s - loss: 0.0111 - precision: 0.9972 - accuracy: 0.9981 - sensitivity: 0.9971\nEpoch 34: val_loss did not improve from 0.02791\n1570/1570 [==============================] - 66s 42ms/step - loss: 0.0111 - precision: 0.9972 - accuracy: 0.9981 - sensitivity: 0.9971 - val_loss: 0.0385 - val_precision: 0.9943 - val_accuracy: 0.9961 - val_sensitivity: 0.9941 - lr: 7.3680e-04\nEpoch 35/40\n1570/1570 [==============================] - ETA: 0s - loss: 0.0110 - precision: 0.9974 - accuracy: 0.9983 - sensitivity: 0.9974\nEpoch 35: val_loss did not improve from 0.02791\n1570/1570 [==============================] - 66s 42ms/step - loss: 0.0110 - precision: 0.9974 - accuracy: 0.9983 - sensitivity: 0.9974 - val_loss: 0.0377 - val_precision: 0.9943 - val_accuracy: 0.9962 - val_sensitivity: 0.9943 - lr: 5.4497e-04\nEpoch 36/40\n1569/1570 [============================>.] - ETA: 0s - loss: 0.0104 - precision: 0.9975 - accuracy: 0.9983 - sensitivity: 0.9974\nEpoch 36: val_loss did not improve from 0.02791\n1570/1570 [==============================] - 66s 42ms/step - loss: 0.0104 - precision: 0.9975 - accuracy: 0.9983 - sensitivity: 0.9974 - val_loss: 0.0390 - val_precision: 0.9943 - val_accuracy: 0.9962 - val_sensitivity: 0.9943 - lr: 3.8060e-04\nEpoch 37/40\n1569/1570 [============================>.] - ETA: 0s - loss: 0.0110 - precision: 0.9976 - accuracy: 0.9984 - sensitivity: 0.9975\nEpoch 37: val_loss did not improve from 0.02791\n1570/1570 [==============================] - 66s 42ms/step - loss: 0.0110 - precision: 0.9976 - accuracy: 0.9984 - sensitivity: 0.9975 - val_loss: 0.0362 - val_precision: 0.9943 - val_accuracy: 0.9962 - val_sensitivity: 0.9943 - lr: 2.4472e-04\nEpoch 38/40\n1569/1570 [============================>.] - ETA: 0s - loss: 0.0104 - precision: 0.9975 - accuracy: 0.9983 - sensitivity: 0.9974\nEpoch 38: val_loss did not improve from 0.02791\n1570/1570 [==============================] - 66s 42ms/step - loss: 0.0104 - precision: 0.9975 - accuracy: 0.9983 - sensitivity: 0.9974 - val_loss: 0.0355 - val_precision: 0.9944 - val_accuracy: 0.9963 - val_sensitivity: 0.9944 - lr: 1.3815e-04\nEpoch 39/40\n1569/1570 [============================>.] - ETA: 0s - loss: 0.0103 - precision: 0.9977 - accuracy: 0.9984 - sensitivity: 0.9976\nEpoch 39: val_loss did not improve from 0.02791\n1570/1570 [==============================] - 66s 42ms/step - loss: 0.0103 - precision: 0.9977 - accuracy: 0.9984 - sensitivity: 0.9977 - val_loss: 0.0356 - val_precision: 0.9944 - val_accuracy: 0.9963 - val_sensitivity: 0.9944 - lr: 6.1558e-05\nEpoch 40/40\n1570/1570 [==============================] - ETA: 0s - loss: 0.0103 - precision: 0.9975 - accuracy: 0.9983 - sensitivity: 0.9975\nEpoch 40: val_loss did not improve from 0.02791\n1570/1570 [==============================] - 66s 42ms/step - loss: 0.0103 - precision: 0.9975 - accuracy: 0.9983 - sensitivity: 0.9975 - val_loss: 0.0355 - val_precision: 0.9946 - val_accuracy: 0.9963 - val_sensitivity: 0.9944 - lr: 1.5413e-05\n############################################################\n","output_type":"stream"}]},{"cell_type":"code","source":"model.save('model_last_epoch.h5')\nmodel_till_last_epoch = model\nSAVED_MODEL_LOC = \"model-0.h5\"\nmodel = dpm_sacc()\nmodel.load_weights(SAVED_MODEL_LOC)","metadata":{"execution":{"iopub.status.busy":"2023-10-28T18:06:11.854292Z","iopub.execute_input":"2023-10-28T18:06:11.854890Z","iopub.status.idle":"2023-10-28T18:06:12.552153Z","shell.execute_reply.started":"2023-10-28T18:06:11.854833Z","shell.execute_reply":"2023-10-28T18:06:12.551084Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"NAME = ['Normal', \"SEB\",  'VEB']\nfor model_type, trained_model in zip(['best_epoch', 'last_epoch'], [model, model_till_last_epoch]):\n    test_imgs = DATA.test_dataset().map(lambda data, ids: data)\n    img_labels_ds = DATA.test_dataset().map(lambda data, ids: ids).unbatch()\n\n    STEPS = (CONFIG.TOTAL_TEST_IMG)//(CONFIG.BATCH_SIZE*4) + 1\n\n    y_pred = trained_model.predict(test_imgs,steps = int(STEPS), verbose=1)\n    test_labels = next(iter(img_labels_ds.batch(int(CONFIG.TOTAL_TEST_IMG) + 1)))\n    y_true = test_labels[\"target10\"].numpy()\n    pd.DataFrame({\n            'image_id'  : test_labels[\"image_id\"].numpy(),\n            'actual'  : np.argmax(y_true, axis=1),\n            'predicted'      : np.argmax(y_pred, axis=1)\n            }).to_csv('prediction_{}.csv'.format(model_type), index=False)\n\n    df = pd.read_csv(\"prediction_{}.csv\".format(model_type))\n\n    run_.log({f\"{model_type}_pr\": wandb.plot.pr_curve(np.argmax(y_true, axis=1), y_pred, labels=NAME)})\n    run_.log({f\"{model_type}_roc\": wandb.plot.roc_curve(np.argmax(y_true, axis=1), y_pred, labels=NAME)})\n\n    cm = wandb.plot.confusion_matrix(\n                    y_true=np.argmax(y_true, axis=1),\n                    preds=np.argmax(y_pred, axis=1),\n                    class_names=NAME)\n\n    run_.log({f\"{model_type}_conf_mat\": cm})\n\n    harvest = confusion_matrix(df['actual'], df['predicted'])\n    fig, ax = plt.subplots(figsize=(8,8))\n    im = ax.imshow(harvest)\n    ax.set_xticks(np.arange(len(NAME)))\n    ax.set_yticks(np.arange(len(NAME)))\n    ax.set_xticklabels(NAME)\n    ax.set_yticklabels(NAME)\n\n    # Rotate the tick labels and set their alignment.\n    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n            rotation_mode=\"anchor\")\n\n    for i in range(len(NAME)):\n        for j in range(len(NAME)):\n            text = ax.text(j, i, harvest[i, j],\n                        ha=\"center\", va=\"center\", color=\"w\")\n\n    fig.tight_layout()\n    run_.log({f\"{model_type}_cm\": plt})\n\n    from sklearn.metrics import classification_report\n    target_names = NAME\n    x_ = classification_report(df['actual'], df['predicted'], target_names=target_names, digits=4)\n    x2 = classification_report(df['actual'], df['predicted'], target_names=target_names, digits=4, output_dict=True)\n    print(x_)\n    run_.log({f\"{model_type}_CR\": x2})\n\n\n## log more\nartifact = wandb.Artifact(\"Full_Logs\", type=\"logs\")\nartifact.add_dir(\"/kaggle/working/\")\nwandb.log_artifact(artifact)","metadata":{"execution":{"iopub.status.busy":"2023-10-28T18:06:47.828576Z","iopub.execute_input":"2023-10-28T18:06:47.829354Z","iopub.status.idle":"2023-10-28T18:06:53.195492Z","shell.execute_reply.started":"2023-10-28T18:06:47.829321Z","shell.execute_reply":"2023-10-28T18:06:53.194316Z"},"trusted":true},"execution_count":43,"outputs":[{"name":"stdout","text":"50/50 [==============================] - 1s 15ms/step\n              precision    recall  f1-score   support\n\n      Normal     0.9939    0.9963    0.9951      4086\n         SEB     0.9620    0.8939    0.9267       198\n         VEB     0.9955    0.9975    0.9965      1998\n\n    accuracy                         0.9935      6282\n   macro avg     0.9838    0.9626    0.9728      6282\nweighted avg     0.9934    0.9935    0.9934      6282\n\n50/50 [==============================] - 1s 18ms/step\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/kaggle/working)... Done. 0.0s\n","output_type":"stream"},{"name":"stdout","text":"              precision    recall  f1-score   support\n\n      Normal     0.9966    0.9976    0.9971      4086\n         SEB     0.9691    0.9495    0.9592       198\n         VEB     0.9975    0.9975    0.9975      1998\n\n    accuracy                         0.9960      6282\n   macro avg     0.9877    0.9815    0.9846      6282\nweighted avg     0.9960    0.9960    0.9960      6282\n\n","output_type":"stream"},{"execution_count":43,"output_type":"execute_result","data":{"text/plain":"<Artifact Full_Logs>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 800x800 with 1 Axes>","image/png":"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"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 800x800 with 1 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.python.framework.convert_to_constants import  convert_variables_to_constants_v2_as_graph\n\n\ndef get_flops(model):\n    concrete = tf.function(lambda inputs: model(inputs))\n    concrete_func = concrete.get_concrete_function(\n        [tf.TensorSpec([1, *inputs.shape[1:]]) for inputs in model.inputs])\n    frozen_func, graph_def = convert_variables_to_constants_v2_as_graph(concrete_func)\n    with tf.Graph().as_default() as graph:\n        tf.graph_util.import_graph_def(graph_def, name='')\n        run_meta = tf.compat.v1.RunMetadata()\n        opts = tf.compat.v1.profiler.ProfileOptionBuilder.float_operation()\n        flops = tf.compat.v1.profiler.profile(graph=graph, run_meta=run_meta, cmd=\"op\", options=opts)\n        return flops.total_float_ops\n\nmodel = dpm_sacc()\n\nprint(\"The FLOPs is:{}\".format(get_flops(model)) ,flush=True )\n\n","metadata":{"execution":{"iopub.status.busy":"2023-10-28T18:07:48.265092Z","iopub.execute_input":"2023-10-28T18:07:48.265848Z","iopub.status.idle":"2023-10-28T18:07:49.247930Z","shell.execute_reply.started":"2023-10-28T18:07:48.265818Z","shell.execute_reply":"2023-10-28T18:07:49.246905Z"},"trusted":true},"execution_count":44,"outputs":[{"name":"stdout","text":"\n=========================Options=============================\n-max_depth                  10000\n-min_bytes                  0\n-min_peak_bytes             0\n-min_residual_bytes         0\n-min_output_bytes           0\n-min_micros                 0\n-min_accelerator_micros     0\n-min_cpu_micros             0\n-min_params                 0\n-min_float_ops              1\n-min_occurrence             0\n-step                       -1\n-order_by                   float_ops\n-account_type_regexes       .*\n-start_name_regexes         .*\n-trim_name_regexes          \n-show_name_regexes          .*\n-hide_name_regexes          \n-account_displayed_op_only  true\n-select                     float_ops\n-output                     stdout:\n\n==================Model Analysis Report======================\n\nDoc:\nop: The nodes are operation kernel type, such as MatMul, Conv2D. Graph nodes belonging to the same type are aggregated together.\nflops: Number of float operations. Note: Please read the implementation for the math behind it.\n\nProfile:\nnode name | # float_ops\nConv2D                   74.45m float_ops (100.00%, 89.97%)\nDepthwiseConv2dNative    6.73m float_ops (10.03%, 8.13%)\nBiasAdd                  1.08m float_ops (1.89%, 1.31%)\nMatMul                   480.67k float_ops (0.58%, 0.58%)\nSoftmax                  2.58k float_ops (0.00%, 0.00%)\nMul                        512 float_ops (0.00%, 0.00%)\n\n======================End of Report==========================\nThe FLOPs is:82744186\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2023-10-28T18:07:49.249420Z","iopub.execute_input":"2023-10-28T18:07:49.249728Z","iopub.status.idle":"2023-10-28T18:07:49.427726Z","shell.execute_reply.started":"2023-10-28T18:07:49.249700Z","shell.execute_reply":"2023-10-28T18:07:49.426628Z"},"trusted":true},"execution_count":45,"outputs":[{"name":"stdout","text":"Model: \"model_8\"\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to                     \n==================================================================================================\n inp1 (InputLayer)              [(None, 128, 128, 1  0           []                               \n                                )]                                                                \n                                                                                                  \n conv2d_64 (Conv2D)             (None, 64, 64, 32)   320         ['inp1[0][0]']                   \n                                                                                                  \n conv2d_65 (Conv2D)             (None, 64, 64, 32)   832         ['inp1[0][0]']                   \n                                                                                                  \n batch_normalization_112 (Batch  (None, 64, 64, 32)  128         ['conv2d_64[0][0]']              \n Normalization)                                                                                   \n                                                                                                  \n batch_normalization_113 (Batch  (None, 64, 64, 32)  128         ['conv2d_65[0][0]']              \n Normalization)                                                                                   \n                                                                                                  \n re_lu_112 (ReLU)               (None, 64, 64, 32)   0           ['batch_normalization_112[0][0]']\n                                                                                                  \n re_lu_113 (ReLU)               (None, 64, 64, 32)   0           ['batch_normalization_113[0][0]']\n                                                                                                  \n concatenate_16 (Concatenate)   (None, 64, 64, 64)   0           ['re_lu_112[0][0]',              \n                                                                  're_lu_113[0][0]']              \n                                                                                                  \n depthwise_conv2d_48 (Depthwise  (None, 64, 64, 64)  640         ['concatenate_16[0][0]']         \n Conv2D)                                                                                          \n                                                                                                  \n batch_normalization_114 (Batch  (None, 64, 64, 64)  256         ['depthwise_conv2d_48[0][0]']    \n Normalization)                                                                                   \n                                                                                                  \n re_lu_114 (ReLU)               (None, 64, 64, 64)   0           ['batch_normalization_114[0][0]']\n                                                                                                  \n conv2d_66 (Conv2D)             (None, 64, 64, 64)   4160        ['re_lu_114[0][0]']              \n                                                                                                  \n batch_normalization_115 (Batch  (None, 64, 64, 64)  256         ['conv2d_66[0][0]']              \n Normalization)                                                                                   \n                                                                                                  \n re_lu_115 (ReLU)               (None, 64, 64, 64)   0           ['batch_normalization_115[0][0]']\n                                                                                                  \n depthwise_conv2d_49 (Depthwise  (None, 32, 32, 64)  640         ['re_lu_115[0][0]']              \n Conv2D)                                                                                          \n                                                                                                  \n batch_normalization_116 (Batch  (None, 32, 32, 64)  256         ['depthwise_conv2d_49[0][0]']    \n Normalization)                                                                                   \n                                                                                                  \n re_lu_116 (ReLU)               (None, 32, 32, 64)   0           ['batch_normalization_116[0][0]']\n                                                                                                  \n conv2d_67 (Conv2D)             (None, 32, 32, 128)  8320        ['re_lu_116[0][0]']              \n                                                                                                  \n batch_normalization_117 (Batch  (None, 32, 32, 128)  512        ['conv2d_67[0][0]']              \n Normalization)                                                                                   \n                                                                                                  \n re_lu_117 (ReLU)               (None, 32, 32, 128)  0           ['batch_normalization_117[0][0]']\n                                                                                                  \n dropout_32 (Dropout)           (None, 32, 32, 128)  0           ['re_lu_117[0][0]']              \n                                                                                                  \n depthwise_conv2d_50 (Depthwise  (None, 16, 16, 128)  1280       ['dropout_32[0][0]']             \n Conv2D)                                                                                          \n                                                                                                  \n batch_normalization_118 (Batch  (None, 16, 16, 128)  512        ['depthwise_conv2d_50[0][0]']    \n Normalization)                                                                                   \n                                                                                                  \n re_lu_118 (ReLU)               (None, 16, 16, 128)  0           ['batch_normalization_118[0][0]']\n                                                                                                  \n conv2d_68 (Conv2D)             (None, 16, 16, 128)  16512       ['re_lu_118[0][0]']              \n                                                                                                  \n batch_normalization_119 (Batch  (None, 16, 16, 128)  512        ['conv2d_68[0][0]']              \n Normalization)                                                                                   \n                                                                                                  \n re_lu_119 (ReLU)               (None, 16, 16, 128)  0           ['batch_normalization_119[0][0]']\n                                                                                                  \n depthwise_conv2d_51 (Depthwise  (None, 8, 8, 128)   1280        ['re_lu_119[0][0]']              \n Conv2D)                                                                                          \n                                                                                                  \n batch_normalization_120 (Batch  (None, 8, 8, 128)   512         ['depthwise_conv2d_51[0][0]']    \n Normalization)                                                                                   \n                                                                                                  \n re_lu_120 (ReLU)               (None, 8, 8, 128)    0           ['batch_normalization_120[0][0]']\n                                                                                                  \n conv2d_69 (Conv2D)             (None, 8, 8, 256)    33024       ['re_lu_120[0][0]']              \n                                                                                                  \n batch_normalization_121 (Batch  (None, 8, 8, 256)   1024        ['conv2d_69[0][0]']              \n Normalization)                                                                                   \n                                                                                                  \n re_lu_121 (ReLU)               (None, 8, 8, 256)    0           ['batch_normalization_121[0][0]']\n                                                                                                  \n dropout_33 (Dropout)           (None, 8, 8, 256)    0           ['re_lu_121[0][0]']              \n                                                                                                  \n depthwise_conv2d_52 (Depthwise  (None, 4, 4, 256)   2560        ['dropout_33[0][0]']             \n Conv2D)                                                                                          \n                                                                                                  \n batch_normalization_122 (Batch  (None, 4, 4, 256)   1024        ['depthwise_conv2d_52[0][0]']    \n Normalization)                                                                                   \n                                                                                                  \n re_lu_122 (ReLU)               (None, 4, 4, 256)    0           ['batch_normalization_122[0][0]']\n                                                                                                  \n conv2d_70 (Conv2D)             (None, 4, 4, 256)    65792       ['re_lu_122[0][0]']              \n                                                                                                  \n batch_normalization_123 (Batch  (None, 4, 4, 256)   1024        ['conv2d_70[0][0]']              \n Normalization)                                                                                   \n                                                                                                  \n re_lu_123 (ReLU)               (None, 4, 4, 256)    0           ['batch_normalization_123[0][0]']\n                                                                                                  \n depthwise_conv2d_53 (Depthwise  (None, 2, 2, 256)   2560        ['re_lu_123[0][0]']              \n Conv2D)                                                                                          \n                                                                                                  \n batch_normalization_124 (Batch  (None, 2, 2, 256)   1024        ['depthwise_conv2d_53[0][0]']    \n Normalization)                                                                                   \n                                                                                                  \n re_lu_124 (ReLU)               (None, 2, 2, 256)    0           ['batch_normalization_124[0][0]']\n                                                                                                  \n conv2d_71 (Conv2D)             (None, 2, 2, 256)    65792       ['re_lu_124[0][0]']              \n                                                                                                  \n batch_normalization_125 (Batch  (None, 2, 2, 256)   1024        ['conv2d_71[0][0]']              \n Normalization)                                                                                   \n                                                                                                  \n re_lu_125 (ReLU)               (None, 2, 2, 256)    0           ['batch_normalization_125[0][0]']\n                                                                                                  \n inp2 (InputLayer)              [(None, 2)]          0           []                               \n                                                                                                  \n global_max_pooling2d_8 (Global  (None, 256)         0           ['re_lu_125[0][0]']              \n MaxPooling2D)                                                                                    \n                                                                                                  \n metadata_feature_dense_1 (Dens  (None, 8)           24          ['inp2[0][0]']                   \n e)                                                                                               \n                                                                                                  \n dropout_34 (Dropout)           (None, 256)          0           ['global_max_pooling2d_8[0][0]'] \n                                                                                                  \n concatenate_17 (Concatenate)   (None, 10)           0           ['metadata_feature_dense_1[0][0]'\n                                                                 , 'inp2[0][0]']                  \n                                                                                                  \n sacc_layer_8 (SACCLayer)       (None, 256)          199936      ['dropout_34[0][0]',             \n                                                                  'concatenate_17[0][0]']         \n                                                                                                  \n combine_feature_dense_1 (Dense  (None, 128)         32896       ['sacc_layer_8[0][0]']           \n )                                                                                                \n                                                                                                  \n combine_feature_dense_2 (Dense  (None, 64)          8256        ['combine_feature_dense_1[0][0]']\n )                                                                                                \n                                                                                                  \n dropout_35 (Dropout)           (None, 64)           0           ['combine_feature_dense_2[0][0]']\n                                                                                                  \n target10 (Dense)               (None, 3)            195         ['dropout_35[0][0]']             \n                                                                                                  \n==================================================================================================\nTotal params: 453,211\nTrainable params: 449,115\nNon-trainable params: 4,096\n__________________________________________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"tf.keras.utils.plot_model(\n    dpm_sacc(),\n    to_file='model.png',\n    show_shapes=True,\n    show_dtype=False,\n    show_layer_names=False,\n    rankdir='TB',\n    expand_nested=False,\n    dpi=96,\n    layer_range=None,\n    show_layer_activations=True,\n    show_trainable=True\n)\n\nrun_.finish()","metadata":{"execution":{"iopub.status.busy":"2023-10-28T18:07:54.233176Z","iopub.execute_input":"2023-10-28T18:07:54.233548Z","iopub.status.idle":"2023-10-28T18:08:00.459333Z","shell.execute_reply.started":"2023-10-28T18:07:54.233519Z","shell.execute_reply":"2023-10-28T18:08:00.458589Z"},"trusted":true},"execution_count":46,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>▁▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇███████████████████</td></tr><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss</td><td>█▅▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>███████▇▇▇▇▇▇▆▆▆▆▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>precision</td><td>▁▄▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇████████████████</td></tr><tr><td>sensitivity</td><td>▁▅▅▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇███████████████████</td></tr><tr><td>val_accuracy</td><td>▁█████▇█████████████████████████████████</td></tr><tr><td>val_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_precision</td><td>▁█████▇█████████████████████████████████</td></tr><tr><td>val_sensitivity</td><td>▁███▇█▇█████████████████████████████████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>0.99834</td></tr><tr><td>best_epoch</td><td>19</td></tr><tr><td>best_val_loss</td><td>0.02791</td></tr><tr><td>epoch</td><td>39</td></tr><tr><td>loss</td><td>0.01034</td></tr><tr><td>lr</td><td>2e-05</td></tr><tr><td>precision</td><td>0.99751</td></tr><tr><td>sensitivity</td><td>0.99751</td></tr><tr><td>val_accuracy</td><td>0.99634</td></tr><tr><td>val_loss</td><td>0.03545</td></tr><tr><td>val_precision</td><td>0.99459</td></tr><tr><td>val_sensitivity</td><td>0.99443</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">logical-sponge-195</strong> at: <a href='https://wandb.ai/shreya-srivas02/ECG_BEAT_CLASSIFICATION_PAPER_COMMENTS/runs/9vnf4cg5' target=\"_blank\">https://wandb.ai/shreya-srivas02/ECG_BEAT_CLASSIFICATION_PAPER_COMMENTS/runs/9vnf4cg5</a><br/>Synced 6 W&B file(s), 12 media file(s), 43 artifact file(s) and 1 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>/root/wandb/run-20231028_172013-9vnf4cg5/logs</code>"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}