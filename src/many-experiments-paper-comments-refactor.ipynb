{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install tensorflow==2.8.0\n!pip install efficientnet\n!pip install --upgrade wandb","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!tree / > files.txt","metadata":{"execution":{"iopub.status.busy":"2023-10-18T15:50:25.547937Z","iopub.execute_input":"2023-10-18T15:50:25.548308Z","iopub.status.idle":"2023-10-18T15:50:34.930769Z","shell.execute_reply.started":"2023-10-18T15:50:25.548276Z","shell.execute_reply":"2023-10-18T15:50:34.928895Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# !pip install wandb==0.15.0 ","metadata":{"execution":{"iopub.status.busy":"2023-10-17T13:51:27.727440Z","iopub.execute_input":"2023-10-17T13:51:27.727818Z","iopub.status.idle":"2023-10-17T13:51:27.733490Z","shell.execute_reply.started":"2023-10-17T13:51:27.727786Z","shell.execute_reply":"2023-10-17T13:51:27.732442Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import os\nprint(os.environ)","metadata":{"execution":{"iopub.status.busy":"2023-10-18T05:42:46.131379Z","iopub.execute_input":"2023-10-18T05:42:46.131718Z","iopub.status.idle":"2023-10-18T05:42:46.136544Z","shell.execute_reply.started":"2023-10-18T05:42:46.131690Z","shell.execute_reply":"2023-10-18T05:42:46.135683Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"environ({'LIBRARY_PATH': ':/opt/conda/lib', 'PIP_ROOT_USER_ACTION': 'ignore', 'MKL_THREADING_LAYER': 'GNU', 'KAGGLE_URL_BASE': 'https://www.kaggle.com', 'KAGGLE_KERNEL_RUN_TYPE': 'Interactive', 'MPLBACKEND': 'agg', 'HOSTNAME': '30cf356e8ea7', 'GCSFUSE_METADATA_IMAGE_TYPE': 'DLC', 'SHLVL': '1', 'LD_LIBRARY_PATH': '/usr/local/cuda/lib64:/usr/local/cuda/lib:/usr/local/lib/x86_64-linux-gnu:/usr/local/nvidia/lib:/usr/local/nvidia/lib64::/opt/conda/lib', 'GIT_COMMIT': '5b37fe29fefdef02f55a8db1fff9f9b63826f777', 'ENABLE_MULTI_ENV': 'false', 'HOME': '/root', 'TENSORFLOW_VERSION': '2.12.0', 'KMP_SETTINGS': 'false', 'TESSERACT_PATH': '/usr/bin/tesseract', 'KAGGLE_DATA_PROXY_URL': 'https://dp.kaggle.net', 'NODE_OPTIONS': '--max-old-space-size=4096', 'KMP_BLOCKTIME': '0', 'TF_CPP_MIN_LOG_LEVEL': '2', 'BUILD_DATE': '20230905-170153', 'KAGGLE_GCP_ZONE': 'us-west1-b', 'KAGGLE_CONTAINER_NAME': 'kaggle_lDlHbyQJiZMMpzCIlr8Me60t48VYLTLYZz29qJctkoY-147033544-webtier', '_': '/opt/conda/bin/jupyter', 'ANACONDA_PYTHON_VERSION': '3.10', 'TENSORFLOW_IO_VERSION': '0.32.0', 'PATH': '/opt/conda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin', 'DL_ANACONDA_HOME': '/opt/conda', 'KMP_AFFINITY': 'granularity=fine,verbose,compact,1,0', 'LANG': 'C.UTF-8', 'PYTHONUSERBASE': '/root/.local', 'CONTAINER_NAME': 'tf2-cpu/2-12', 'KAGGLE_KERNEL_INTEGRATIONS': '', 'SHELL': '/bin/bash', 'KAGGLE_DATA_PROXY_PROJECT': 'kaggle-161607', 'KAGGLE_DATA_PROXY_TOKEN': 'eyJhbGciOiJBMTI4S1ciLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0.tACfmBWBM_RZrUPvzOkBk4wnJbFawVMtI-IvejwUBSLfEh3K6ufvCA.wZpXvfZVw_JkiZ2F8sbICg.yfpRQaAy1dJuaUxCtClVnxWBQBN6Zrj5Mf3Y9yaqIxcr6IsGuK37rgkzjI_LWFfu3U2A3si-afmh-6wo_no3yMMkYiAVQo_TmkZMWo6QFKn4PwPtu_7mUXyt-64GyovW2PYWMj7_O5-fQFdVBsSwbUxtbYRI-oatFOC7eErfzr36rshE78UiVFBob0HqRX5_Vr16mXo79_MMhRK_9knCrXZRrh7yDQsmfyh3b5g3rpx9esNUzGVt5RTLCfPnK6xImjpM-qs41zypWhrCi6kkupVAXVQV97LYLRIbxKhqtQ0.CcGPbnRM7w4ELYlAlPtEpA', 'PWD': '/kaggle/working', 'KAGGLE_USER_SECRETS_TOKEN': 'eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..XWb50XmXKQYj5p5_fg-TQA.HGmiIQQTt_Hb4w-zQid0JTw4N9A-PR5CnLcc1BjHDw8UuvYms_jKMTjEIYkOam6sjKWI0FyE8hoSICUEAmIFOTr6ukdOcFJ8z5HJ7DGebCDlUVC7D7x0y5YkSXz90_s7lBhbWAxEYW8Nxgu5__9KCQ.LS5VtRS7CJnXd8LzEbH16A', 'PROJ_LIB': '/opt/conda/share/proj', 'LC_ALL': 'C.UTF-8', 'PYTHONPATH': '/kaggle/lib/kagglegym:/kaggle/lib', 'KMP_WARNINGS': '0', 'KAGGLE_DOCKER_IMAGE': 'gcr.io/kaggle-images/python@sha256:06be44bb0cdd920e11adfa5aac1927b61741655325d2d62aa18a300c7cc2e8e8', 'GIT_PYTHON_REFRESH': 'quiet', 'PYDEVD_USE_FRAME_EVAL': 'NO', 'JPY_PARENT_PID': '9', 'TERM': 'xterm-color', 'CLICOLOR': '1', 'FORCE_COLOR': '1', 'CLICOLOR_FORCE': '1', 'PAGER': 'cat', 'GIT_PAGER': 'cat'})\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport re\nimport random\nimport pandas as pd\nimport numpy as np\nimport json\nimport math\nimport string\nimport uuid\n\n\n### Tensorflow Imports\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import roc_auc_score,confusion_matrix\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras.applications import MobileNet\n\n\n### External models\nimport efficientnet.tfkeras as efn\n\n\n### Matplotlib Imports\nimport matplotlib\nimport matplotlib.pyplot as plt\n\n\n### Kaggle specific imports\nif os.environ.get('KAGGLE_DOCKER_IMAGE', None):\n    from kaggle_datasets import KaggleDatasets\n    DATA_PATH = KaggleDatasets().get_gcs_path()\n    DATA_PATH = DATA_PATH + '/mitdb128x128'\n","metadata":{"execution":{"iopub.status.busy":"2023-10-18T15:34:24.675495Z","iopub.execute_input":"2023-10-18T15:34:24.675820Z","iopub.status.idle":"2023-10-18T15:34:37.372543Z","shell.execute_reply.started":"2023-10-18T15:34:24.675795Z","shell.execute_reply":"2023-10-18T15:34:37.370930Z"},"trusted":true},"execution_count":1,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 21\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapplications\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MobileNet\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m### External models\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mefficientnet\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtfkeras\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mefn\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m### Matplotlib Imports\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'efficientnet'"],"ename":"ModuleNotFoundError","evalue":"No module named 'efficientnet'","output_type":"error"}]},{"cell_type":"code","source":"hparams = {\n    \"backbone\" : \"b0\",\n    \"batch_size\" : 32,\n    \"epochs\" : 35,\n    \"img_size\" : 128,\n    \"lr\" : 0.0029,\n    \"optimizer\" : \"adam\",\n    \"seed\": 127\n}","metadata":{"execution":{"iopub.status.busy":"2023-10-18T15:34:43.763237Z","iopub.execute_input":"2023-10-18T15:34:43.763642Z","iopub.status.idle":"2023-10-18T15:34:43.768292Z","shell.execute_reply.started":"2023-10-18T15:34:43.763604Z","shell.execute_reply":"2023-10-18T15:34:43.767294Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import wandb\nimport os\n\nclass WandBConfigurations():\n    def __init__(self, exp_name = \"ECG_BEAT_CLASSIFICATION_PAPER_COMMENTS\"):\n        self.EXPERIMENT_NAME = exp_name\n        os.environ[\"WANDB_API_KEY\"] = \"221507f411c2ddcc0c17238e115a12c528a482f6\"\n#         os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"\"\n        wandb.login()\n        \nWB = WandBConfigurations()","metadata":{"execution":{"iopub.status.busy":"2023-10-18T15:34:44.665838Z","iopub.execute_input":"2023-10-18T15:34:44.666181Z","iopub.status.idle":"2023-10-18T15:34:48.470703Z","shell.execute_reply.started":"2023-10-18T15:34:44.666156Z","shell.execute_reply":"2023-10-18T15:34:48.469881Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mshreya-srivas02\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"}]},{"cell_type":"code","source":"!tree /kaggle","metadata":{"execution":{"iopub.status.busy":"2023-10-18T05:52:57.183239Z","iopub.execute_input":"2023-10-18T05:52:57.183891Z","iopub.status.idle":"2023-10-18T05:52:58.356263Z","shell.execute_reply.started":"2023-10-18T05:52:57.183822Z","shell.execute_reply":"2023-10-18T05:52:58.355194Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"\u001b[01;34m/kaggle\u001b[00m\n├── \u001b[01;34minput\u001b[00m\n│   └── \u001b[01;34mmitdb128x128\u001b[00m\n│       └── \u001b[01;34mmitdb_128x128\u001b[00m\n│           ├── data-mitdb.csv\n│           ├── data-mitdb.json\n│           ├── \u001b[01;35msample_image_for_heart_beat.jpeg\u001b[00m\n│           ├── splitted_data.csv\n│           ├── trainfile_class10_fold0_6117.tfrec\n│           ├── trainfile_class10_fold1_6116.tfrec\n│           ├── trainfile_class10_fold2_6116.tfrec\n│           ├── trainfile_class10_fold3_6116.tfrec\n│           ├── trainfile_class10_fold4_6116.tfrec\n│           ├── trainfile_class10_fold5_6116.tfrec\n│           ├── trainfile_class10_fold6_6116.tfrec\n│           ├── trainfile_class10_fold7_6117.tfrec\n│           ├── trainfile_class10_fold8_6116.tfrec\n│           └── trainfile_class10_fold9_6116.tfrec\n├── \u001b[01;34mlib\u001b[00m\n│   └── \u001b[01;34mkaggle\u001b[00m\n│       └── gcp.py\n└── \u001b[01;34mworking\u001b[00m\n\n6 directories, 15 files\n","output_type":"stream"}]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"  run_ = wandb.init(\n      project= WB.EXPERIMENT_NAME,\n      reinit=True,\n      dir = \"/root\",\n      allow_val_change = True\n  )","metadata":{"execution":{"iopub.status.busy":"2023-10-18T15:38:39.050788Z","iopub.execute_input":"2023-10-18T15:38:39.051196Z","iopub.status.idle":"2023-10-18T15:39:11.171162Z","shell.execute_reply.started":"2023-10-18T15:38:39.051165Z","shell.execute_reply":"2023-10-18T15:39:11.170099Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.15.12 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.15.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/root/wandb/run-20231018_153839-27pghrhy</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/shreya-srivas02/ECG_BEAT_CLASSIFICATION_PAPER_COMMENTS/runs/27pghrhy' target=\"_blank\">skilled-sky-69</a></strong> to <a href='https://wandb.ai/shreya-srivas02/ECG_BEAT_CLASSIFICATION_PAPER_COMMENTS' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/shreya-srivas02/ECG_BEAT_CLASSIFICATION_PAPER_COMMENTS' target=\"_blank\">https://wandb.ai/shreya-srivas02/ECG_BEAT_CLASSIFICATION_PAPER_COMMENTS</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/shreya-srivas02/ECG_BEAT_CLASSIFICATION_PAPER_COMMENTS/runs/27pghrhy' target=\"_blank\">https://wandb.ai/shreya-srivas02/ECG_BEAT_CLASSIFICATION_PAPER_COMMENTS/runs/27pghrhy</a>"},"metadata":{}}]},{"cell_type":"code","source":"wandb.save(\"kaggle.ipynb\")","metadata":{"execution":{"iopub.status.busy":"2023-10-18T15:39:17.514442Z","iopub.execute_input":"2023-10-18T15:39:17.514796Z","iopub.status.idle":"2023-10-18T15:39:17.524898Z","shell.execute_reply.started":"2023-10-18T15:39:17.514767Z","shell.execute_reply":"2023-10-18T15:39:17.523633Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"[]"},"metadata":{}}]},{"cell_type":"code","source":"!tree / |","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef id_generator(size=6):\n    return str(uuid.uuid4())[:size]\nMODELS = []\n\nhparams = {\n    \"backbone\" : \"b0\",\n    \"batch_size\" : 32,\n    \"epochs\" : 35,\n    \"img_size\" : 128,\n    \"lr\" : 0.0029,\n    \"optimizer\" : \"adam\",\n    \"seed\": 127\n}\n\ndef get_optimizer():\n    if hparams['optimizer'] == 'adam':\n        return tf.keras.optimizers.Adam(learning_rate=hparams[\"lr\"])\n    if hparams['optimizer'] == 'rmsprop':\n        return tf.keras.optimizers.RMSprop(learning_rate=hparams[\"lr\"])\n    if hparams['optimizer'] == 'adagrad':\n        return tf.keras.optimizers.Adagrad(learning_rate=hparams[\"lr\"])\n    if hparams['optimizer'] == 'adadelta':\n        return tf.keras.optimizers.Adadelta(learning_rate=hparams[\"lr\"])\n\n    return tf.keras.optimizers.Adam(learning_rate=hparams[\"lr\"])\n\nDO_VAL_SPLIT = True\nTRAIN_FILES = sorted(tf.io.gfile.glob('/kaggle/input/mitdb128x128/mitdb_128x128/train*.tfrec'))[:-1]\nTOTAL_TRAIN_IMG = 48929\nTOTAL_VAL_IMG = 6116\nTOTAL_TEST_IMG = 6116\nBACKBONE = hparams['backbone']\nIMG_TRAIN_SHAPE = [hparams[\"img_size\"],hparams[\"img_size\"]]\nDO_FINETUNE = True\nBATCH_SIZE = hparams[\"batch_size\"] # 16\nEPOCHES = hparams[\"epochs\"]\nSEED = hparams[\"seed\"]\nLOSS = tf.keras.losses.CategoricalCrossentropy()\nOPTIMIZER = get_optimizer()\nACCURACY = []\nCALLBACKS = []\nSTRATEGY = None\nFOLDS = 9\n\nprint(TRAIN_FILES)\n\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n    STRATEGY = strategy\n    BATCH_SIZE = 8 * strategy.num_replicas_in_sync\n    # wandb.config.hardware = 'TPU'\nelse:\n    strategy = tf.distribute.get_strategy()\n\n\ndef seed_everything():\n    np.random.seed(SEED)\n    tf.random.set_seed(SEED)\n    random.seed(a=SEED)\n    os.environ['PYTHONHASHSEED'] = str(SEED)\n\nseed_everything()\n\n\ndef process_training_data(data_file):\n    LABELED_TFREC_FORMAT = {\n        \"image\": tf.io.FixedLenFeature([], tf.string),\n        'target10': tf.io.FixedLenFeature([], tf.int64),\n        'gender' : tf.io.FixedLenFeature([], tf.int64),\n        'age_interval' : tf.io.FixedLenFeature([], tf.int64),\n    }\n    data = tf.io.parse_single_example(data_file, LABELED_TFREC_FORMAT)\n    img = tf.image.decode_jpeg(data['image'], channels=1)\n    img = tf.cast(img, tf.float32) / 255.0\n    img = tf.reshape(img, [*IMG_TRAIN_SHAPE, 1])\n\n    age = tf.cast(data['age_interval'], tf.float32) / 10.0\n    sex = tf.cast(data['gender'], tf.float32) / 1.0\n    tab_data = [tf.cast(tfeat, dtype = tf.float32) for tfeat in [age, sex]]\n    tabular_data = tf.stack(tab_data)\n\n    target10 = tf.one_hot(data['target10'], depth=10)\n\n    return {'inp1' : img, 'inp2' : tabular_data}, {\"target10\" : target10 }\n\ndef get_cosine_schedule_with_warmup(lr = 0.00004, num_warmup_steps = 0 , num_training_steps = EPOCHES, num_cycles=0.5):\n    def lrfn(epoch):\n        if epoch < num_warmup_steps:\n            return (float(epoch) / float(max(1, num_warmup_steps))) * lr\n        progress = float(epoch - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))\n        return max(0.0, 0.5 * (1.0 + math.cos(math.pi * float(num_cycles) * 2.0 * progress))) * lr\n\n    return lrfn\n\nlrfn = get_cosine_schedule_with_warmup(lr=hparams['lr'])\nlr_schedule = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=0)\nCALLBACKS.append(lr_schedule)\n\n\ndef se_block(x, ratio=16):\n    \"\"\"Creates a squeeze and excitation block.\"\"\"\n    channels = x.shape[-1]\n    se_shape = (1, 1, channels)\n    \n    se = tf.keras.layers.GlobalAveragePooling2D()(x)\n    se = tf.keras.layers.Reshape(se_shape)(se)\n    se = tf.keras.layers.Dense(channels // ratio, activation='relu')(se)\n    se = tf.keras.layers.Dense(channels, activation='sigmoid')(se)\n    \n    return tf.keras.layers.multiply([x, se])\n\ndef residual_block(x, filters, kernel_size=3, stride=1):\n    # Shortcut\n    shortcut = x\n\n    # First convolution layer\n    x = tf.keras.layers.Conv2D(filters, kernel_size, strides=stride, padding=\"same\")(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.ReLU()(x)\n\n    # Second convolution layer\n    x = tf.keras.layers.Conv2D(filters, kernel_size, strides=stride, padding=\"same\")(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n                           \n    x = se_block(x)\n\n    # Adding the shortcut to the output\n    x = tf.keras.layers.Add()([x, shortcut])\n    x = tf.keras.layers.ReLU()(x)\n\n    return x\n\nfrom tensorflow.keras.layers import Input, Conv1D, Add, Activation\n\n# Define the residual block for the TCN\ndef residual_block(x, filters, kernel_size=3, dilation_rate=1):\n    prev_x = x\n    for k in range(2):\n        x = Conv1D(filters=filters,\n                   kernel_size=kernel_size,\n                   dilation_rate=dilation_rate,\n                   padding='causal',\n                   activation='relu')(x)  # 'causal' for time-series\n        \n    # Ensure the output and input are of the same shape\n    if x.shape[-1] != prev_x.shape[-1]:\n        prev_x = Conv1D(filters=filters,\n                        kernel_size=1)(prev_x)\n        \n    return Add()([prev_x, x])\n\nfrom tensorflow.keras.layers import UpSampling1D, Add, Conv2D\n\ndef create_fpn(features, num_channels=32):\n    pyramid = []\n    for feature in features[::-1]:\n        if pyramid:\n            feature = Add()([UpSampling1D()(pyramid[-1]), feature])\n        feature = Conv1D(num_channels, 32, padding='same')(feature)\n        pyramid.append(feature)\n    return pyramid[::-1]\n\nfrom tensorflow.keras.layers import Layer\n\nclass SelfAttention(Layer):\n    def __init__(self, head_size, num_heads):\n        super(SelfAttention, self).__init__()\n        self.head_size = head_size\n        self.num_heads = num_heads\n        \n    def build(self, input_shape):\n        self.w_q = self.add_weight(name='w_q', shape=(input_shape[-1], self.head_size * self.num_heads))\n        self.w_k = self.add_weight(name='w_k', shape=(input_shape[-1], self.head_size * self.num_heads))\n        self.w_v = self.add_weight(name='w_v', shape=(input_shape[-1], self.head_size * self.num_heads))\n        super(SelfAttention, self).build(input_shape)\n        \n    def call(self, inputs):\n        q = tf.matmul(inputs, self.w_q)\n        k = tf.matmul(inputs, self.w_k)\n        v = tf.matmul(inputs, self.w_v)\n        \n        q = tf.reshape(q, [-1, self.num_heads, self.head_size])\n        k = tf.reshape(k, [-1, self.num_heads, self.head_size])\n        v = tf.reshape(v, [-1, self.num_heads, self.head_size])\n        \n        attn = tf.nn.softmax(tf.matmul(q, k, transpose_b=True) / tf.sqrt(float(self.head_size)))\n        output = tf.matmul(attn, v)\n        output = tf.reshape(output, [-1, self.head_size * self.num_heads])\n        return output\n\n\nclass SelfAttentiveCCALayer(tf.keras.layers.Layer):\n    def __init__(self,output_dim, **kwargs):\n        self.output_dim = output_dim\n        super(SelfAttentiveCCALayer, self).__init__(**kwargs)\n        self.self_attention1 = SelfAttention(head_size=256, num_heads=1)\n        \n    def build(self, input_shape):\n        # Create a trainable weight variable for each modality\n        self.kernel_1 = self.add_weight(name='kernel_1',\n                                        shape=(input_shape[0][-1], input_shape[0][-1]),\n                                        initializer=tf.keras.initializers.Identity(),\n                                        trainable=True)\n        self.kernel_2 = self.add_weight(name='kernel_2',\n                                        shape=(input_shape[1][-1], input_shape[1][-1]),\n                                        initializer=tf.keras.initializers.Identity(),\n                                        trainable=False)\n        self.weight1 = self.add_weight(name='weight1',\n                                             shape=(1,),\n                                             initializer=tf.keras.initializers.Zeros(),\n                                             trainable=True)\n        self.weight2 = self.add_weight(name='weight2',\n                                             shape=(1,),\n                                             initializer=tf.keras.initializers.Zeros(),\n                                             trainable=True)\n        super(SelfAttentiveCCALayer, self).build(input_shape)\n\n    def call(self, inputs):\n        attn_out1 = self.self_attention1(inputs[0])\n        proj_1 = K.dot(attn_out1, self.kernel_1)\n        proj_1 = tf.nn.sigmoid(proj_1)\n        proj_2 = K.dot(inputs[1], self.kernel_2)\n        combined = [self.weight1 * proj_1, self.weight2 * proj_2]\n        return tf.keras.layers.concatenate(combined)\n    \n    def compute_output_shape(self, input_shape):\n        return (input_shape[0][0] + input_shape[1][0], input_shape[0][-1] + input_shape[1][-1])\n\nclass CCALayer(tf.keras.layers.Layer):\n    def __init__(self,output_dim, **kwargs):\n        self.output_dim = output_dim\n        super(CCALayer, self).__init__(**kwargs)\n        \n    def build(self, input_shape):\n        # Create a trainable weight variable for each modality\n        self.kernel_1 = self.add_weight(name='kernel_1',\n                                        shape=(input_shape[0][-1],  self.output_dim),\n                                        initializer=tf.keras.initializers.Identity(),\n                                        trainable=True)\n        self.kernel_2 = self.add_weight(name='kernel_2',\n                                        shape=(input_shape[1][-1],  self.output_dim),\n                                        initializer=tf.keras.initializers.Identity(),\n                                        trainable=True)\n#         self.weight1 = self.add_weight(name='weight1',\n#                                              shape=(1,),\n#                                              initializer=tf.keras.initializers.Zeros(),\n#                                              trainable=True)\n#         self.weight2 = self.add_weight(name='weight2',\n#                                              shape=(1,),\n#                                              initializer=tf.keras.initializers.Zeros(),\n#                                              trainable=True)\n        super(CCALayer, self).build(input_shape)\n\n    def call(self, inputs):\n        # inputs is a list of two feature tensors: [features_1, features_2]\n        proj_1 = K.dot(inputs[0], self.kernel_1)\n#         proj_1 = tf.nn.sigmoid(proj_1)\n        proj_2 = K.dot(inputs[1], self.kernel_2)\n#         combined = [self.weight1 * proj_1, self.weight2 * proj_2]\n        combined = [proj_1, proj_2]\n#         proj_2 = K.dot(inputs[1], self.kernel_2)\n#         self.add_loss(-tf.reduce_mean(tf.linalg.diag_part(tf.matmul(proj_1, proj_2, transpose_a=True))) / self.output_dim)  # Maximize correlation\n        return tf.keras.layers.concatenate(combined)\n#         return self.combine_weight * proj_1\n#         return combined\n    \n    def compute_output_shape(self, input_shape):\n#         return [(input_shape[0][0], self.output_dim), (input_shape[1][0], self.output_dim)]\n        return (input_shape[0][0], 2*self.output_dim)\n#         return (input_shape[0][0], self.output_dim)\n\nfrom tensorflow.keras.layers import Input, DepthwiseConv2D, Conv2D, BatchNormalization, ReLU, AvgPool2D, Flatten, Dense\ndef depthwise_separable_conv(x, filters, stride):\n    # Depthwise Convolution\n    x = DepthwiseConv2D((3, 3), strides=stride, padding='same')(x)\n    x = BatchNormalization()(x)\n    x = ReLU()(x)\n\n    # Pointwise Convolution\n    x = Conv2D(filters, (1, 1), strides=(1, 1), padding='same')(x)\n    x = BatchNormalization()(x)\n    x = ReLU()(x)\n\n    return x\n\ndef MYMobileNetV1(inp1):\n    # Initial Conv Layer\n    x = Conv2D(32, (3, 3), strides=(2, 2), padding='same')(inp1)\n    x = BatchNormalization()(x)\n    x = ReLU()(x)\n\n    # Depthwise Separable Convolution Blocks\n    x = depthwise_separable_conv(x, 64, (1, 1))\n    x = depthwise_separable_conv(x, 128, (2, 2))\n    x = depthwise_separable_conv(x, 128, (1, 1))\n    x = depthwise_separable_conv(x, 256, (2, 2))\n#     x = depthwise_separable_conv(x, 256, (1, 1))\n#     x = depthwise_separable_conv(x, 512, (2, 2))\n\n#     # Repeating the same block for additional 5 times\n#     for _ in range(5):\n#         x = depthwise_separable_conv(x, 512, (1, 1))\n\n#     x = depthwise_separable_conv(x, 1024, (2, 2))\n#     x = depthwise_separable_conv(x, 1024, (1, 1))\n\n    return x\n\n\nclass DependencyLayer(Layer):\n    def __init__(self, output_dim, **kwargs):\n        self.output_dim = output_dim\n        super(DependencyLayer, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        self.kernel = self.add_weight(name='kernel',\n                                     shape=(input_shape[1][-1], self.output_dim),\n                                     initializer='uniform',\n                                     trainable=True)\n        super(DependencyLayer, self).build(input_shape)\n\n    def call(self, inputs):\n        input1, input2 = inputs\n        influence = tf.linalg.matmul(input2, self.kernel)\n        influenced_input1 = tf.math.matmul(input1, influence)\n        return influenced_input1\n    \n    def compute_output_shape(self, input_shape):\n#         return [(input_shape[0][0], self.output_dim), (input_shape[1][0], self.output_dim)]\n        return (input_shape[0][-1], input_shape[0][0])\n#         return (input_shape[0][0], self.output_dim)\n\nclass GatingLayer(Layer):\n    def __init__(self, **kwargs):\n        super(GatingLayer, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        self.kernel = self.add_weight(name='kernel',\n                                     shape=(input_shape[1][-1], input_shape[0][-1]),\n                                     initializer='uniform',\n                                     trainable=True)\n        super(GatingLayer, self).build(input_shape)\n\n    def call(self, inputs):\n        image_features, metadata = inputs\n        gating_values = tf.linalg.matmul(metadata, self.kernel)\n        gating_values = tf.nn.sigmoid(gating_values)  # Make gating values sum to 1\n        return image_features * gating_values\n\ndef multimodeleffbx():\n    inp1  = tf.keras.layers.Input(shape = (*IMG_TRAIN_SHAPE, 1), name='inp1')\n    inp2  = tf.keras.layers.Input(shape = (2,), name='inp2')\n    x1 = MYMobileNetV1(inp1)\n    \n    # Assuming you want to apply TCN on the image input, you might need to reshape it\n#     x1 = tf.keras.layers.Reshape((-1, 128))(inp1)  # Adjust the shape based on how you want to apply the TCN\n    \n#     # Apply multiple residual blocks with increasing dilation rates\n#     x1 = residual_block(x1, filters=32, dilation_rate=1)\n#     x2 = residual_block(x1, filters=32, dilation_rate=2)\n#     x3 = residual_block(x2, filters=32, dilation_rate=4)\n#     x1 = residual_block(x3, filters=32, dilation_rate=8)\n        \n#     features = [x1, x2, x3, x4]  # Add your layer names\n    \n# #     # Create the FPN using extracted feature maps\n#     fpn_features = create_fpn(features)\n    \n# #     # You can concatenate FPN features, or use them separately\n#     x1 = tf.keras.layers.Concatenate(axis=-1)(fpn_features)\n#     print(x.output_shape)\n\n#     x1 = tf.keras.layers.Conv2D(64, (3, 3), strides=2, padding=\"same\")(inp1)\n#     x1 = tf.keras.layers.BatchNormalization()(x1)\n#     x1 = tf.keras.layers.ReLU()(x1)\n#     x1 = tf.keras.layers.MaxPooling2D((3, 3), strides=2, padding=\"same\")(x1)\n                           \n#     # Multi-Scale Feature Integration\n#     x_small = tf.keras.layers.MaxPooling2D(4)(x1)\n#     x_medium = tf.keras.layers.MaxPooling2D(2)(x1)\n\n#     # Residual Blocks\n#     x_large = residual_block(x1, 64)\n#     x_large = residual_block(x_large, 64)\n#     x_large = residual_block(x_large, 64)\n    \n#     # Global Average Pooling layer\n#     x1 = tf.keras.layers.Concatenate()([\n#         tf.keras.layers.GlobalAveragePooling2D()(x_small),\n#         tf.keras.layers.GlobalAveragePooling2D()(x_medium),\n#         tf.keras.layers.GlobalAveragePooling2D()(x_large)\n#     ])\n\n    x1 = tf.keras.layers.GlobalMaxPooling2D()(x1)\n    x1    = tf.keras.layers.Dropout(0.1)(x1)\n\n    x2    = tf.keras.layers.Dense(8)(inp2) ##more layers to come after that\n    x2     = tf.keras.layers.concatenate([x2, inp2])\n    \n    x = GatingLayer()([x1, x2])\n\n#     x     = tf.keras.layers.concatenate(cca_layer)\n#     x    = tf.keras.layers.Dense(128)(x)\n    x    = tf.keras.layers.Dense(64)(x)\n    x     = tf.keras.layers.Dropout(0.15)(x)\n\n    output10     = tf.keras.layers.Dense(10, activation='softmax', name='target10')(x)\n\n    model = tf.keras.models.Model(inputs = [inp1, inp2], outputs = [output10])\n\n    return model\n\n\ndef cca_loss(y_true, y_pred):\n    return 0.0 \n\n# def fitengine(model, traindataset, valdataset = None, istraining = True):\n#     model.compile(\n#         optimizer   =  OPTIMIZER,\n#         loss        =  LOSS,\n#         metrics     =  ACCURACY\n#     )\n\n#     history = model.fit(\n#                 traindataset,\n#                 epochs            =   EPOCHES,\n#                 steps_per_epoch   =   TOTAL_TRAIN_IMG//BATCH_SIZE,\n#                 callbacks         =   CALLBACKS,\n#                 validation_data   =   valdataset,\n#                 validation_steps = (TOTAL_VAL_IMG)//(BATCH_SIZE) + 1,\n#                 verbose           =   1\n#             )\n\n#     return history\n\n# skf = KFold(n_splits=FOLDS,shuffle=True,random_state=SEED)\n# FOLDS_DICT = {}\n# for fold_number,(idxT,idxV) in enumerate(skf.split(np.arange(len(TRAIN_FILES)))):\n#     FOLDS_DICT['fold_{}'.format(fold_number)] = {\n#                                             \"trainfiles\" : [TRAIN_FILES[x] for x in idxT],\n#                                             \"valfiles\"   : [TRAIN_FILES[x] for x in idxV]\n#                                             }\n\n# for fold_number in range(1):\n#   print(FOLDS_DICT['fold_{}'.format(fold_number)]['trainfiles'])\n#   print(FOLDS_DICT['fold_{}'.format(fold_number)]['valfiles'])\n#   HISTORY = {}\n\n#   run_ = wandb.init(\n#       project= EXPERIMENT_NAME,\n#       reinit=True,\n#       dir = \"/root\",\n#       allow_val_change = True\n#   )\n\n#   if STRATEGY is not None:\n#       with strategy.scope():\n#           x2 = tf.keras.metrics.Precision(name='precision')\n#           x3 = tf.keras.metrics.BinaryAccuracy(name='accuracy')\n#           x4 = tf.keras.metrics.Recall(name='sensitivity')\n\n#           ACCURACY.append(x2)\n#           ACCURACY.append(x3)\n#           ACCURACY.append(x4)\n\n#           model = multimodeleffbx()\n#   else:\n#       x2 = tf.keras.metrics.Precision(name='precision')\n#       x3 = tf.keras.metrics.BinaryAccuracy(name='accuracy')\n#       x4 = tf.keras.metrics.Recall(name='sensitivity')\n#       x4 = tf.keras.metrics.Recall(name='sensitivity')\n\n#       ACCURACY.append(x2)\n#       ACCURACY.append(x3)\n#       ACCURACY.append(x4)\n\n#       model = multimodeleffbx()\n\n#   CALLBACKS.append(tf.keras.callbacks.ModelCheckpoint(\n#                                   'model-%s.h5'%(fold_number), monitor='val_loss', verbose=1, save_best_only=True,\n#                                   save_weights_only=True, mode='min', save_freq='epoch'))\n\n#   CALLBACKS.append(WandbCallback(save_weights_only=True,\n#                                                 log_weights=True,\n#                                                 log_evaluation=True))\n\n\n\n\n#   ignore_order = tf.data.Options()\n#   ignore_order.experimental_deterministic = False\n#   train_dataset = (\n#       tf.data.TFRecordDataset(\n#           FOLDS_DICT[\"fold_{}\".format(fold_number)][\"trainfiles\"],\n#           num_parallel_reads=tf.data.experimental.AUTOTUNE\n#       ).with_options(\n#           ignore_order\n#       ).map(\n#           process_training_data,\n#           num_parallel_calls=tf.data.experimental.AUTOTUNE\n#       ).repeat(\n#       ).shuffle(\n#           SEED\n#       ).batch(\n#           BATCH_SIZE\n#       ).prefetch(\n#           tf.data.experimental.AUTOTUNE\n#       )\n#   )\n\n#   ignore_order = tf.data.Options()\n#   val_dataset = (\n#       tf.data.TFRecordDataset(\n#           FOLDS_DICT[\"fold_{}\".format(fold_number)][\"valfiles\"],\n#           num_parallel_reads=tf.data.experimental.AUTOTUNE\n#       ).with_options(\n#           ignore_order\n#       ).map(\n#           process_training_data,\n#           num_parallel_calls=tf.data.experimental.AUTOTUNE\n#       ).batch(\n#           BATCH_SIZE\n#       ).prefetch(\n#           tf.data.experimental.AUTOTUNE\n#       )\n#   )\n\n#   print(\"##\"*30)\n\n#   history = fitengine(model, train_dataset, valdataset=val_dataset) #training model\n#   MODELS.append(model)\n\n#   print('##'*30)\n\n\n#   run_.finish()\n\n","metadata":{"execution":{"iopub.status.busy":"2023-10-18T01:17:30.175144Z","iopub.execute_input":"2023-10-18T01:17:30.175650Z","iopub.status.idle":"2023-10-18T01:57:36.320106Z","shell.execute_reply.started":"2023-10-18T01:17:30.175621Z","shell.execute_reply":"2023-10-18T01:57:36.319383Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"2.8.0\n['/kaggle/input/mitdb128x128/mitdb_128x128/trainfile_class10_fold0_6117.tfrec', '/kaggle/input/mitdb128x128/mitdb_128x128/trainfile_class10_fold1_6116.tfrec', '/kaggle/input/mitdb128x128/mitdb_128x128/trainfile_class10_fold2_6116.tfrec', '/kaggle/input/mitdb128x128/mitdb_128x128/trainfile_class10_fold3_6116.tfrec', '/kaggle/input/mitdb128x128/mitdb_128x128/trainfile_class10_fold4_6116.tfrec', '/kaggle/input/mitdb128x128/mitdb_128x128/trainfile_class10_fold5_6116.tfrec', '/kaggle/input/mitdb128x128/mitdb_128x128/trainfile_class10_fold6_6116.tfrec', '/kaggle/input/mitdb128x128/mitdb_128x128/trainfile_class10_fold7_6117.tfrec', '/kaggle/input/mitdb128x128/mitdb_128x128/trainfile_class10_fold8_6116.tfrec']\n['/kaggle/input/mitdb128x128/mitdb_128x128/trainfile_class10_fold0_6117.tfrec', '/kaggle/input/mitdb128x128/mitdb_128x128/trainfile_class10_fold1_6116.tfrec', '/kaggle/input/mitdb128x128/mitdb_128x128/trainfile_class10_fold2_6116.tfrec', '/kaggle/input/mitdb128x128/mitdb_128x128/trainfile_class10_fold3_6116.tfrec', '/kaggle/input/mitdb128x128/mitdb_128x128/trainfile_class10_fold4_6116.tfrec', '/kaggle/input/mitdb128x128/mitdb_128x128/trainfile_class10_fold5_6116.tfrec', '/kaggle/input/mitdb128x128/mitdb_128x128/trainfile_class10_fold6_6116.tfrec', '/kaggle/input/mitdb128x128/mitdb_128x128/trainfile_class10_fold7_6117.tfrec']\n['/kaggle/input/mitdb128x128/mitdb_128x128/trainfile_class10_fold8_6116.tfrec']\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Finishing last run (ID:d7tdbqos) before initializing another..."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.007 MB of 0.007 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5da478741ded4e77b5e99bbed3c5016b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">ruby-firebrand-66</strong> at: <a href='https://wandb.ai/shreya-srivas02/ECG_BEAT_CLASSIFICATION_PAPER_COMMENTS/runs/d7tdbqos' target=\"_blank\">https://wandb.ai/shreya-srivas02/ECG_BEAT_CLASSIFICATION_PAPER_COMMENTS/runs/d7tdbqos</a><br/>Synced 6 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>/root/wandb/run-20231018_010928-d7tdbqos/logs</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Successfully finished last run (ID:d7tdbqos). Initializing new run:<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.15.12"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/root/wandb/run-20231018_011730-ahenxgvj</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/shreya-srivas02/ECG_BEAT_CLASSIFICATION_PAPER_COMMENTS/runs/ahenxgvj' target=\"_blank\">eager-capybara-67</a></strong> to <a href='https://wandb.ai/shreya-srivas02/ECG_BEAT_CLASSIFICATION_PAPER_COMMENTS' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/shreya-srivas02/ECG_BEAT_CLASSIFICATION_PAPER_COMMENTS' target=\"_blank\">https://wandb.ai/shreya-srivas02/ECG_BEAT_CLASSIFICATION_PAPER_COMMENTS</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/shreya-srivas02/ECG_BEAT_CLASSIFICATION_PAPER_COMMENTS/runs/ahenxgvj' target=\"_blank\">https://wandb.ai/shreya-srivas02/ECG_BEAT_CLASSIFICATION_PAPER_COMMENTS/runs/ahenxgvj</a>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m WandbCallback is unable to log validation data. When using a generator for validation_data, you must pass validation_steps\n","output_type":"stream"},{"name":"stdout","text":"############################################################\nEpoch 1/35\n1529/1529 [==============================] - ETA: 0s - loss: 0.4656 - precision: 0.8935 - accuracy: 0.9749 - sensitivity: 0.8501\nEpoch 1: val_loss improved from inf to 2.48347, saving model to model-0.h5\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/root/wandb/run-20231018_011730-ahenxgvj/files/model-best)... Done. 0.0s\n","output_type":"stream"},{"name":"stdout","text":"1529/1529 [==============================] - 66s 42ms/step - loss: 0.4656 - precision: 0.8935 - accuracy: 0.9749 - sensitivity: 0.8501 - val_loss: 2.4835 - val_precision: 0.2984 - val_accuracy: 0.8664 - val_sensitivity: 0.2487 - lr: 0.0029\nEpoch 2/35\n1529/1529 [==============================] - ETA: 0s - loss: 0.2183 - precision: 0.9414 - accuracy: 0.9871 - sensitivity: 0.9289\nEpoch 2: val_loss did not improve from 2.48347\n1529/1529 [==============================] - 61s 40ms/step - loss: 0.2183 - precision: 0.9414 - accuracy: 0.9871 - sensitivity: 0.9289 - val_loss: 3.4994 - val_precision: 0.3166 - val_accuracy: 0.8706 - val_sensitivity: 0.2541 - lr: 0.0029\nEpoch 3/35\n1529/1529 [==============================] - ETA: 0s - loss: 0.1754 - precision: 0.9521 - accuracy: 0.9896 - sensitivity: 0.9433\nEpoch 3: val_loss improved from 2.48347 to 2.06322, saving model to model-0.h5\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/root/wandb/run-20231018_011730-ahenxgvj/files/model-best)... Done. 0.0s\n","output_type":"stream"},{"name":"stdout","text":"1529/1529 [==============================] - 65s 42ms/step - loss: 0.1754 - precision: 0.9521 - accuracy: 0.9896 - sensitivity: 0.9433 - val_loss: 2.0632 - val_precision: 0.6056 - val_accuracy: 0.9207 - val_sensitivity: 0.5943 - lr: 0.0029\nEpoch 4/35\n1529/1529 [==============================] - ETA: 0s - loss: 0.1514 - precision: 0.9581 - accuracy: 0.9908 - sensitivity: 0.9499\nEpoch 4: val_loss improved from 2.06322 to 1.97982, saving model to model-0.h5\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/root/wandb/run-20231018_011730-ahenxgvj/files/model-best)... Done. 0.0s\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m A graphql request initiated by the public wandb API timed out (timeout=19 sec). Create a new API with an integer timeout larger than 19, e.g., `api = wandb.Api(timeout=29)` to increase the graphql timeout.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m A graphql request initiated by the public wandb API timed out (timeout=19 sec). Create a new API with an integer timeout larger than 19, e.g., `api = wandb.Api(timeout=29)` to increase the graphql timeout.\n","output_type":"stream"},{"name":"stdout","text":"1529/1529 [==============================] - 104s 68ms/step - loss: 0.1514 - precision: 0.9581 - accuracy: 0.9908 - sensitivity: 0.9499 - val_loss: 1.9798 - val_precision: 0.6120 - val_accuracy: 0.9096 - val_sensitivity: 0.2618 - lr: 0.0028\nEpoch 5/35\n 199/1529 [==>...........................] - ETA: 51s - loss: 0.1518 - precision: 0.9609 - accuracy: 0.9913 - sensitivity: 0.9523","output_type":"stream"},{"name":"stderr","text":"wandb: Network error (ReadTimeout), entering retry loop.\n","output_type":"stream"},{"name":"stdout","text":"1529/1529 [==============================] - ETA: 0s - loss: 0.1339 - precision: 0.9635 - accuracy: 0.9921 - sensitivity: 0.9567\nEpoch 5: val_loss improved from 1.97982 to 1.78370, saving model to model-0.h5\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/root/wandb/run-20231018_011730-ahenxgvj/files/model-best)... Done. 0.0s\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m A graphql request initiated by the public wandb API timed out (timeout=19 sec). Create a new API with an integer timeout larger than 19, e.g., `api = wandb.Api(timeout=29)` to increase the graphql timeout.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m A graphql request initiated by the public wandb API timed out (timeout=19 sec). Create a new API with an integer timeout larger than 19, e.g., `api = wandb.Api(timeout=29)` to increase the graphql timeout.\n","output_type":"stream"},{"name":"stdout","text":"1529/1529 [==============================] - 104s 68ms/step - loss: 0.1339 - precision: 0.9635 - accuracy: 0.9921 - sensitivity: 0.9567 - val_loss: 1.7837 - val_precision: 0.4937 - val_accuracy: 0.8989 - val_sensitivity: 0.4315 - lr: 0.0028\nEpoch 6/35\n1529/1529 [==============================] - ETA: 0s - loss: 0.1226 - precision: 0.9653 - accuracy: 0.9925 - sensitivity: 0.9597\nEpoch 6: val_loss improved from 1.78370 to 0.44911, saving model to model-0.h5\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/root/wandb/run-20231018_011730-ahenxgvj/files/model-best)... Done. 0.0s\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m A graphql request initiated by the public wandb API timed out (timeout=19 sec). Create a new API with an integer timeout larger than 19, e.g., `api = wandb.Api(timeout=29)` to increase the graphql timeout.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m A graphql request initiated by the public wandb API timed out (timeout=19 sec). Create a new API with an integer timeout larger than 19, e.g., `api = wandb.Api(timeout=29)` to increase the graphql timeout.\n","output_type":"stream"},{"name":"stdout","text":"1529/1529 [==============================] - 103s 68ms/step - loss: 0.1226 - precision: 0.9653 - accuracy: 0.9925 - sensitivity: 0.9597 - val_loss: 0.4491 - val_precision: 0.8729 - val_accuracy: 0.9695 - val_sensitivity: 0.8138 - lr: 0.0028\nEpoch 7/35\n1529/1529 [==============================] - ETA: 0s - loss: 0.1120 - precision: 0.9684 - accuracy: 0.9933 - sensitivity: 0.9641\nEpoch 7: val_loss did not improve from 0.44911\n1529/1529 [==============================] - 61s 40ms/step - loss: 0.1120 - precision: 0.9684 - accuracy: 0.9933 - sensitivity: 0.9641 - val_loss: 1.8133 - val_precision: 0.6681 - val_accuracy: 0.9322 - val_sensitivity: 0.6408 - lr: 0.0027\nEpoch 8/35\n1529/1529 [==============================] - ETA: 0s - loss: 0.1040 - precision: 0.9710 - accuracy: 0.9938 - sensitivity: 0.9667\nEpoch 8: val_loss improved from 0.44911 to 0.16251, saving model to model-0.h5\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/root/wandb/run-20231018_011730-ahenxgvj/files/model-best)... Done. 0.0s\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m A graphql request initiated by the public wandb API timed out (timeout=19 sec). Create a new API with an integer timeout larger than 19, e.g., `api = wandb.Api(timeout=29)` to increase the graphql timeout.\n","output_type":"stream"},{"name":"stdout","text":"1529/1529 [==============================] - 101s 66ms/step - loss: 0.1040 - precision: 0.9710 - accuracy: 0.9938 - sensitivity: 0.9667 - val_loss: 0.1625 - val_precision: 0.9507 - val_accuracy: 0.9897 - val_sensitivity: 0.9460 - lr: 0.0026\nEpoch 9/35\n1529/1529 [==============================] - ETA: 0s - loss: 0.0950 - precision: 0.9734 - accuracy: 0.9943 - sensitivity: 0.9696\nEpoch 9: val_loss did not improve from 0.16251\n1529/1529 [==============================] - 61s 40ms/step - loss: 0.0950 - precision: 0.9734 - accuracy: 0.9943 - sensitivity: 0.9696 - val_loss: 0.4627 - val_precision: 0.8707 - val_accuracy: 0.9705 - val_sensitivity: 0.8280 - lr: 0.0025\nEpoch 10/35\n1529/1529 [==============================] - ETA: 0s - loss: 0.0901 - precision: 0.9743 - accuracy: 0.9945 - sensitivity: 0.9709\nEpoch 10: val_loss did not improve from 0.16251\n1529/1529 [==============================] - 61s 40ms/step - loss: 0.0901 - precision: 0.9743 - accuracy: 0.9945 - sensitivity: 0.9709 - val_loss: 0.1681 - val_precision: 0.9530 - val_accuracy: 0.9899 - val_sensitivity: 0.9454 - lr: 0.0025\nEpoch 11/35\n1529/1529 [==============================] - ETA: 0s - loss: 0.0810 - precision: 0.9765 - accuracy: 0.9950 - sensitivity: 0.9732\nEpoch 11: val_loss improved from 0.16251 to 0.13502, saving model to model-0.h5\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/root/wandb/run-20231018_011730-ahenxgvj/files/model-best)... Done. 0.0s\n","output_type":"stream"},{"name":"stdout","text":"1529/1529 [==============================] - 84s 55ms/step - loss: 0.0810 - precision: 0.9765 - accuracy: 0.9950 - sensitivity: 0.9732 - val_loss: 0.1350 - val_precision: 0.9654 - val_accuracy: 0.9928 - val_sensitivity: 0.9624 - lr: 0.0024\nEpoch 12/35\n1529/1529 [==============================] - ETA: 0s - loss: 0.0769 - precision: 0.9773 - accuracy: 0.9952 - sensitivity: 0.9747\nEpoch 12: val_loss improved from 0.13502 to 0.12616, saving model to model-0.h5\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/root/wandb/run-20231018_011730-ahenxgvj/files/model-best)... Done. 0.0s\n","output_type":"stream"},{"name":"stdout","text":"1529/1529 [==============================] - 64s 42ms/step - loss: 0.0769 - precision: 0.9773 - accuracy: 0.9952 - sensitivity: 0.9747 - val_loss: 0.1262 - val_precision: 0.9685 - val_accuracy: 0.9933 - val_sensitivity: 0.9644 - lr: 0.0022\nEpoch 13/35\n1528/1529 [============================>.] - ETA: 0s - loss: 0.0696 - precision: 0.9798 - accuracy: 0.9957 - sensitivity: 0.9771\nEpoch 13: val_loss did not improve from 0.12616\n1529/1529 [==============================] - 61s 40ms/step - loss: 0.0696 - precision: 0.9797 - accuracy: 0.9957 - sensitivity: 0.9771 - val_loss: 0.2604 - val_precision: 0.9333 - val_accuracy: 0.9861 - val_sensitivity: 0.9272 - lr: 0.0021\nEpoch 14/35\n1529/1529 [==============================] - ETA: 0s - loss: 0.0633 - precision: 0.9811 - accuracy: 0.9960 - sensitivity: 0.9790\nEpoch 14: val_loss did not improve from 0.12616\n1529/1529 [==============================] - 61s 40ms/step - loss: 0.0633 - precision: 0.9811 - accuracy: 0.9960 - sensitivity: 0.9790 - val_loss: 0.1398 - val_precision: 0.9641 - val_accuracy: 0.9926 - val_sensitivity: 0.9621 - lr: 0.0020\nEpoch 15/35\n1529/1529 [==============================] - ETA: 0s - loss: 0.0585 - precision: 0.9824 - accuracy: 0.9963 - sensitivity: 0.9802\nEpoch 15: val_loss did not improve from 0.12616\n1529/1529 [==============================] - 61s 40ms/step - loss: 0.0585 - precision: 0.9824 - accuracy: 0.9963 - sensitivity: 0.9802 - val_loss: 0.1288 - val_precision: 0.9709 - val_accuracy: 0.9940 - val_sensitivity: 0.9694 - lr: 0.0019\nEpoch 16/35\n1529/1529 [==============================] - ETA: 0s - loss: 0.0527 - precision: 0.9838 - accuracy: 0.9966 - sensitivity: 0.9824\nEpoch 16: val_loss did not improve from 0.12616\n1529/1529 [==============================] - 61s 40ms/step - loss: 0.0527 - precision: 0.9838 - accuracy: 0.9966 - sensitivity: 0.9824 - val_loss: 0.3462 - val_precision: 0.9050 - val_accuracy: 0.9793 - val_sensitivity: 0.8859 - lr: 0.0018\nEpoch 17/35\n1529/1529 [==============================] - ETA: 0s - loss: 0.0472 - precision: 0.9853 - accuracy: 0.9969 - sensitivity: 0.9836\nEpoch 17: val_loss improved from 0.12616 to 0.12474, saving model to model-0.h5\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/root/wandb/run-20231018_011730-ahenxgvj/files/model-best)... Done. 0.0s\n","output_type":"stream"},{"name":"stdout","text":"1529/1529 [==============================] - 65s 42ms/step - loss: 0.0472 - precision: 0.9853 - accuracy: 0.9969 - sensitivity: 0.9836 - val_loss: 0.1247 - val_precision: 0.9753 - val_accuracy: 0.9949 - val_sensitivity: 0.9735 - lr: 0.0016\nEpoch 18/35\n1529/1529 [==============================] - ETA: 0s - loss: 0.0433 - precision: 0.9868 - accuracy: 0.9972 - sensitivity: 0.9854\nEpoch 18: val_loss did not improve from 0.12474\n1529/1529 [==============================] - 61s 40ms/step - loss: 0.0433 - precision: 0.9868 - accuracy: 0.9972 - sensitivity: 0.9854 - val_loss: 0.1579 - val_precision: 0.9615 - val_accuracy: 0.9920 - val_sensitivity: 0.9588 - lr: 0.0015\nEpoch 19/35\n1529/1529 [==============================] - ETA: 0s - loss: 0.0378 - precision: 0.9875 - accuracy: 0.9974 - sensitivity: 0.9867\nEpoch 19: val_loss did not improve from 0.12474\n1529/1529 [==============================] - 61s 40ms/step - loss: 0.0378 - precision: 0.9875 - accuracy: 0.9974 - sensitivity: 0.9867 - val_loss: 0.1603 - val_precision: 0.9612 - val_accuracy: 0.9918 - val_sensitivity: 0.9562 - lr: 0.0014\nEpoch 20/35\n1529/1529 [==============================] - ETA: 0s - loss: 0.0326 - precision: 0.9895 - accuracy: 0.9978 - sensitivity: 0.9886\nEpoch 20: val_loss did not improve from 0.12474\n1529/1529 [==============================] - 61s 40ms/step - loss: 0.0326 - precision: 0.9895 - accuracy: 0.9978 - sensitivity: 0.9886 - val_loss: 0.1278 - val_precision: 0.9741 - val_accuracy: 0.9947 - val_sensitivity: 0.9730 - lr: 0.0013\nEpoch 21/35\n1529/1529 [==============================] - ETA: 0s - loss: 0.0299 - precision: 0.9907 - accuracy: 0.9981 - sensitivity: 0.9900\nEpoch 21: val_loss did not improve from 0.12474\n1529/1529 [==============================] - 61s 40ms/step - loss: 0.0299 - precision: 0.9907 - accuracy: 0.9981 - sensitivity: 0.9900 - val_loss: 0.1605 - val_precision: 0.9712 - val_accuracy: 0.9942 - val_sensitivity: 0.9706 - lr: 0.0011\nEpoch 22/35\n1529/1529 [==============================] - ETA: 0s - loss: 0.0244 - precision: 0.9917 - accuracy: 0.9983 - sensitivity: 0.9911\nEpoch 22: val_loss did not improve from 0.12474\n1529/1529 [==============================] - 60s 39ms/step - loss: 0.0244 - precision: 0.9917 - accuracy: 0.9983 - sensitivity: 0.9911 - val_loss: 0.1416 - val_precision: 0.9748 - val_accuracy: 0.9949 - val_sensitivity: 0.9738 - lr: 0.0010\nEpoch 23/35\n1529/1529 [==============================] - ETA: 0s - loss: 0.0222 - precision: 0.9929 - accuracy: 0.9985 - sensitivity: 0.9923\nEpoch 23: val_loss improved from 0.12474 to 0.11754, saving model to model-0.h5\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/root/wandb/run-20231018_011730-ahenxgvj/files/model-best)... Done. 0.0s\n","output_type":"stream"},{"name":"stdout","text":"1529/1529 [==============================] - 64s 42ms/step - loss: 0.0222 - precision: 0.9929 - accuracy: 0.9985 - sensitivity: 0.9923 - val_loss: 0.1175 - val_precision: 0.9776 - val_accuracy: 0.9955 - val_sensitivity: 0.9773 - lr: 8.8011e-04\nEpoch 24/35\n1529/1529 [==============================] - ETA: 0s - loss: 0.0173 - precision: 0.9948 - accuracy: 0.9989 - sensitivity: 0.9943\nEpoch 24: val_loss did not improve from 0.11754\n1529/1529 [==============================] - 61s 40ms/step - loss: 0.0173 - precision: 0.9948 - accuracy: 0.9989 - sensitivity: 0.9943 - val_loss: 0.1272 - val_precision: 0.9782 - val_accuracy: 0.9956 - val_sensitivity: 0.9773 - lr: 7.6289e-04\nEpoch 25/35\n1529/1529 [==============================] - ETA: 0s - loss: 0.0149 - precision: 0.9950 - accuracy: 0.9990 - sensitivity: 0.9945\nEpoch 25: val_loss did not improve from 0.11754\n1529/1529 [==============================] - 61s 40ms/step - loss: 0.0149 - precision: 0.9950 - accuracy: 0.9990 - sensitivity: 0.9945 - val_loss: 0.1427 - val_precision: 0.9777 - val_accuracy: 0.9955 - val_sensitivity: 0.9769 - lr: 6.5120e-04\nEpoch 26/35\n1529/1529 [==============================] - ETA: 0s - loss: 0.0123 - precision: 0.9961 - accuracy: 0.9992 - sensitivity: 0.9959\nEpoch 26: val_loss did not improve from 0.11754\n1529/1529 [==============================] - 61s 40ms/step - loss: 0.0123 - precision: 0.9961 - accuracy: 0.9992 - sensitivity: 0.9959 - val_loss: 0.1438 - val_precision: 0.9763 - val_accuracy: 0.9952 - val_sensitivity: 0.9755 - lr: 5.4594e-04\nEpoch 27/35\n1529/1529 [==============================] - ETA: 0s - loss: 0.0098 - precision: 0.9969 - accuracy: 0.9993 - sensitivity: 0.9965\nEpoch 27: val_loss did not improve from 0.11754\n1529/1529 [==============================] - 61s 40ms/step - loss: 0.0098 - precision: 0.9969 - accuracy: 0.9993 - sensitivity: 0.9965 - val_loss: 0.1595 - val_precision: 0.9766 - val_accuracy: 0.9953 - val_sensitivity: 0.9763 - lr: 4.4796e-04\nEpoch 28/35\n1528/1529 [============================>.] - ETA: 0s - loss: 0.0077 - precision: 0.9978 - accuracy: 0.9995 - sensitivity: 0.9977\nEpoch 28: val_loss did not improve from 0.11754\n1529/1529 [==============================] - 62s 41ms/step - loss: 0.0078 - precision: 0.9978 - accuracy: 0.9995 - sensitivity: 0.9976 - val_loss: 0.1473 - val_precision: 0.9786 - val_accuracy: 0.9957 - val_sensitivity: 0.9783 - lr: 3.5805e-04\nEpoch 29/35\n1528/1529 [============================>.] - ETA: 0s - loss: 0.0076 - precision: 0.9976 - accuracy: 0.9995 - sensitivity: 0.9973\nEpoch 29: val_loss did not improve from 0.11754\n1529/1529 [==============================] - 62s 40ms/step - loss: 0.0076 - precision: 0.9976 - accuracy: 0.9995 - sensitivity: 0.9973 - val_loss: 0.1487 - val_precision: 0.9784 - val_accuracy: 0.9956 - val_sensitivity: 0.9778 - lr: 2.7693e-04\nEpoch 30/35\n1529/1529 [==============================] - ETA: 0s - loss: 0.0057 - precision: 0.9984 - accuracy: 0.9997 - sensitivity: 0.9983\nEpoch 30: val_loss did not improve from 0.11754\n1529/1529 [==============================] - 63s 41ms/step - loss: 0.0057 - precision: 0.9984 - accuracy: 0.9997 - sensitivity: 0.9983 - val_loss: 0.1585 - val_precision: 0.9764 - val_accuracy: 0.9952 - val_sensitivity: 0.9756 - lr: 2.0525e-04\nEpoch 31/35\n1528/1529 [============================>.] - ETA: 0s - loss: 0.0049 - precision: 0.9987 - accuracy: 0.9997 - sensitivity: 0.9986\nEpoch 31: val_loss did not improve from 0.11754\n1529/1529 [==============================] - 62s 40ms/step - loss: 0.0049 - precision: 0.9987 - accuracy: 0.9997 - sensitivity: 0.9986 - val_loss: 0.1560 - val_precision: 0.9772 - val_accuracy: 0.9953 - val_sensitivity: 0.9761 - lr: 1.4360e-04\nEpoch 32/35\n1529/1529 [==============================] - ETA: 0s - loss: 0.0041 - precision: 0.9989 - accuracy: 0.9998 - sensitivity: 0.9988\nEpoch 32: val_loss did not improve from 0.11754\n1529/1529 [==============================] - 62s 40ms/step - loss: 0.0041 - precision: 0.9989 - accuracy: 0.9998 - sensitivity: 0.9988 - val_loss: 0.1573 - val_precision: 0.9779 - val_accuracy: 0.9955 - val_sensitivity: 0.9771 - lr: 9.2459e-05\nEpoch 33/35\n1529/1529 [==============================] - ETA: 0s - loss: 0.0039 - precision: 0.9989 - accuracy: 0.9998 - sensitivity: 0.9988\nEpoch 33: val_loss did not improve from 0.11754\n1529/1529 [==============================] - 61s 40ms/step - loss: 0.0039 - precision: 0.9989 - accuracy: 0.9998 - sensitivity: 0.9988 - val_loss: 0.1580 - val_precision: 0.9781 - val_accuracy: 0.9956 - val_sensitivity: 0.9774 - lr: 5.2254e-05\nEpoch 34/35\n1528/1529 [============================>.] - ETA: 0s - loss: 0.0029 - precision: 0.9993 - accuracy: 0.9999 - sensitivity: 0.9992\nEpoch 34: val_loss did not improve from 0.11754\n1529/1529 [==============================] - 61s 40ms/step - loss: 0.0029 - precision: 0.9993 - accuracy: 0.9999 - sensitivity: 0.9992 - val_loss: 0.1598 - val_precision: 0.9779 - val_accuracy: 0.9955 - val_sensitivity: 0.9771 - lr: 2.3302e-05\nEpoch 35/35\n1528/1529 [============================>.] - ETA: 0s - loss: 0.0032 - precision: 0.9991 - accuracy: 0.9998 - sensitivity: 0.9990\nEpoch 35: val_loss did not improve from 0.11754\n1529/1529 [==============================] - 61s 40ms/step - loss: 0.0032 - precision: 0.9991 - accuracy: 0.9998 - sensitivity: 0.9990 - val_loss: 0.1594 - val_precision: 0.9782 - val_accuracy: 0.9956 - val_sensitivity: 0.9773 - lr: 5.8373e-06\n############################################################\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>▁▄▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇███████████████</td></tr><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>loss</td><td>█▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>███████▇▇▇▇▆▆▆▆▅▅▅▄▄▄▃▃▃▃▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>precision</td><td>▁▄▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇██████████████</td></tr><tr><td>sensitivity</td><td>▁▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇████████████████</td></tr><tr><td>val_accuracy</td><td>▁▁▄▃▃▇▅█▇███▇██▇███████████████████</td></tr><tr><td>val_loss</td><td>▆█▅▅▄▂▅▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_precision</td><td>▁▁▄▄▃▇▅█▇██████▇███████████████████</td></tr><tr><td>val_sensitivity</td><td>▁▁▄▁▃▆▅█▇██████▇███████████████████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>0.99981</td></tr><tr><td>best_epoch</td><td>22</td></tr><tr><td>best_val_loss</td><td>0.11754</td></tr><tr><td>epoch</td><td>34</td></tr><tr><td>loss</td><td>0.00318</td></tr><tr><td>lr</td><td>1e-05</td></tr><tr><td>precision</td><td>0.99908</td></tr><tr><td>sensitivity</td><td>0.999</td></tr><tr><td>val_accuracy</td><td>0.99555</td></tr><tr><td>val_loss</td><td>0.15936</td></tr><tr><td>val_precision</td><td>0.97823</td></tr><tr><td>val_sensitivity</td><td>0.97727</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">eager-capybara-67</strong> at: <a href='https://wandb.ai/shreya-srivas02/ECG_BEAT_CLASSIFICATION_PAPER_COMMENTS/runs/ahenxgvj' target=\"_blank\">https://wandb.ai/shreya-srivas02/ECG_BEAT_CLASSIFICATION_PAPER_COMMENTS/runs/ahenxgvj</a><br/>Synced 6 W&B file(s), 1 media file(s), 40 artifact file(s) and 1 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>/root/wandb/run-20231018_011730-ahenxgvj/logs</code>"},"metadata":{}}]},{"cell_type":"code","source":"run_.finish()","metadata":{"execution":{"iopub.status.busy":"2023-10-18T00:03:31.131394Z","iopub.execute_input":"2023-10-18T00:03:31.131954Z","iopub.status.idle":"2023-10-18T00:03:31.136211Z","shell.execute_reply.started":"2023-10-18T00:03:31.131925Z","shell.execute_reply":"2023-10-18T00:03:31.135331Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"code","source":"multimodeleffbx().summary()","metadata":{"execution":{"iopub.status.busy":"2023-10-18T00:00:32.416102Z","iopub.execute_input":"2023-10-18T00:00:32.416868Z","iopub.status.idle":"2023-10-18T00:00:32.749982Z","shell.execute_reply.started":"2023-10-18T00:00:32.416838Z","shell.execute_reply":"2023-10-18T00:00:32.749509Z"},"trusted":true},"execution_count":62,"outputs":[{"name":"stdout","text":"Model: \"model_13\"\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to                     \n==================================================================================================\n inp1 (InputLayer)              [(None, 128, 128, 1  0           []                               \n                                )]                                                                \n                                                                                                  \n conv2d_69 (Conv2D)             (None, 64, 64, 32)   320         ['inp1[0][0]']                   \n                                                                                                  \n batch_normalization_124 (Batch  (None, 64, 64, 32)  128         ['conv2d_69[0][0]']              \n Normalization)                                                                                   \n                                                                                                  \n re_lu_124 (ReLU)               (None, 64, 64, 32)   0           ['batch_normalization_124[0][0]']\n                                                                                                  \n depthwise_conv2d_56 (Depthwise  (None, 64, 64, 32)  320         ['re_lu_124[0][0]']              \n Conv2D)                                                                                          \n                                                                                                  \n batch_normalization_125 (Batch  (None, 64, 64, 32)  128         ['depthwise_conv2d_56[0][0]']    \n Normalization)                                                                                   \n                                                                                                  \n re_lu_125 (ReLU)               (None, 64, 64, 32)   0           ['batch_normalization_125[0][0]']\n                                                                                                  \n conv2d_70 (Conv2D)             (None, 64, 64, 64)   2112        ['re_lu_125[0][0]']              \n                                                                                                  \n batch_normalization_126 (Batch  (None, 64, 64, 64)  256         ['conv2d_70[0][0]']              \n Normalization)                                                                                   \n                                                                                                  \n re_lu_126 (ReLU)               (None, 64, 64, 64)   0           ['batch_normalization_126[0][0]']\n                                                                                                  \n depthwise_conv2d_57 (Depthwise  (None, 32, 32, 64)  640         ['re_lu_126[0][0]']              \n Conv2D)                                                                                          \n                                                                                                  \n batch_normalization_127 (Batch  (None, 32, 32, 64)  256         ['depthwise_conv2d_57[0][0]']    \n Normalization)                                                                                   \n                                                                                                  \n re_lu_127 (ReLU)               (None, 32, 32, 64)   0           ['batch_normalization_127[0][0]']\n                                                                                                  \n conv2d_71 (Conv2D)             (None, 32, 32, 128)  8320        ['re_lu_127[0][0]']              \n                                                                                                  \n batch_normalization_128 (Batch  (None, 32, 32, 128)  512        ['conv2d_71[0][0]']              \n Normalization)                                                                                   \n                                                                                                  \n re_lu_128 (ReLU)               (None, 32, 32, 128)  0           ['batch_normalization_128[0][0]']\n                                                                                                  \n depthwise_conv2d_58 (Depthwise  (None, 32, 32, 128)  1280       ['re_lu_128[0][0]']              \n Conv2D)                                                                                          \n                                                                                                  \n batch_normalization_129 (Batch  (None, 32, 32, 128)  512        ['depthwise_conv2d_58[0][0]']    \n Normalization)                                                                                   \n                                                                                                  \n re_lu_129 (ReLU)               (None, 32, 32, 128)  0           ['batch_normalization_129[0][0]']\n                                                                                                  \n conv2d_72 (Conv2D)             (None, 32, 32, 128)  16512       ['re_lu_129[0][0]']              \n                                                                                                  \n batch_normalization_130 (Batch  (None, 32, 32, 128)  512        ['conv2d_72[0][0]']              \n Normalization)                                                                                   \n                                                                                                  \n re_lu_130 (ReLU)               (None, 32, 32, 128)  0           ['batch_normalization_130[0][0]']\n                                                                                                  \n depthwise_conv2d_59 (Depthwise  (None, 16, 16, 128)  1280       ['re_lu_130[0][0]']              \n Conv2D)                                                                                          \n                                                                                                  \n batch_normalization_131 (Batch  (None, 16, 16, 128)  512        ['depthwise_conv2d_59[0][0]']    \n Normalization)                                                                                   \n                                                                                                  \n re_lu_131 (ReLU)               (None, 16, 16, 128)  0           ['batch_normalization_131[0][0]']\n                                                                                                  \n conv2d_73 (Conv2D)             (None, 16, 16, 256)  33024       ['re_lu_131[0][0]']              \n                                                                                                  \n batch_normalization_132 (Batch  (None, 16, 16, 256)  1024       ['conv2d_73[0][0]']              \n Normalization)                                                                                   \n                                                                                                  \n re_lu_132 (ReLU)               (None, 16, 16, 256)  0           ['batch_normalization_132[0][0]']\n                                                                                                  \n inp2 (InputLayer)              [(None, 2)]          0           []                               \n                                                                                                  \n global_max_pooling2d_14 (Globa  (None, 256)         0           ['re_lu_132[0][0]']              \n lMaxPooling2D)                                                                                   \n                                                                                                  \n dense_31 (Dense)               (None, 8)            24          ['inp2[0][0]']                   \n                                                                                                  \n dropout_31 (Dropout)           (None, 256)          0           ['global_max_pooling2d_14[0][0]']\n                                                                                                  \n concatenate_18 (Concatenate)   (None, 10)           0           ['dense_31[0][0]',               \n                                                                  'inp2[0][0]']                   \n                                                                                                  \n self_attentive_cca_layer_4 (Se  (None, 266)         262246      ['dropout_31[0][0]',             \n lfAttentiveCCALayer)                                             'concatenate_18[0][0]']         \n                                                                                                  \n dense_32 (Dense)               (None, 64)           17088       ['self_attentive_cca_layer_4[0][0\n                                                                 ]']                              \n                                                                                                  \n dropout_32 (Dropout)           (None, 64)           0           ['dense_32[0][0]']               \n                                                                                                  \n target10 (Dense)               (None, 10)           650         ['dropout_32[0][0]']             \n                                                                                                  \n==================================================================================================\nTotal params: 347,656\nTrainable params: 345,636\nNon-trainable params: 2,020\n__________________________________________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import wandb\nrun = wandb.init()\nartifact = run.use_artifact('shreya-srivas02/ECG_BEAT_CLASSIFICATION_PAPER_COMMENTS/model-lilac-serenity-46:v7', type='model')\nartifact_dir = artifact.download()","metadata":{"execution":{"iopub.status.busy":"2023-10-17T16:27:42.264091Z","iopub.execute_input":"2023-10-17T16:27:42.264560Z","iopub.status.idle":"2023-10-17T16:28:14.094242Z","shell.execute_reply.started":"2023-10-17T16:27:42.264524Z","shell.execute_reply":"2023-10-17T16:28:14.093193Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.15.12"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20231017_162742-u7hpc83n</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/shreya-srivas02/uncategorized/runs/u7hpc83n' target=\"_blank\">astral-sunset-14</a></strong> to <a href='https://wandb.ai/shreya-srivas02/uncategorized' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/shreya-srivas02/uncategorized' target=\"_blank\">https://wandb.ai/shreya-srivas02/uncategorized</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/shreya-srivas02/uncategorized/runs/u7hpc83n' target=\"_blank\">https://wandb.ai/shreya-srivas02/uncategorized/runs/u7hpc83n</a>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m:   4 of 4 files downloaded.  \n","output_type":"stream"}]},{"cell_type":"code","source":"model0 = tf.keras.models.load_model('./artifacts/model-lilac-serenity-46:v7')","metadata":{"execution":{"iopub.status.busy":"2023-10-17T16:28:24.866547Z","iopub.execute_input":"2023-10-17T16:28:24.866918Z","iopub.status.idle":"2023-10-17T16:28:28.743297Z","shell.execute_reply.started":"2023-10-17T16:28:24.866889Z","shell.execute_reply":"2023-10-17T16:28:28.741827Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"model_till_last_epoch = model\nSAVED_MODEL_LOC = \"/kaggle/working/model-0.h5\"\nmodel = multimodeleffbx()\nmodel.load_weights(SAVED_MODEL_LOC)","metadata":{"execution":{"iopub.status.busy":"2023-10-17T23:39:47.335163Z","iopub.execute_input":"2023-10-17T23:39:47.335981Z","iopub.status.idle":"2023-10-17T23:39:47.559243Z","shell.execute_reply.started":"2023-10-17T23:39:47.335953Z","shell.execute_reply":"2023-10-17T23:39:47.558558Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"model.layers[-4].get_weights()","metadata":{"execution":{"iopub.status.busy":"2023-10-17T23:39:51.428690Z","iopub.execute_input":"2023-10-17T23:39:51.429189Z","iopub.status.idle":"2023-10-17T23:39:51.438726Z","shell.execute_reply.started":"2023-10-17T23:39:51.429161Z","shell.execute_reply":"2023-10-17T23:39:51.437910Z"},"trusted":true},"execution_count":48,"outputs":[{"execution_count":48,"output_type":"execute_result","data":{"text/plain":"[array([[ 0.34099966,  0.35806328, -0.64132684, ...,  0.53833324,\n         -0.48714924,  0.2555556 ],\n        [-0.22259298,  0.6418431 , -0.54863226, ...,  0.01287498,\n         -0.67329305,  1.3271586 ],\n        [ 0.1477995 ,  0.43886405,  1.0501593 , ..., -0.77713877,\n         -0.03431987,  0.05665112],\n        ...,\n        [ 0.02020958, -0.00601517,  0.02160772, ...,  1.0120411 ,\n          0.01292967, -0.03050697],\n        [ 0.04812548,  0.03095824, -0.9921004 , ..., -0.17983432,\n          0.6540935 ,  0.1910245 ],\n        [-0.5215914 ,  0.13641198, -0.10008233, ..., -0.6478999 ,\n         -0.17243706,  0.9624469 ]], dtype=float32),\n array([-0.10074648], dtype=float32),\n array([-0.2950195], dtype=float32),\n array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]], dtype=float32)]"},"metadata":{}}]},{"cell_type":"code","source":"def process_testing_data(data_file):\n    LABELED_TFREC_FORMAT = {\n            \"image_id\": tf.io.FixedLenFeature([], tf.string),\n            \"image\": tf.io.FixedLenFeature([], tf.string),\n            'target10': tf.io.FixedLenFeature([], tf.int64),\n            'gender' : tf.io.FixedLenFeature([], tf.int64),\n            'age_interval' : tf.io.FixedLenFeature([], tf.int64),\n        }\n    data = tf.io.parse_single_example(data_file, LABELED_TFREC_FORMAT)\n    img = tf.image.decode_jpeg(data['image'], channels=1)\n    img = tf.cast(img, tf.float32) / 255.0\n    img = tf.reshape(img, [*IMG_TRAIN_SHAPE, 1])\n\n    age = tf.cast(data['age_interval'], tf.float32) / 10.0\n    sex = tf.cast(data['gender'], tf.float32) / 1.0\n    tab_data = [tf.cast(tfeat, dtype = tf.float32) for tfeat in [age, sex]]\n    tabular_data = tf.stack(tab_data)\n\n    target10 = tf.one_hot(data['target10'], depth=10)\n    image_id = data[\"image_id\"]\n\n    return {'inp1' : img, 'inp2' : tabular_data}, {\"target10\" : target10, \"image_id\":  data['image_id']}\n\nignore_order = tf.data.Options()\nTEST_FILES = sorted(tf.io.gfile.glob('/kaggle/input/mitdb128x128/mitdb_128x128/train*.tfrec'))[-1]\ntest_dataset = (\n    tf.data.TFRecordDataset(\n        TEST_FILES,\n        num_parallel_reads=tf.data.experimental.AUTOTUNE\n    ).with_options(\n        ignore_order\n    ).map(\n        process_testing_data,\n        num_parallel_calls=tf.data.experimental.AUTOTUNE\n    ).batch(\n        BATCH_SIZE *  4\n    ).prefetch(\n        tf.data.experimental.AUTOTUNE\n    )\n)\n\n\nfold = 0\ntest_imgs = test_dataset.map(lambda data, ids: data)\nimg_labels_ds = test_dataset.map(lambda data, ids: ids).unbatch()\n\nSTEPS = (TOTAL_TEST_IMG)//(BATCH_SIZE*4) + 1\n\npred = model.predict(test_imgs,steps = int(STEPS), verbose=1)\n# # pred = pred.squeeze()\ntest_labels = next(iter(img_labels_ds.batch(int(TOTAL_TEST_IMG) + 1)))\npd.DataFrame({\n        'image_id'  : test_labels[\"image_id\"].numpy(),\n        'actual'  : np.argmax(test_labels[\"target10\"].numpy(), axis=1),\n        'predicted'      : np.argmax(pred, axis=1)\n        }).to_csv('prediction_{}.csv'.format(fold + 1), index=False)\n\ndf = pd.read_csv(\"prediction_{}.csv\".format(fold + 1))\n\n\n\nNAME = ['/', \"A\",  'F', 'L', 'N', 'R', 'V', 'a', 'f', 'j']\nharvest = confusion_matrix(df['actual'], df['predicted'])\nfig, ax = plt.subplots(figsize=(8,8))\nim = ax.imshow(harvest)\nax.set_xticks(np.arange(len(NAME)))\nax.set_yticks(np.arange(len(NAME)))\nax.set_xticklabels(NAME)\nax.set_yticklabels(NAME)\n\n# Rotate the tick labels and set their alignment.\nplt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n        rotation_mode=\"anchor\")\n\nfor i in range(len(NAME)):\n    for j in range(len(NAME)):\n        text = ax.text(j, i, harvest[i, j],\n                    ha=\"center\", va=\"center\", color=\"w\")\n\nfig.tight_layout()\n\nfrom sklearn.metrics import classification_report\ntarget_names = NAME\nx_ = classification_report(df['actual'], df['predicted'], target_names=target_names)\nprint(x_)","metadata":{"execution":{"iopub.status.busy":"2023-10-17T23:40:14.206793Z","iopub.execute_input":"2023-10-17T23:40:14.207378Z","iopub.status.idle":"2023-10-17T23:40:16.952885Z","shell.execute_reply.started":"2023-10-17T23:40:14.207351Z","shell.execute_reply":"2023-10-17T23:40:16.952179Z"},"trusted":true},"execution_count":50,"outputs":[{"name":"stdout","text":"48/48 [==============================] - 1s 16ms/step\n              precision    recall  f1-score   support\n\n           /       0.99      1.00      0.99       382\n           A       0.88      0.78      0.83       245\n           F       0.92      0.79      0.85        76\n           L       0.99      0.99      0.99       801\n           N       0.97      0.98      0.97      3252\n           R       0.98      0.99      0.99       586\n           V       0.93      0.93      0.93       713\n           a       0.73      0.73      0.73        11\n           f       0.86      0.86      0.86        21\n           j       0.75      0.52      0.61        29\n\n    accuracy                           0.96      6116\n   macro avg       0.90      0.86      0.88      6116\nweighted avg       0.96      0.96      0.96      6116\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 800x800 with 1 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"code","source":"print(x_) # with meta data mobilenetv1 -> 261k","metadata":{"execution":{"iopub.status.busy":"2023-10-16T15:33:23.344712Z","iopub.execute_input":"2023-10-16T15:33:23.345060Z","iopub.status.idle":"2023-10-16T15:33:23.353044Z","shell.execute_reply.started":"2023-10-16T15:33:23.345035Z","shell.execute_reply":"2023-10-16T15:33:23.352018Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"              precision    recall  f1-score   support\n\n           /       1.00      1.00      1.00       382\n           A       0.91      0.82      0.86       245\n           F       0.89      0.72      0.80        76\n           L       0.98      0.99      0.99       801\n           N       0.97      0.99      0.98      3252\n           R       0.99      0.99      0.99       586\n           V       0.95      0.91      0.93       713\n           a       0.78      0.64      0.70        11\n           f       0.95      0.90      0.93        21\n           j       0.74      0.90      0.81        29\n\n    accuracy                           0.97      6116\n   macro avg       0.92      0.89      0.90      6116\nweighted avg       0.97      0.97      0.97      6116\n\n","output_type":"stream"}]},{"cell_type":"code","source":"!cp /kaggle/working/model-0.h5 /kaggle/working/model-0-b.h5","metadata":{"execution":{"iopub.status.busy":"2023-10-16T01:39:27.321425Z","iopub.execute_input":"2023-10-16T01:39:27.321952Z","iopub.status.idle":"2023-10-16T01:39:28.679373Z","shell.execute_reply.started":"2023-10-16T01:39:27.321910Z","shell.execute_reply":"2023-10-16T01:39:28.677962Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"wandb.finish()","metadata":{"execution":{"iopub.status.busy":"2023-10-18T15:36:40.592894Z","iopub.execute_input":"2023-10-18T15:36:40.593276Z","iopub.status.idle":"2023-10-18T15:36:43.503318Z","shell.execute_reply.started":"2023-10-18T15:36:40.593248Z","shell.execute_reply":"2023-10-18T15:36:43.502300Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">true-puddle-68</strong> at: <a href='https://wandb.ai/shreya-srivas02/ECG_BEAT_CLASSIFICATION_PAPER_COMMENTS/runs/on7lu00p' target=\"_blank\">https://wandb.ai/shreya-srivas02/ECG_BEAT_CLASSIFICATION_PAPER_COMMENTS/runs/on7lu00p</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>/root/wandb/run-20231018_153506-on7lu00p/logs</code>"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}