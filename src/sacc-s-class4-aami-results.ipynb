{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# !pip install tensorflow==2.8.0\n!pip install efficientnet\n!pip install --upgrade wandb\n!pip install boto3\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-11-02T17:57:23.343119Z","iopub.execute_input":"2023-11-02T17:57:23.343482Z","iopub.status.idle":"2023-11-02T17:58:07.147641Z","shell.execute_reply.started":"2023-11-02T17:57:23.343450Z","shell.execute_reply":"2023-11-02T17:58:07.146658Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting efficientnet\n  Downloading efficientnet-1.1.1-py3-none-any.whl (18 kB)\nCollecting keras-applications<=1.0.8,>=1.0.7 (from efficientnet)\n  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.7/50.7 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: scikit-image in /opt/conda/lib/python3.10/site-packages (from efficientnet) (0.21.0)\nRequirement already satisfied: numpy>=1.9.1 in /opt/conda/lib/python3.10/site-packages (from keras-applications<=1.0.8,>=1.0.7->efficientnet) (1.23.5)\nRequirement already satisfied: h5py in /opt/conda/lib/python3.10/site-packages (from keras-applications<=1.0.8,>=1.0.7->efficientnet) (3.9.0)\nRequirement already satisfied: scipy>=1.8 in /opt/conda/lib/python3.10/site-packages (from scikit-image->efficientnet) (1.11.2)\nRequirement already satisfied: networkx>=2.8 in /opt/conda/lib/python3.10/site-packages (from scikit-image->efficientnet) (3.1)\nRequirement already satisfied: pillow>=9.0.1 in /opt/conda/lib/python3.10/site-packages (from scikit-image->efficientnet) (9.5.0)\nRequirement already satisfied: imageio>=2.27 in /opt/conda/lib/python3.10/site-packages (from scikit-image->efficientnet) (2.31.1)\nRequirement already satisfied: tifffile>=2022.8.12 in /opt/conda/lib/python3.10/site-packages (from scikit-image->efficientnet) (2023.4.12)\nRequirement already satisfied: PyWavelets>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-image->efficientnet) (1.4.1)\nRequirement already satisfied: packaging>=21 in /opt/conda/lib/python3.10/site-packages (from scikit-image->efficientnet) (21.3)\nRequirement already satisfied: lazy_loader>=0.2 in /opt/conda/lib/python3.10/site-packages (from scikit-image->efficientnet) (0.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=21->scikit-image->efficientnet) (3.0.9)\nInstalling collected packages: keras-applications, efficientnet\nSuccessfully installed efficientnet-1.1.1 keras-applications-1.0.8\nRequirement already satisfied: wandb in /opt/conda/lib/python3.10/site-packages (0.15.9)\nCollecting wandb\n  Downloading wandb-0.15.12-py3-none-any.whl (2.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.1 in /opt/conda/lib/python3.10/site-packages (from wandb) (8.1.7)\nRequirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.1.31)\nRequirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (2.31.0)\nRequirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (5.9.3)\nRequirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (1.30.0)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (0.4.0)\nRequirement already satisfied: PyYAML in /opt/conda/lib/python3.10/site-packages (from wandb) (6.0)\nRequirement already satisfied: pathtools in /opt/conda/lib/python3.10/site-packages (from wandb) (0.1.2)\nRequirement already satisfied: setproctitle in /opt/conda/lib/python3.10/site-packages (from wandb) (1.3.2)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb) (68.0.0)\nRequirement already satisfied: appdirs>=1.4.3 in /opt/conda/lib/python3.10/site-packages (from wandb) (1.4.4)\nRequirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.20.3)\nRequirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.10)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2023.7.22)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.0)\nInstalling collected packages: wandb\n  Attempting uninstall: wandb\n    Found existing installation: wandb 0.15.9\n    Uninstalling wandb-0.15.9:\n      Successfully uninstalled wandb-0.15.9\nSuccessfully installed wandb-0.15.12\nRequirement already satisfied: boto3 in /opt/conda/lib/python3.10/site-packages (1.26.100)\nCollecting botocore<1.30.0,>=1.29.100 (from boto3)\n  Downloading botocore-1.29.165-py3-none-any.whl (11.0 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.0/11.0 MB\u001b[0m \u001b[31m79.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from boto3) (1.0.1)\nRequirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from boto3) (0.6.2)\nRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.10/site-packages (from botocore<1.30.0,>=1.29.100->boto3) (2.8.2)\nRequirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/conda/lib/python3.10/site-packages (from botocore<1.30.0,>=1.29.100->boto3) (1.26.15)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.30.0,>=1.29.100->boto3) (1.16.0)\nInstalling collected packages: botocore\n  Attempting uninstall: botocore\n    Found existing installation: botocore 1.31.17\n    Uninstalling botocore-1.31.17:\n      Successfully uninstalled botocore-1.31.17\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\naiobotocore 2.5.4 requires botocore<1.31.18,>=1.31.17, but you have botocore 1.29.165 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed botocore-1.29.165\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport re\nimport random\nimport pandas as pd\nimport numpy as np\nimport json\nimport math\nimport string\nimport uuid\n\n\n### Tensorflow Imports\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import roc_auc_score,confusion_matrix\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras.layers import Input, Conv1D, Add, Activation, Layer, \\\n                        UpSampling1D, Input, DepthwiseConv2D, Conv2D, \\\n                        BatchNormalization, ReLU, AvgPool2D, Flatten, Dense\nfrom tensorflow.keras.applications import MobileNet\n\n\n### External models\nimport efficientnet.tfkeras as efn\n\n\n### Matplotlib Imports\nimport matplotlib\nimport matplotlib.pyplot as plt\n\n\n### import wandb\nimport wandb\nfrom wandb.keras import WandbCallback\n","metadata":{"execution":{"iopub.status.busy":"2023-11-02T17:58:07.149538Z","iopub.execute_input":"2023-11-02T17:58:07.149855Z","iopub.status.idle":"2023-11-02T17:58:16.112427Z","shell.execute_reply.started":"2023-11-02T17:58:07.149827Z","shell.execute_reply":"2023-11-02T17:58:16.111499Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"import boto3\nimport os\nfrom botocore import UNSIGNED\nfrom botocore.config import Config\n\n\ndef download_files(bucket_name, s3_prefix, local_directory):\n    s3 = boto3.resource('s3', config=Config(signature_version=UNSIGNED))\n    bucket = s3.Bucket(bucket_name)\n\n    for obj in bucket.objects.filter(Prefix=s3_prefix):\n        local_file = os.path.join(local_directory, obj.key)\n\n        if not os.path.exists(os.path.dirname(local_file)):\n            os.makedirs(os.path.dirname(local_file))\n\n        bucket.download_file(obj.key, local_file)\n        print(f\"Downloaded {obj.key} to {local_file}\")\n\ndownload_files('mitdb128x128class4', 'train', '/content/input')","metadata":{"execution":{"iopub.status.busy":"2023-11-02T19:53:13.246604Z","iopub.execute_input":"2023-11-02T19:53:13.247031Z","iopub.status.idle":"2023-11-02T19:53:19.132849Z","shell.execute_reply.started":"2023-11-02T19:53:13.246996Z","shell.execute_reply":"2023-11-02T19:53:19.131872Z"},"trusted":true},"execution_count":41,"outputs":[{"name":"stdout","text":"Downloaded trainfile_class5_fold0_6377.tfrec to /content/input/trainfile_class5_fold0_6377.tfrec\nDownloaded trainfile_class5_fold1_6377.tfrec to /content/input/trainfile_class5_fold1_6377.tfrec\nDownloaded trainfile_class5_fold2_6377.tfrec to /content/input/trainfile_class5_fold2_6377.tfrec\nDownloaded trainfile_class5_fold3_6377.tfrec to /content/input/trainfile_class5_fold3_6377.tfrec\nDownloaded trainfile_class5_fold4_6377.tfrec to /content/input/trainfile_class5_fold4_6377.tfrec\nDownloaded trainfile_class5_fold5_6377.tfrec to /content/input/trainfile_class5_fold5_6377.tfrec\nDownloaded trainfile_class5_fold6_6377.tfrec to /content/input/trainfile_class5_fold6_6377.tfrec\nDownloaded trainfile_class5_fold7_6377.tfrec to /content/input/trainfile_class5_fold7_6377.tfrec\nDownloaded trainfile_class5_fold8_6377.tfrec to /content/input/trainfile_class5_fold8_6377.tfrec\nDownloaded trainfile_class5_fold9_6377.tfrec to /content/input/trainfile_class5_fold9_6377.tfrec\n","output_type":"stream"}]},{"cell_type":"code","source":"!ls -rf /content/input","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-11-02T19:53:23.572686Z","iopub.execute_input":"2023-11-02T19:53:23.573077Z","iopub.status.idle":"2023-11-02T19:53:24.590027Z","shell.execute_reply.started":"2023-11-02T19:53:23.573047Z","shell.execute_reply":"2023-11-02T19:53:24.588945Z"},"trusted":true},"execution_count":42,"outputs":[{"name":"stdout","text":"trainfile_class5_fold9_6377.tfrec  trainfile_class5_fold3_6377.tfrec\ntrainfile_class5_fold8_6377.tfrec  trainfile_class5_fold4_6377.tfrec\ntrainfile_class5_fold7_6377.tfrec  trainfile_class5_fold1_6377.tfrec\n..\t\t\t\t   trainfile_class5_fold2_6377.tfrec\n.\t\t\t\t   trainfile_class5_fold0_6377.tfrec\ntrainfile_class5_fold6_6377.tfrec  trainfile_class5_fold5_6377.tfrec\n","output_type":"stream"}]},{"cell_type":"code","source":"hparams = {\n    \"backbone\" : \"b0\",\n    \"batch_size\" : 32,\n    \"epochs\" : 40,\n    \"img_size\" : 128,\n    \"lr\" : 0.01,\n    \"optimizer\" : \"adam\",\n    \"seed\": 257,\n    \"notes\": \"SACC-s-dropout-changes\"\n}","metadata":{"execution":{"iopub.status.busy":"2023-11-02T17:58:23.612131Z","iopub.execute_input":"2023-11-02T17:58:23.613110Z","iopub.status.idle":"2023-11-02T17:58:23.618565Z","shell.execute_reply.started":"2023-11-02T17:58:23.613072Z","shell.execute_reply":"2023-11-02T17:58:23.617578Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"class WandBConfigurations():\n    def __init__(self, exp_name = \"ECG_BEAT_CLASSIFICATION_PAPER_COMMENTS\"):\n        self.EXPERIMENT_NAME = exp_name\n        os.environ[\"WANDB_API_KEY\"] = \"221507f411c2ddcc0c17238e115a12c528a482f6\"\n        wandb.login()\n\nWB = WandBConfigurations()","metadata":{"execution":{"iopub.status.busy":"2023-11-02T17:58:23.619723Z","iopub.execute_input":"2023-11-02T17:58:23.619999Z","iopub.status.idle":"2023-11-02T17:58:25.913846Z","shell.execute_reply.started":"2023-11-02T17:58:23.619976Z","shell.execute_reply":"2023-11-02T17:58:25.912898Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mshreya-srivas02\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"}]},{"cell_type":"code","source":" class Utils():\n    def __init__(self):\n        self.seed_everything()\n\n    def id_generator(size=6):\n        return str(uuid.uuid4())[:size]\n\n    def setupTPU(self):\n\n        try:\n            tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n            print('Running on TPU ', tpu.cluster_spec().as_dict())\n        except ValueError:\n            tpu = None\n\n        if tpu:\n            tf.config.experimental_connect_to_cluster(tpu)\n            tf.tpu.experimental.initialize_tpu_system(tpu)\n            strategy = tf.distribute.TPUStrategy(tpu)\n            STRATEGY = strategy\n            BATCH_SIZE = 8 * strategy.num_replicas_in_sync\n            # wandb.config.hardware = 'TPU'\n        else:\n            strategy = tf.distribute.get_strategy()\n            \n        return strategy\n\n    def seed_everything(self):\n        np.random.seed(hparams['seed'])\n        tf.random.set_seed(hparams['seed'])\n        random.seed(a=hparams['seed'])\n        os.environ['PYTHONHASHSEED'] = str(hparams['seed'])\n\nUTILS = Utils()\nSTRATEGY = UTILS.setupTPU()","metadata":{"execution":{"iopub.status.busy":"2023-11-02T17:58:25.915134Z","iopub.execute_input":"2023-11-02T17:58:25.915744Z","iopub.status.idle":"2023-11-02T17:58:25.931536Z","shell.execute_reply.started":"2023-11-02T17:58:25.915715Z","shell.execute_reply":"2023-11-02T17:58:25.930587Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"STRATEGY","metadata":{"execution":{"iopub.status.busy":"2023-11-02T17:58:25.932737Z","iopub.execute_input":"2023-11-02T17:58:25.933041Z","iopub.status.idle":"2023-11-02T17:58:25.964097Z","shell.execute_reply.started":"2023-11-02T17:58:25.933016Z","shell.execute_reply":"2023-11-02T17:58:25.962986Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"<tensorflow.python.distribute.distribute_lib._DefaultDistributionStrategy at 0x783a209d3f40>"},"metadata":{}}]},{"cell_type":"code","source":"class Config():\n    def __init__(self):\n        self.DO_VAL_SPLIT = True\n        self.TRAIN_FILES = sorted(tf.io.gfile.glob('/content/input/train*.tfrec'))[:-2]\n        self.TOTAL_TRAIN_IMG = 51016\n        self.TOTAL_VAL_IMG = 6377\n        self.TOTAL_TEST_IMG = 6377\n        self.BACKBONE = hparams['backbone']\n        self.IMG_TRAIN_SHAPE = [hparams[\"img_size\"],hparams[\"img_size\"]]\n        self.DO_FINETUNE = True\n        self.BATCH_SIZE = hparams[\"batch_size\"] # 16\n        self.EPOCHES = hparams[\"epochs\"]\n        self.SEED = hparams[\"seed\"]\n        self.LOSS = tf.keras.losses.CategoricalCrossentropy()\n        self.OPTIMIZER = self.get_optimizer()\n        self.ACCURACY = []\n        self.CALLBACKS = []\n        self.STRATEGY = STRATEGY\n        self.FOLDS = 8\n        self.USE_LR_SCHEDULER = True\n        self.FOLD_NUMBER = 0\n        self.FOLDS_DICT = {}\n\n        if self.USE_LR_SCHEDULER:\n            lrfn = self.get_cosine_schedule_with_warmup(lr=hparams['lr'])\n            lr_schedule = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=0)\n            self.CALLBACKS.append(lr_schedule)\n\n    def get_optimizer(self):\n        if hparams['optimizer'] == 'adam':\n            return tf.keras.optimizers.Adam(learning_rate=hparams[\"lr\"])\n        if hparams['optimizer'] == 'rmsprop':\n            return tf.keras.optimizers.RMSprop(learning_rate=hparams[\"lr\"])\n        if hparams['optimizer'] == 'adagrad':\n            return tf.keras.optimizers.Adagrad(learning_rate=hparams[\"lr\"])\n        if hparams['optimizer'] == 'adadelta':\n            return tf.keras.optimizers.Adadelta(learning_rate=hparams[\"lr\"])\n\n        return tf.keras.optimizers.Adam(learning_rate=hparams[\"lr\"])\n\n    def get_cosine_schedule_with_warmup(\n        self,\n        lr = 0.00004,\n        num_warmup_steps = 0,\n        num_cycles=0.5):\n        num_training_steps = self.EPOCHES\n        def lrfn(epoch):\n            if epoch < num_warmup_steps:\n                return (float(epoch) / float(max(5, num_warmup_steps))) * lr\n            progress = float(epoch - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))\n            return max(0.0, 0.5 * (1.0 + math.cos(math.pi * float(num_cycles) * 2.0 * progress))) * lr\n\n        return lrfn\n\n\nCONFIG = Config()","metadata":{"execution":{"iopub.status.busy":"2023-11-02T19:55:11.296621Z","iopub.execute_input":"2023-11-02T19:55:11.297471Z","iopub.status.idle":"2023-11-02T19:55:11.331557Z","shell.execute_reply.started":"2023-11-02T19:55:11.297437Z","shell.execute_reply":"2023-11-02T19:55:11.330480Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"CONFIG.BATCH_SIZE","metadata":{"execution":{"iopub.status.busy":"2023-11-02T19:55:12.261520Z","iopub.execute_input":"2023-11-02T19:55:12.261888Z","iopub.status.idle":"2023-11-02T19:55:12.270664Z","shell.execute_reply.started":"2023-11-02T19:55:12.261859Z","shell.execute_reply":"2023-11-02T19:55:12.269665Z"},"trusted":true},"execution_count":44,"outputs":[{"execution_count":44,"output_type":"execute_result","data":{"text/plain":"32"},"metadata":{}}]},{"cell_type":"code","source":"class Data():\n    def __init__(self):\n        self.LABELED_TFREC_FORMAT = {\n            \"image_id\": tf.io.FixedLenFeature([], tf.string),\n            \"image\": tf.io.FixedLenFeature([], tf.string),\n            'target5': tf.io.FixedLenFeature([], tf.int64),\n            'gender' : tf.io.FixedLenFeature([], tf.int64),\n            'age_interval' : tf.io.FixedLenFeature([], tf.int64),\n        }\n\n    def process_training_data(self, data_file):\n        data = tf.io.parse_single_example(data_file, self.LABELED_TFREC_FORMAT)\n        img = tf.image.decode_jpeg(data['image'], channels=1)\n        img = tf.cast(img, tf.float32) / 255.0\n        img = tf.reshape(img, [*CONFIG.IMG_TRAIN_SHAPE, 1])\n\n        age = tf.cast(data['age_interval'], tf.float32) / 10.0\n        sex = tf.cast(data['gender'], tf.float32) / 1.0\n        tab_data = [tf.cast(tfeat, dtype = tf.float32) for tfeat in [age, sex]]\n        tabular_data = tf.stack(tab_data)\n\n        target5 = tf.one_hot(data['target5'], depth=4)\n\n        return {'inp1' : img, 'inp2' : tabular_data}, {\"target5\" : target5 }\n\n    def process_testing_data(self, data_file):\n        data = tf.io.parse_single_example(data_file, self.LABELED_TFREC_FORMAT)\n        img = tf.image.decode_jpeg(data['image'], channels=1)\n        img = tf.cast(img, tf.float32) / 255.0\n        img = tf.reshape(img, [*CONFIG.IMG_TRAIN_SHAPE, 1])\n\n        age = tf.cast(data['age_interval'], tf.float32) / 10.0\n        sex = tf.cast(data['gender'], tf.float32) / 1.0\n        tab_data = [tf.cast(tfeat, dtype = tf.float32) for tfeat in [age, sex]]\n        tabular_data = tf.stack(tab_data)\n\n        target5 = tf.one_hot(data['target5'], depth=4)\n        image_id = data[\"image_id\"]\n\n        return {'inp1' : img, 'inp2' : tabular_data}, {\"target5\" : target5, \"image_id\":  data['image_id']}\n\n    def val_dataset(self):\n        ignore_order = tf.data.Options()\n        val_dataset = (\n            tf.data.TFRecordDataset(\n                CONFIG.FOLDS_DICT[\"fold_{}\".format(CONFIG.FOLD_NUMBER)][\"valfiles\"],\n                num_parallel_reads=tf.data.experimental.AUTOTUNE\n            ).with_options(\n                ignore_order\n            ).map(\n                self.process_training_data,\n                num_parallel_calls=tf.data.experimental.AUTOTUNE\n            ).batch(\n                CONFIG.BATCH_SIZE\n            ).prefetch(\n                tf.data.experimental.AUTOTUNE\n            )\n        )\n\n        return val_dataset\n\n    def train_dataset(self):\n        ignore_order = tf.data.Options()\n        ignore_order.experimental_deterministic = False\n        train_dataset = (\n            tf.data.TFRecordDataset(\n                CONFIG.FOLDS_DICT[\"fold_{}\".format(fold_number)][\"trainfiles\"],\n                num_parallel_reads=tf.data.experimental.AUTOTUNE\n            ).with_options(\n                ignore_order\n            ).map(\n                self.process_training_data,\n                num_parallel_calls=tf.data.experimental.AUTOTUNE\n            ).repeat(\n            ).shuffle(\n                CONFIG.SEED\n            ).batch(\n                CONFIG.BATCH_SIZE\n            ).prefetch(\n                tf.data.experimental.AUTOTUNE\n            )\n        )\n\n        return train_dataset\n\n    def test_dataset(self):\n        ignore_order = tf.data.Options()\n        TEST_FILES = sorted(tf.io.gfile.glob('/content/input/train*.tfrec'))[3]\n        test_dataset = (\n            tf.data.TFRecordDataset(\n                TEST_FILES,\n                num_parallel_reads=tf.data.experimental.AUTOTUNE\n            ).with_options(\n                ignore_order\n            ).map(\n                self.process_testing_data,\n                num_parallel_calls=tf.data.experimental.AUTOTUNE\n            ).batch(\n                CONFIG.BATCH_SIZE *  4\n            ).prefetch(\n                tf.data.experimental.AUTOTUNE\n            )\n        )\n        return test_dataset\n","metadata":{"execution":{"iopub.status.busy":"2023-11-02T19:55:13.008155Z","iopub.execute_input":"2023-11-02T19:55:13.008513Z","iopub.status.idle":"2023-11-02T19:55:13.031847Z","shell.execute_reply.started":"2023-11-02T19:55:13.008485Z","shell.execute_reply":"2023-11-02T19:55:13.030784Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"class SACCLayer(tf.keras.layers.Layer):\n    def __init__(self, output_dim, **kwargs):\n        self.output_dim = output_dim\n        super(SACCLayer, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        self.kernel_1 = self.add_weight(name='weights_ECG',\n                                        shape=(input_shape[0][-1], self.output_dim),\n                                        initializer='he_normal',\n                                        trainable=True)\n        self.kernel_2 = self.add_weight(name='weights_Patient_Metadata',\n                                        shape=(input_shape[1][-1], self.output_dim),\n                                        initializer='he_normal',\n                                        trainable=True)\n        self.attention_weights1 = self.add_weight(name='attention_weights_ECG',\n                                                 shape=(self.output_dim,),\n                                                 initializer='uniform',\n                                                 trainable=True)\n        self.attention_weights2 = self.add_weight(name='attention_weights_Patient_metadata',\n                                                 shape=(self.output_dim,),\n                                                 initializer='uniform',\n                                                 trainable=True)\n        self.dense_layer = tf.keras.layers.Dense(self.output_dim, activation='relu', name='cca_dense')\n        super(SACCLayer, self).build(input_shape)\n\n    def call(self, inputs):\n        proj_1 = K.dot(inputs[0], self.kernel_1)\n        proj_2 = K.dot(inputs[1], self.kernel_2)\n\n        # Apply non-linear transformation\n        proj_1 = tf.keras.activations.relu(proj_1)\n        proj_2 = tf.keras.activations.relu(proj_2)\n\n        # Attention mechanism\n        attention_scores1 = tf.nn.softmax(self.attention_weights1)\n        attention_scores2 = tf.nn.softmax(self.attention_weights2)\n        proj_1 = attention_scores1 * proj_1\n        proj_2 = attention_scores2 * proj_2\n\n        # Non-linear fusion\n        fused_representation = tf.keras.layers.concatenate([proj_1, proj_2])\n        fused_representation = self.dense_layer(fused_representation)\n\n        return fused_representation\n\n    def compute_output_shape(self, input_shape):\n        return (input_shape[0][0], self.output_dim)\n\n\ndef depthwise_separable_conv_with_residual_block(x, filters, stride):\n    # Depthwise Convolution\n    depthwise = DepthwiseConv2D((3, 3), strides=stride, padding='same')(x)\n    depthwise = BatchNormalization()(depthwise)\n    depthwise = ReLU()(depthwise)\n\n    # Pointwise Convolution\n    pointwise = Conv2D(filters, (1, 1), strides=(1, 1), padding='same')(depthwise)\n    pointwise = BatchNormalization()(pointwise)\n    pointwise = ReLU()(pointwise)\n\n\n    return pointwise\n\n\ndef DualPathwayModel(inp1):\n    # Initial Convolution Layers\n    conv1 = Conv2D(32, (3, 3), strides=(2, 2), padding='same')(inp1)\n    conv1 = BatchNormalization()(conv1)\n    conv1 = ReLU()(conv1)\n\n    conv2 = Conv2D(32, (5, 5), strides=(2, 2), padding='same')(inp1)\n    conv2 = BatchNormalization()(conv2)\n    conv2 = ReLU()(conv2)\n    \n\n    concatenated = tf.keras.layers.concatenate([conv1, conv2])\n\n    x = depthwise_separable_conv_with_residual_block(concatenated, 64, (1, 1))\n    x = depthwise_separable_conv_with_residual_block(x, 128, (2, 2))\n    x = tf.keras.layers.Dropout(0.2)(x)\n    x = depthwise_separable_conv_with_residual_block(x, 128, (2, 2))\n    x = depthwise_separable_conv_with_residual_block(x, 256, (2, 2))\n    x = tf.keras.layers.Dropout(0.2)(x)\n    x = depthwise_separable_conv_with_residual_block(x, 256, (2, 2))\n#     x = depthwise_separable_conv_with_residual_block(x, 256, (2, 2)) #<--- Can be removed\n\n    return x\n\n    \ndef dpm_sacc():\n    inp1  = tf.keras.layers.Input(shape = (*CONFIG.IMG_TRAIN_SHAPE, 1), name='inp1')\n    inp2  = tf.keras.layers.Input(shape = (2,), name='inp2')\n    x1 = DualPathwayModel(inp1)\n\n    x1 = tf.keras.layers.GlobalMaxPooling2D()(x1) # AVG is not good\n    x1 = tf.keras.layers.Dropout(0.2)(x1)\n\n    x2 = tf.keras.layers.Dense(8, name='metadata_feature_dense_1', activation='relu')(inp2)\n    x2 = tf.keras.layers.concatenate([x2, inp2])\n\n    x = SACCLayer(output_dim=256)([x1, x2])\n    \n    \n    x = tf.keras.layers.Dense(128, name='combine_feature_dense_1', activation='relu')(x)\n    x = tf.keras.layers.Dense(64, name='combine_feature_dense_2', activation='relu')(x)\n    x = tf.keras.layers.Dropout(0.1)(x)\n\n    output5 = tf.keras.layers.Dense(4, activation='softmax', name='target5')(x)\n\n    model = tf.keras.models.Model(inputs = [inp1, inp2], outputs = [output5])\n\n    return model","metadata":{"execution":{"iopub.status.busy":"2023-11-02T19:55:14.658479Z","iopub.execute_input":"2023-11-02T19:55:14.658855Z","iopub.status.idle":"2023-11-02T19:55:14.685378Z","shell.execute_reply.started":"2023-11-02T19:55:14.658817Z","shell.execute_reply":"2023-11-02T19:55:14.684376Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"dpm_sacc().summary()","metadata":{"execution":{"iopub.status.busy":"2023-11-02T19:55:19.625163Z","iopub.execute_input":"2023-11-02T19:55:19.626012Z","iopub.status.idle":"2023-11-02T19:55:20.150925Z","shell.execute_reply.started":"2023-11-02T19:55:19.625981Z","shell.execute_reply":"2023-11-02T19:55:20.149819Z"},"trusted":true},"execution_count":47,"outputs":[{"name":"stdout","text":"Model: \"model_14\"\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to                     \n==================================================================================================\n inp1 (InputLayer)              [(None, 128, 128, 1  0           []                               \n                                )]                                                                \n                                                                                                  \n conv2d_103 (Conv2D)            (None, 64, 64, 32)   320         ['inp1[0][0]']                   \n                                                                                                  \n conv2d_104 (Conv2D)            (None, 64, 64, 32)   832         ['inp1[0][0]']                   \n                                                                                                  \n batch_normalization_178 (Batch  (None, 64, 64, 32)  128         ['conv2d_103[0][0]']             \n Normalization)                                                                                   \n                                                                                                  \n batch_normalization_179 (Batch  (None, 64, 64, 32)  128         ['conv2d_104[0][0]']             \n Normalization)                                                                                   \n                                                                                                  \n re_lu_178 (ReLU)               (None, 64, 64, 32)   0           ['batch_normalization_178[0][0]']\n                                                                                                  \n re_lu_179 (ReLU)               (None, 64, 64, 32)   0           ['batch_normalization_179[0][0]']\n                                                                                                  \n concatenate_28 (Concatenate)   (None, 64, 64, 64)   0           ['re_lu_178[0][0]',              \n                                                                  're_lu_179[0][0]']              \n                                                                                                  \n depthwise_conv2d_75 (Depthwise  (None, 64, 64, 64)  640         ['concatenate_28[0][0]']         \n Conv2D)                                                                                          \n                                                                                                  \n batch_normalization_180 (Batch  (None, 64, 64, 64)  256         ['depthwise_conv2d_75[0][0]']    \n Normalization)                                                                                   \n                                                                                                  \n re_lu_180 (ReLU)               (None, 64, 64, 64)   0           ['batch_normalization_180[0][0]']\n                                                                                                  \n conv2d_105 (Conv2D)            (None, 64, 64, 64)   4160        ['re_lu_180[0][0]']              \n                                                                                                  \n batch_normalization_181 (Batch  (None, 64, 64, 64)  256         ['conv2d_105[0][0]']             \n Normalization)                                                                                   \n                                                                                                  \n re_lu_181 (ReLU)               (None, 64, 64, 64)   0           ['batch_normalization_181[0][0]']\n                                                                                                  \n depthwise_conv2d_76 (Depthwise  (None, 32, 32, 64)  640         ['re_lu_181[0][0]']              \n Conv2D)                                                                                          \n                                                                                                  \n batch_normalization_182 (Batch  (None, 32, 32, 64)  256         ['depthwise_conv2d_76[0][0]']    \n Normalization)                                                                                   \n                                                                                                  \n re_lu_182 (ReLU)               (None, 32, 32, 64)   0           ['batch_normalization_182[0][0]']\n                                                                                                  \n conv2d_106 (Conv2D)            (None, 32, 32, 128)  8320        ['re_lu_182[0][0]']              \n                                                                                                  \n batch_normalization_183 (Batch  (None, 32, 32, 128)  512        ['conv2d_106[0][0]']             \n Normalization)                                                                                   \n                                                                                                  \n re_lu_183 (ReLU)               (None, 32, 32, 128)  0           ['batch_normalization_183[0][0]']\n                                                                                                  \n dropout_56 (Dropout)           (None, 32, 32, 128)  0           ['re_lu_183[0][0]']              \n                                                                                                  \n depthwise_conv2d_77 (Depthwise  (None, 16, 16, 128)  1280       ['dropout_56[0][0]']             \n Conv2D)                                                                                          \n                                                                                                  \n batch_normalization_184 (Batch  (None, 16, 16, 128)  512        ['depthwise_conv2d_77[0][0]']    \n Normalization)                                                                                   \n                                                                                                  \n re_lu_184 (ReLU)               (None, 16, 16, 128)  0           ['batch_normalization_184[0][0]']\n                                                                                                  \n conv2d_107 (Conv2D)            (None, 16, 16, 128)  16512       ['re_lu_184[0][0]']              \n                                                                                                  \n batch_normalization_185 (Batch  (None, 16, 16, 128)  512        ['conv2d_107[0][0]']             \n Normalization)                                                                                   \n                                                                                                  \n re_lu_185 (ReLU)               (None, 16, 16, 128)  0           ['batch_normalization_185[0][0]']\n                                                                                                  \n depthwise_conv2d_78 (Depthwise  (None, 8, 8, 128)   1280        ['re_lu_185[0][0]']              \n Conv2D)                                                                                          \n                                                                                                  \n batch_normalization_186 (Batch  (None, 8, 8, 128)   512         ['depthwise_conv2d_78[0][0]']    \n Normalization)                                                                                   \n                                                                                                  \n re_lu_186 (ReLU)               (None, 8, 8, 128)    0           ['batch_normalization_186[0][0]']\n                                                                                                  \n conv2d_108 (Conv2D)            (None, 8, 8, 256)    33024       ['re_lu_186[0][0]']              \n                                                                                                  \n batch_normalization_187 (Batch  (None, 8, 8, 256)   1024        ['conv2d_108[0][0]']             \n Normalization)                                                                                   \n                                                                                                  \n re_lu_187 (ReLU)               (None, 8, 8, 256)    0           ['batch_normalization_187[0][0]']\n                                                                                                  \n dropout_57 (Dropout)           (None, 8, 8, 256)    0           ['re_lu_187[0][0]']              \n                                                                                                  \n depthwise_conv2d_79 (Depthwise  (None, 4, 4, 256)   2560        ['dropout_57[0][0]']             \n Conv2D)                                                                                          \n                                                                                                  \n batch_normalization_188 (Batch  (None, 4, 4, 256)   1024        ['depthwise_conv2d_79[0][0]']    \n Normalization)                                                                                   \n                                                                                                  \n re_lu_188 (ReLU)               (None, 4, 4, 256)    0           ['batch_normalization_188[0][0]']\n                                                                                                  \n conv2d_109 (Conv2D)            (None, 4, 4, 256)    65792       ['re_lu_188[0][0]']              \n                                                                                                  \n batch_normalization_189 (Batch  (None, 4, 4, 256)   1024        ['conv2d_109[0][0]']             \n Normalization)                                                                                   \n                                                                                                  \n re_lu_189 (ReLU)               (None, 4, 4, 256)    0           ['batch_normalization_189[0][0]']\n                                                                                                  \n inp2 (InputLayer)              [(None, 2)]          0           []                               \n                                                                                                  \n global_max_pooling2d_14 (Globa  (None, 256)         0           ['re_lu_189[0][0]']              \n lMaxPooling2D)                                                                                   \n                                                                                                  \n metadata_feature_dense_1 (Dens  (None, 8)           24          ['inp2[0][0]']                   \n e)                                                                                               \n                                                                                                  \n dropout_58 (Dropout)           (None, 256)          0           ['global_max_pooling2d_14[0][0]']\n                                                                                                  \n concatenate_29 (Concatenate)   (None, 10)           0           ['metadata_feature_dense_1[0][0]'\n                                                                 , 'inp2[0][0]']                  \n                                                                                                  \n sacc_layer_14 (SACCLayer)      (None, 256)          199936      ['dropout_58[0][0]',             \n                                                                  'concatenate_29[0][0]']         \n                                                                                                  \n combine_feature_dense_1 (Dense  (None, 128)         32896       ['sacc_layer_14[0][0]']          \n )                                                                                                \n                                                                                                  \n combine_feature_dense_2 (Dense  (None, 64)          8256        ['combine_feature_dense_1[0][0]']\n )                                                                                                \n                                                                                                  \n dropout_59 (Dropout)           (None, 64)           0           ['combine_feature_dense_2[0][0]']\n                                                                                                  \n target5 (Dense)                (None, 4)            260         ['dropout_59[0][0]']             \n                                                                                                  \n==================================================================================================\nTotal params: 382,876\nTrainable params: 379,804\nNon-trainable params: 3,072\n__________________________________________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"def fitengine(model, traindataset, valdataset = None, istraining = True):\n    model.compile(\n        optimizer   =  CONFIG.OPTIMIZER,\n        loss        =  CONFIG.LOSS,\n        metrics     =  CONFIG.ACCURACY\n    )\n\n    history = model.fit(\n                traindataset,\n                epochs            =   CONFIG.EPOCHES,\n                steps_per_epoch   =   CONFIG.TOTAL_TRAIN_IMG//CONFIG.BATCH_SIZE,\n                callbacks         =   CONFIG.CALLBACKS,\n                validation_data   =   valdataset,\n                validation_steps = (CONFIG.TOTAL_VAL_IMG)//(CONFIG.BATCH_SIZE) + 1,\n                verbose           =   1\n            )\n\n    return history\n\nskf = KFold(n_splits=CONFIG.FOLDS,shuffle=True,random_state=CONFIG.SEED)\nfor fold_number,(idxT,idxV) in enumerate(skf.split(np.arange(len(CONFIG.TRAIN_FILES)))):\n    CONFIG.FOLDS_DICT['fold_{}'.format(fold_number)] = {\n                                            \"trainfiles\" : [CONFIG.TRAIN_FILES[x] for x in idxT],\n                                            \"valfiles\"   : [CONFIG.TRAIN_FILES[x] for x in idxV]\n                                            }\n\nfold_number = CONFIG.FOLD_NUMBER\nprint(CONFIG.FOLDS_DICT['fold_{}'.format(fold_number)]['trainfiles'])\nprint(CONFIG.FOLDS_DICT['fold_{}'.format(fold_number)]['valfiles'])\n\nrun_ = wandb.init(\n    project= WB.EXPERIMENT_NAME,\n    reinit=True,\n    dir = \"/root\",\n    allow_val_change = True,\n    config = hparams\n)\n\nif CONFIG.STRATEGY is not None:\n    with CONFIG.STRATEGY.scope():\n        x2 = tf.keras.metrics.Precision(name='precision')\n        x3 = tf.keras.metrics.BinaryAccuracy(name='accuracy')\n        x4 = tf.keras.metrics.Recall(name='sensitivity')\n\n        CONFIG.ACCURACY.append(x2)\n        CONFIG.ACCURACY.append(x3)\n        CONFIG.ACCURACY.append(x4)\n\n        model = dpm_sacc()\n#         CONFIG.CALLBACKS.append(InLayerLossCallback())\nelse:\n    x2 = tf.keras.metrics.Precision(name='precision')\n    x3 = tf.keras.metrics.BinaryAccuracy(name='accuracy')\n    x4 = tf.keras.metrics.Recall(name='sensitivity')\n    x4 = tf.keras.metrics.Recall(name='sensitivity')\n\n    CONFIG.ACCURACY.append(x2)\n    CONFIG.ACCURACY.append(x3)\n    CONFIG.ACCURACY.append(x4)\n\n    model = dpm_sacc()\n\nCONFIG.CALLBACKS.append(tf.keras.callbacks.ModelCheckpoint(\n                                'model-%s.h5'%(fold_number), monitor='val_loss', verbose=1, save_best_only=True,\n                                save_weights_only=True, mode='min', save_freq='epoch'))\n\nCONFIG.CALLBACKS.append(WandbCallback(save_weights_only=True,\n                                            log_weights=True,\n                                            log_evaluation=True))\n\n\n\n\nDATA = Data()\n\nprint(\"##\"*30)\n\nhistory = fitengine(model, DATA.train_dataset(), valdataset=DATA.val_dataset()) #training model\n\nprint('##'*30)","metadata":{"execution":{"iopub.status.busy":"2023-11-02T19:55:24.049271Z","iopub.execute_input":"2023-11-02T19:55:24.049621Z","iopub.status.idle":"2023-11-02T20:41:25.731866Z","shell.execute_reply.started":"2023-11-02T19:55:24.049596Z","shell.execute_reply":"2023-11-02T20:41:25.730878Z"},"trusted":true},"execution_count":48,"outputs":[{"name":"stdout","text":"['/content/input/trainfile_class5_fold0_6377.tfrec', '/content/input/trainfile_class5_fold1_6377.tfrec', '/content/input/trainfile_class5_fold2_6377.tfrec', '/content/input/trainfile_class5_fold4_6377.tfrec', '/content/input/trainfile_class5_fold5_6377.tfrec', '/content/input/trainfile_class5_fold6_6377.tfrec', '/content/input/trainfile_class5_fold7_6377.tfrec']\n['/content/input/trainfile_class5_fold3_6377.tfrec']\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Finishing last run (ID:dihpiy1t) before initializing another..."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>▁▄▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇███████████████</td></tr><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss</td><td>█▅▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>███████▇▇▇▇▇▇▆▆▆▆▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>precision</td><td>▁▄▄▅▅▅▆▆▆▆▆▆▇▆▇▇▇▇▇▇▇▇▇▇▇███████████████</td></tr><tr><td>sensitivity</td><td>▁▄▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇████████████████</td></tr><tr><td>val_accuracy</td><td>▁▁▁▆▇▆▆██▁████▇█▆█▁▇▆███████████████████</td></tr><tr><td>val_loss</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_precision</td><td>▁▁▁▆▇▆▇██▁████▇█▆█▁▇▆███████████████████</td></tr><tr><td>val_sensitivity</td><td>▁▁▁▅▇▅▆██▁█▇█▇▇█▅█▁▇▆███████████████████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>0.99666</td></tr><tr><td>best_epoch</td><td>24</td></tr><tr><td>best_val_loss</td><td>0.10284</td></tr><tr><td>epoch</td><td>39</td></tr><tr><td>loss</td><td>0.02043</td></tr><tr><td>lr</td><td>2e-05</td></tr><tr><td>precision</td><td>0.99376</td></tr><tr><td>sensitivity</td><td>0.99286</td></tr><tr><td>val_accuracy</td><td>0.99029</td></tr><tr><td>val_loss</td><td>0.13786</td></tr><tr><td>val_precision</td><td>0.98079</td></tr><tr><td>val_sensitivity</td><td>0.98037</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">hardy-grass-207</strong> at: <a href='https://wandb.ai/shreya-srivas02/ECG_BEAT_CLASSIFICATION_PAPER_COMMENTS/runs/dihpiy1t' target=\"_blank\">https://wandb.ai/shreya-srivas02/ECG_BEAT_CLASSIFICATION_PAPER_COMMENTS/runs/dihpiy1t</a><br/>Synced 6 W&B file(s), 9 media file(s), 51 artifact file(s) and 1 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>/root/wandb/run-20231102_191947-dihpiy1t/logs</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Successfully finished last run (ID:dihpiy1t). Initializing new run:<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.15.12"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/root/wandb/run-20231102_195524-0arypq3p</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/shreya-srivas02/ECG_BEAT_CLASSIFICATION_PAPER_COMMENTS/runs/0arypq3p' target=\"_blank\">balmy-rain-209</a></strong> to <a href='https://wandb.ai/shreya-srivas02/ECG_BEAT_CLASSIFICATION_PAPER_COMMENTS' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/shreya-srivas02/ECG_BEAT_CLASSIFICATION_PAPER_COMMENTS' target=\"_blank\">https://wandb.ai/shreya-srivas02/ECG_BEAT_CLASSIFICATION_PAPER_COMMENTS</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/shreya-srivas02/ECG_BEAT_CLASSIFICATION_PAPER_COMMENTS/runs/0arypq3p' target=\"_blank\">https://wandb.ai/shreya-srivas02/ECG_BEAT_CLASSIFICATION_PAPER_COMMENTS/runs/0arypq3p</a>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m WandbCallback is unable to log validation data. When using a generator for validation_data, you must pass validation_steps\n","output_type":"stream"},{"name":"stdout","text":"############################################################\nEpoch 1/40\n","output_type":"stream"},{"name":"stderr","text":"2023-11-02 19:56:28.787994: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape inmodel_15/dropout_60/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n","output_type":"stream"},{"name":"stdout","text":"1593/1594 [============================>.] - ETA: 0s - loss: 0.2720 - precision: 0.9346 - accuracy: 0.9645 - sensitivity: 0.9225\nEpoch 1: val_loss improved from inf to 0.38771, saving model to model-0.h5\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/root/wandb/run-20231102_195524-0arypq3p/files/model-best)... Done. 0.0s\n","output_type":"stream"},{"name":"stdout","text":"1594/1594 [==============================] - 84s 46ms/step - loss: 0.2719 - precision: 0.9346 - accuracy: 0.9645 - sensitivity: 0.9225 - val_loss: 0.3877 - val_precision: 0.9099 - val_accuracy: 0.9492 - val_sensitivity: 0.8841 - lr: 0.0100\nEpoch 2/40\n1593/1594 [============================>.] - ETA: 0s - loss: 0.1808 - precision: 0.9552 - accuracy: 0.9759 - sensitivity: 0.9482\nEpoch 2: val_loss did not improve from 0.38771\n1594/1594 [==============================] - 65s 41ms/step - loss: 0.1807 - precision: 0.9552 - accuracy: 0.9759 - sensitivity: 0.9482 - val_loss: 2.4845 - val_precision: 0.3388 - val_accuracy: 0.6706 - val_sensitivity: 0.3339 - lr: 0.0100\nEpoch 3/40\n1594/1594 [==============================] - ETA: 0s - loss: 0.1424 - precision: 0.9632 - accuracy: 0.9806 - sensitivity: 0.9590\nEpoch 3: val_loss did not improve from 0.38771\n1594/1594 [==============================] - 65s 41ms/step - loss: 0.1424 - precision: 0.9632 - accuracy: 0.9806 - sensitivity: 0.9590 - val_loss: 1.4613 - val_precision: 0.5675 - val_accuracy: 0.7817 - val_sensitivity: 0.5332 - lr: 0.0099\nEpoch 4/40\n1593/1594 [============================>.] - ETA: 0s - loss: 0.1268 - precision: 0.9666 - accuracy: 0.9825 - sensitivity: 0.9632\nEpoch 4: val_loss improved from 0.38771 to 0.19115, saving model to model-0.h5\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/root/wandb/run-20231102_195524-0arypq3p/files/model-best)... Done. 0.0s\n","output_type":"stream"},{"name":"stdout","text":"1594/1594 [==============================] - 74s 46ms/step - loss: 0.1268 - precision: 0.9666 - accuracy: 0.9825 - sensitivity: 0.9633 - val_loss: 0.1911 - val_precision: 0.9462 - val_accuracy: 0.9726 - val_sensitivity: 0.9440 - lr: 0.0099\nEpoch 5/40\n1593/1594 [============================>.] - ETA: 0s - loss: 0.1167 - precision: 0.9695 - accuracy: 0.9839 - sensitivity: 0.9661\nEpoch 5: val_loss did not improve from 0.19115\n1594/1594 [==============================] - 65s 41ms/step - loss: 0.1166 - precision: 0.9695 - accuracy: 0.9839 - sensitivity: 0.9661 - val_loss: 0.3419 - val_precision: 0.8875 - val_accuracy: 0.9403 - val_sensitivity: 0.8717 - lr: 0.0098\nEpoch 6/40\n1594/1594 [==============================] - ETA: 0s - loss: 0.1084 - precision: 0.9712 - accuracy: 0.9848 - sensitivity: 0.9679\nEpoch 6: val_loss improved from 0.19115 to 0.11300, saving model to model-0.h5\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/root/wandb/run-20231102_195524-0arypq3p/files/model-best)... Done. 0.0s\n","output_type":"stream"},{"name":"stdout","text":"1594/1594 [==============================] - 73s 46ms/step - loss: 0.1084 - precision: 0.9712 - accuracy: 0.9848 - sensitivity: 0.9679 - val_loss: 0.1130 - val_precision: 0.9727 - val_accuracy: 0.9853 - val_sensitivity: 0.9683 - lr: 0.0096\nEpoch 7/40\n1594/1594 [==============================] - ETA: 0s - loss: 0.0987 - precision: 0.9746 - accuracy: 0.9867 - sensitivity: 0.9722\nEpoch 7: val_loss did not improve from 0.11300\n1594/1594 [==============================] - 66s 41ms/step - loss: 0.0987 - precision: 0.9746 - accuracy: 0.9867 - sensitivity: 0.9722 - val_loss: 0.4062 - val_precision: 0.8906 - val_accuracy: 0.9423 - val_sensitivity: 0.8771 - lr: 0.0095\nEpoch 8/40\n1594/1594 [==============================] - ETA: 0s - loss: 0.0948 - precision: 0.9750 - accuracy: 0.9869 - sensitivity: 0.9727\nEpoch 8: val_loss did not improve from 0.11300\n1594/1594 [==============================] - 66s 41ms/step - loss: 0.0948 - precision: 0.9750 - accuracy: 0.9869 - sensitivity: 0.9727 - val_loss: 0.1915 - val_precision: 0.9516 - val_accuracy: 0.9738 - val_sensitivity: 0.9431 - lr: 0.0093\nEpoch 9/40\n1593/1594 [============================>.] - ETA: 0s - loss: 0.0857 - precision: 0.9769 - accuracy: 0.9878 - sensitivity: 0.9741\nEpoch 9: val_loss did not improve from 0.11300\n1594/1594 [==============================] - 66s 41ms/step - loss: 0.0857 - precision: 0.9769 - accuracy: 0.9878 - sensitivity: 0.9741 - val_loss: 0.4203 - val_precision: 0.8779 - val_accuracy: 0.9342 - val_sensitivity: 0.8559 - lr: 0.0090\nEpoch 10/40\n1593/1594 [============================>.] - ETA: 0s - loss: 0.0826 - precision: 0.9787 - accuracy: 0.9886 - sensitivity: 0.9757\nEpoch 10: val_loss improved from 0.11300 to 0.10532, saving model to model-0.h5\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/root/wandb/run-20231102_195524-0arypq3p/files/model-best)... Done. 0.0s\n","output_type":"stream"},{"name":"stdout","text":"1594/1594 [==============================] - 72s 45ms/step - loss: 0.0825 - precision: 0.9787 - accuracy: 0.9886 - sensitivity: 0.9757 - val_loss: 0.1053 - val_precision: 0.9773 - val_accuracy: 0.9869 - val_sensitivity: 0.9700 - lr: 0.0088\nEpoch 11/40\n1593/1594 [============================>.] - ETA: 0s - loss: 0.0763 - precision: 0.9801 - accuracy: 0.9894 - sensitivity: 0.9775\nEpoch 11: val_loss improved from 0.10532 to 0.09489, saving model to model-0.h5\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/root/wandb/run-20231102_195524-0arypq3p/files/model-best)... Done. 0.0s\n","output_type":"stream"},{"name":"stdout","text":"1594/1594 [==============================] - 73s 46ms/step - loss: 0.0763 - precision: 0.9801 - accuracy: 0.9894 - sensitivity: 0.9775 - val_loss: 0.0949 - val_precision: 0.9776 - val_accuracy: 0.9875 - val_sensitivity: 0.9724 - lr: 0.0085\nEpoch 12/40\n1594/1594 [==============================] - ETA: 0s - loss: 0.0712 - precision: 0.9818 - accuracy: 0.9901 - sensitivity: 0.9785\nEpoch 12: val_loss did not improve from 0.09489\n1594/1594 [==============================] - 65s 41ms/step - loss: 0.0712 - precision: 0.9818 - accuracy: 0.9901 - sensitivity: 0.9785 - val_loss: 0.9132 - val_precision: 0.7384 - val_accuracy: 0.8648 - val_sensitivity: 0.7110 - lr: 0.0082\nEpoch 13/40\n1593/1594 [============================>.] - ETA: 0s - loss: 0.0694 - precision: 0.9828 - accuracy: 0.9907 - sensitivity: 0.9798\nEpoch 13: val_loss improved from 0.09489 to 0.09261, saving model to model-0.h5\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/root/wandb/run-20231102_195524-0arypq3p/files/model-best)... Done. 0.0s\n","output_type":"stream"},{"name":"stdout","text":"1594/1594 [==============================] - 73s 45ms/step - loss: 0.0694 - precision: 0.9828 - accuracy: 0.9907 - sensitivity: 0.9798 - val_loss: 0.0926 - val_precision: 0.9770 - val_accuracy: 0.9877 - val_sensitivity: 0.9738 - lr: 0.0079\nEpoch 14/40\n1594/1594 [==============================] - ETA: 0s - loss: 0.0665 - precision: 0.9823 - accuracy: 0.9907 - sensitivity: 0.9803\nEpoch 14: val_loss did not improve from 0.09261\n1594/1594 [==============================] - 65s 41ms/step - loss: 0.0665 - precision: 0.9823 - accuracy: 0.9907 - sensitivity: 0.9803 - val_loss: 0.1074 - val_precision: 0.9721 - val_accuracy: 0.9850 - val_sensitivity: 0.9679 - lr: 0.0076\nEpoch 15/40\n1594/1594 [==============================] - ETA: 0s - loss: 0.0639 - precision: 0.9832 - accuracy: 0.9908 - sensitivity: 0.9801\nEpoch 15: val_loss did not improve from 0.09261\n1594/1594 [==============================] - 65s 41ms/step - loss: 0.0639 - precision: 0.9832 - accuracy: 0.9908 - sensitivity: 0.9801 - val_loss: 0.1519 - val_precision: 0.9653 - val_accuracy: 0.9800 - val_sensitivity: 0.9542 - lr: 0.0073\nEpoch 16/40\n1593/1594 [============================>.] - ETA: 0s - loss: 0.0586 - precision: 0.9838 - accuracy: 0.9914 - sensitivity: 0.9817\nEpoch 16: val_loss did not improve from 0.09261\n1594/1594 [==============================] - 65s 41ms/step - loss: 0.0586 - precision: 0.9838 - accuracy: 0.9914 - sensitivity: 0.9817 - val_loss: 0.1676 - val_precision: 0.9577 - val_accuracy: 0.9759 - val_sensitivity: 0.9454 - lr: 0.0069\nEpoch 17/40\n1593/1594 [============================>.] - ETA: 0s - loss: 0.0549 - precision: 0.9850 - accuracy: 0.9921 - sensitivity: 0.9831\nEpoch 17: val_loss improved from 0.09261 to 0.07118, saving model to model-0.h5\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/root/wandb/run-20231102_195524-0arypq3p/files/model-best)... Done. 0.0s\n","output_type":"stream"},{"name":"stdout","text":"1594/1594 [==============================] - 73s 46ms/step - loss: 0.0549 - precision: 0.9850 - accuracy: 0.9921 - sensitivity: 0.9831 - val_loss: 0.0712 - val_precision: 0.9840 - val_accuracy: 0.9915 - val_sensitivity: 0.9821 - lr: 0.0065\nEpoch 18/40\n1594/1594 [==============================] - ETA: 0s - loss: 0.0502 - precision: 0.9870 - accuracy: 0.9931 - sensitivity: 0.9852\nEpoch 18: val_loss did not improve from 0.07118\n1594/1594 [==============================] - 65s 41ms/step - loss: 0.0502 - precision: 0.9870 - accuracy: 0.9931 - sensitivity: 0.9852 - val_loss: 0.1132 - val_precision: 0.9748 - val_accuracy: 0.9865 - val_sensitivity: 0.9711 - lr: 0.0062\nEpoch 19/40\n1594/1594 [==============================] - ETA: 0s - loss: 0.0495 - precision: 0.9868 - accuracy: 0.9928 - sensitivity: 0.9846\nEpoch 19: val_loss improved from 0.07118 to 0.06627, saving model to model-0.h5\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/root/wandb/run-20231102_195524-0arypq3p/files/model-best)... Done. 0.0s\n","output_type":"stream"},{"name":"stdout","text":"1594/1594 [==============================] - 73s 46ms/step - loss: 0.0495 - precision: 0.9868 - accuracy: 0.9928 - sensitivity: 0.9846 - val_loss: 0.0663 - val_precision: 0.9835 - val_accuracy: 0.9914 - val_sensitivity: 0.9821 - lr: 0.0058\nEpoch 20/40\n1593/1594 [============================>.] - ETA: 0s - loss: 0.0460 - precision: 0.9873 - accuracy: 0.9932 - sensitivity: 0.9856\nEpoch 20: val_loss improved from 0.06627 to 0.06456, saving model to model-0.h5\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/root/wandb/run-20231102_195524-0arypq3p/files/model-best)... Done. 0.0s\n","output_type":"stream"},{"name":"stdout","text":"1594/1594 [==============================] - 72s 45ms/step - loss: 0.0460 - precision: 0.9873 - accuracy: 0.9932 - sensitivity: 0.9856 - val_loss: 0.0646 - val_precision: 0.9832 - val_accuracy: 0.9906 - val_sensitivity: 0.9793 - lr: 0.0054\nEpoch 21/40\n1594/1594 [==============================] - ETA: 0s - loss: 0.0419 - precision: 0.9886 - accuracy: 0.9938 - sensitivity: 0.9864\nEpoch 21: val_loss did not improve from 0.06456\n1594/1594 [==============================] - 66s 41ms/step - loss: 0.0419 - precision: 0.9886 - accuracy: 0.9938 - sensitivity: 0.9864 - val_loss: 0.0672 - val_precision: 0.9826 - val_accuracy: 0.9909 - val_sensitivity: 0.9812 - lr: 0.0050\nEpoch 22/40\n1593/1594 [============================>.] - ETA: 0s - loss: 0.0401 - precision: 0.9894 - accuracy: 0.9943 - sensitivity: 0.9880\nEpoch 22: val_loss did not improve from 0.06456\n1594/1594 [==============================] - 65s 41ms/step - loss: 0.0400 - precision: 0.9894 - accuracy: 0.9943 - sensitivity: 0.9880 - val_loss: 0.1594 - val_precision: 0.9681 - val_accuracy: 0.9823 - val_sensitivity: 0.9610 - lr: 0.0046\nEpoch 23/40\n1593/1594 [============================>.] - ETA: 0s - loss: 0.0382 - precision: 0.9892 - accuracy: 0.9943 - sensitivity: 0.9881\nEpoch 23: val_loss did not improve from 0.06456\n1594/1594 [==============================] - 65s 41ms/step - loss: 0.0382 - precision: 0.9892 - accuracy: 0.9943 - sensitivity: 0.9881 - val_loss: 0.1429 - val_precision: 0.9755 - val_accuracy: 0.9857 - val_sensitivity: 0.9671 - lr: 0.0042\nEpoch 24/40\n1593/1594 [============================>.] - ETA: 0s - loss: 0.0358 - precision: 0.9900 - accuracy: 0.9947 - sensitivity: 0.9888\nEpoch 24: val_loss did not improve from 0.06456\n1594/1594 [==============================] - 65s 41ms/step - loss: 0.0358 - precision: 0.9900 - accuracy: 0.9947 - sensitivity: 0.9888 - val_loss: 0.0719 - val_precision: 0.9851 - val_accuracy: 0.9924 - val_sensitivity: 0.9843 - lr: 0.0038\nEpoch 25/40\n1594/1594 [==============================] - ETA: 0s - loss: 0.0334 - precision: 0.9906 - accuracy: 0.9950 - sensitivity: 0.9895\nEpoch 25: val_loss did not improve from 0.06456\n1594/1594 [==============================] - 65s 41ms/step - loss: 0.0334 - precision: 0.9906 - accuracy: 0.9950 - sensitivity: 0.9895 - val_loss: 0.1685 - val_precision: 0.9699 - val_accuracy: 0.9822 - val_sensitivity: 0.9584 - lr: 0.0035\nEpoch 26/40\n1594/1594 [==============================] - ETA: 0s - loss: 0.0319 - precision: 0.9910 - accuracy: 0.9953 - sensitivity: 0.9900\nEpoch 26: val_loss improved from 0.06456 to 0.06349, saving model to model-0.h5\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/root/wandb/run-20231102_195524-0arypq3p/files/model-best)... Done. 0.0s\n","output_type":"stream"},{"name":"stdout","text":"1594/1594 [==============================] - 73s 46ms/step - loss: 0.0319 - precision: 0.9910 - accuracy: 0.9953 - sensitivity: 0.9900 - val_loss: 0.0635 - val_precision: 0.9851 - val_accuracy: 0.9918 - val_sensitivity: 0.9821 - lr: 0.0031\nEpoch 27/40\n1593/1594 [============================>.] - ETA: 0s - loss: 0.0291 - precision: 0.9916 - accuracy: 0.9955 - sensitivity: 0.9904\nEpoch 27: val_loss did not improve from 0.06349\n1594/1594 [==============================] - 65s 41ms/step - loss: 0.0291 - precision: 0.9916 - accuracy: 0.9955 - sensitivity: 0.9904 - val_loss: 0.0785 - val_precision: 0.9860 - val_accuracy: 0.9927 - val_sensitivity: 0.9849 - lr: 0.0027\nEpoch 28/40\n1593/1594 [============================>.] - ETA: 0s - loss: 0.0276 - precision: 0.9924 - accuracy: 0.9959 - sensitivity: 0.9913\nEpoch 28: val_loss did not improve from 0.06349\n1594/1594 [==============================] - 65s 41ms/step - loss: 0.0276 - precision: 0.9924 - accuracy: 0.9959 - sensitivity: 0.9913 - val_loss: 1.0613 - val_precision: 0.8041 - val_accuracy: 0.8940 - val_sensitivity: 0.7616 - lr: 0.0024\nEpoch 29/40\n1593/1594 [============================>.] - ETA: 0s - loss: 0.0262 - precision: 0.9926 - accuracy: 0.9960 - sensitivity: 0.9914\nEpoch 29: val_loss did not improve from 0.06349\n1594/1594 [==============================] - 65s 41ms/step - loss: 0.0262 - precision: 0.9926 - accuracy: 0.9960 - sensitivity: 0.9914 - val_loss: 0.0706 - val_precision: 0.9860 - val_accuracy: 0.9927 - val_sensitivity: 0.9846 - lr: 0.0021\nEpoch 30/40\n1593/1594 [============================>.] - ETA: 0s - loss: 0.0254 - precision: 0.9926 - accuracy: 0.9960 - sensitivity: 0.9914\nEpoch 30: val_loss did not improve from 0.06349\n1594/1594 [==============================] - 65s 41ms/step - loss: 0.0254 - precision: 0.9926 - accuracy: 0.9960 - sensitivity: 0.9914 - val_loss: 0.0673 - val_precision: 0.9851 - val_accuracy: 0.9919 - val_sensitivity: 0.9826 - lr: 0.0018\nEpoch 31/40\n1593/1594 [============================>.] - ETA: 0s - loss: 0.0223 - precision: 0.9935 - accuracy: 0.9965 - sensitivity: 0.9925\nEpoch 31: val_loss did not improve from 0.06349\n1594/1594 [==============================] - 65s 41ms/step - loss: 0.0223 - precision: 0.9935 - accuracy: 0.9965 - sensitivity: 0.9925 - val_loss: 0.0779 - val_precision: 0.9859 - val_accuracy: 0.9926 - val_sensitivity: 0.9845 - lr: 0.0015\nEpoch 32/40\n1593/1594 [============================>.] - ETA: 0s - loss: 0.0223 - precision: 0.9936 - accuracy: 0.9965 - sensitivity: 0.9926\nEpoch 32: val_loss did not improve from 0.06349\n1594/1594 [==============================] - 65s 41ms/step - loss: 0.0223 - precision: 0.9936 - accuracy: 0.9965 - sensitivity: 0.9926 - val_loss: 0.0754 - val_precision: 0.9863 - val_accuracy: 0.9928 - val_sensitivity: 0.9849 - lr: 0.0012\nEpoch 33/40\n1594/1594 [==============================] - ETA: 0s - loss: 0.0208 - precision: 0.9937 - accuracy: 0.9966 - sensitivity: 0.9928\nEpoch 33: val_loss did not improve from 0.06349\n1594/1594 [==============================] - 65s 41ms/step - loss: 0.0208 - precision: 0.9937 - accuracy: 0.9966 - sensitivity: 0.9928 - val_loss: 0.0715 - val_precision: 0.9876 - val_accuracy: 0.9933 - val_sensitivity: 0.9857 - lr: 9.5492e-04\nEpoch 34/40\n1593/1594 [============================>.] - ETA: 0s - loss: 0.0199 - precision: 0.9940 - accuracy: 0.9968 - sensitivity: 0.9931\nEpoch 34: val_loss did not improve from 0.06349\n1594/1594 [==============================] - 65s 41ms/step - loss: 0.0199 - precision: 0.9940 - accuracy: 0.9968 - sensitivity: 0.9931 - val_loss: 0.0915 - val_precision: 0.9833 - val_accuracy: 0.9909 - val_sensitivity: 0.9802 - lr: 7.3680e-04\nEpoch 35/40\n1594/1594 [==============================] - ETA: 0s - loss: 0.0199 - precision: 0.9942 - accuracy: 0.9969 - sensitivity: 0.9934\nEpoch 35: val_loss did not improve from 0.06349\n1594/1594 [==============================] - 66s 41ms/step - loss: 0.0199 - precision: 0.9942 - accuracy: 0.9969 - sensitivity: 0.9934 - val_loss: 0.0752 - val_precision: 0.9879 - val_accuracy: 0.9933 - val_sensitivity: 0.9854 - lr: 5.4497e-04\nEpoch 36/40\n1593/1594 [============================>.] - ETA: 0s - loss: 0.0189 - precision: 0.9945 - accuracy: 0.9971 - sensitivity: 0.9937\nEpoch 36: val_loss did not improve from 0.06349\n1594/1594 [==============================] - 67s 42ms/step - loss: 0.0189 - precision: 0.9945 - accuracy: 0.9971 - sensitivity: 0.9937 - val_loss: 0.0723 - val_precision: 0.9879 - val_accuracy: 0.9935 - val_sensitivity: 0.9862 - lr: 3.8060e-04\nEpoch 37/40\n1593/1594 [============================>.] - ETA: 0s - loss: 0.0192 - precision: 0.9941 - accuracy: 0.9969 - sensitivity: 0.9934\nEpoch 37: val_loss did not improve from 0.06349\n1594/1594 [==============================] - 66s 41ms/step - loss: 0.0192 - precision: 0.9941 - accuracy: 0.9969 - sensitivity: 0.9934 - val_loss: 0.0725 - val_precision: 0.9870 - val_accuracy: 0.9931 - val_sensitivity: 0.9854 - lr: 2.4472e-04\nEpoch 38/40\n1594/1594 [==============================] - ETA: 0s - loss: 0.0165 - precision: 0.9950 - accuracy: 0.9974 - sensitivity: 0.9944\nEpoch 38: val_loss did not improve from 0.06349\n1594/1594 [==============================] - 66s 41ms/step - loss: 0.0165 - precision: 0.9950 - accuracy: 0.9974 - sensitivity: 0.9944 - val_loss: 0.0745 - val_precision: 0.9867 - val_accuracy: 0.9931 - val_sensitivity: 0.9856 - lr: 1.3815e-04\nEpoch 39/40\n1594/1594 [==============================] - ETA: 0s - loss: 0.0157 - precision: 0.9955 - accuracy: 0.9976 - sensitivity: 0.9947\nEpoch 39: val_loss did not improve from 0.06349\n1594/1594 [==============================] - 66s 42ms/step - loss: 0.0157 - precision: 0.9955 - accuracy: 0.9976 - sensitivity: 0.9947 - val_loss: 0.0754 - val_precision: 0.9865 - val_accuracy: 0.9930 - val_sensitivity: 0.9854 - lr: 6.1558e-05\nEpoch 40/40\n1593/1594 [============================>.] - ETA: 0s - loss: 0.0162 - precision: 0.9956 - accuracy: 0.9976 - sensitivity: 0.9949\nEpoch 40: val_loss did not improve from 0.06349\n1594/1594 [==============================] - 67s 42ms/step - loss: 0.0162 - precision: 0.9956 - accuracy: 0.9976 - sensitivity: 0.9949 - val_loss: 0.0757 - val_precision: 0.9868 - val_accuracy: 0.9931 - val_sensitivity: 0.9856 - lr: 1.5413e-05\n############################################################\n","output_type":"stream"}]},{"cell_type":"code","source":"model.save('model_last_epoch.h5')\nmodel_till_last_epoch = model\nSAVED_MODEL_LOC = \"model-0.h5\"\nmodel = dpm_sacc()\nmodel.load_weights(SAVED_MODEL_LOC)","metadata":{"execution":{"iopub.status.busy":"2023-11-02T20:41:25.735285Z","iopub.execute_input":"2023-11-02T20:41:25.735587Z","iopub.status.idle":"2023-11-02T20:41:26.371241Z","shell.execute_reply.started":"2023-11-02T20:41:25.735561Z","shell.execute_reply":"2023-11-02T20:41:26.370283Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"NAME = ['N', 'SEB', 'VEB', 'F']\nDATA = Data()\nfor model_type, trained_model in zip(['best_epoch', 'last_epoch'], [model, model_till_last_epoch]):\n    test_imgs = DATA.test_dataset().map(lambda data, ids: data)\n    img_labels_ds = DATA.test_dataset().map(lambda data, ids: ids).unbatch()\n\n    STEPS = (CONFIG.TOTAL_TEST_IMG)//(CONFIG.BATCH_SIZE*4) + 1\n\n    y_pred = trained_model.predict(test_imgs,steps = int(STEPS), verbose=1)\n    test_labels = next(iter(img_labels_ds.batch(int(CONFIG.TOTAL_TEST_IMG) + 1)))\n    y_true = test_labels[\"target5\"].numpy()\n    pd.DataFrame({\n            'image_id'  : test_labels[\"image_id\"].numpy(),\n            'actual'  : np.argmax(y_true, axis=1),\n            'predicted'      : np.argmax(y_pred, axis=1)\n            }).to_csv('prediction_{}.csv'.format(model_type), index=False)\n\n    df = pd.read_csv(\"prediction_{}.csv\".format(model_type))\n\n    run_.log({f\"{model_type}_pr\": wandb.plot.pr_curve(np.argmax(y_true, axis=1), y_pred, labels=NAME)})\n    run_.log({f\"{model_type}_roc\": wandb.plot.roc_curve(np.argmax(y_true, axis=1), y_pred, labels=NAME)})\n\n    cm = wandb.plot.confusion_matrix(\n                    y_true=np.argmax(y_true, axis=1),\n                    preds=np.argmax(y_pred, axis=1),\n                    class_names=NAME)\n\n    run_.log({f\"{model_type}_conf_mat\": cm})\n\n    harvest = confusion_matrix(df['actual'], df['predicted'])\n    fig, ax = plt.subplots(figsize=(8,8))\n    im = ax.imshow(harvest)\n    ax.set_xticks(np.arange(len(NAME)))\n    ax.set_yticks(np.arange(len(NAME)))\n    ax.set_xticklabels(NAME)\n    ax.set_yticklabels(NAME)\n\n    # Rotate the tick labels and set their alignment.\n    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n            rotation_mode=\"anchor\")\n\n    for i in range(len(NAME)):\n        for j in range(len(NAME)):\n            text = ax.text(j, i, harvest[i, j],\n                        ha=\"center\", va=\"center\", color=\"w\")\n\n    fig.tight_layout()\n    run_.log({f\"{model_type}_cm\": plt})\n\n    from sklearn.metrics import classification_report\n    target_names = NAME\n    x_ = classification_report(df['actual'], df['predicted'], target_names=target_names, digits=4)\n    x2 = classification_report(df['actual'], df['predicted'], target_names=target_names, digits=4, output_dict=True)\n    print(x_)\n    run_.log({f\"{model_type}_CR\": x2})\n\n\n## log more\nartifact = wandb.Artifact(\"Full_Logs\", type=\"logs\")\nartifact.add_dir(\"/kaggle/working/\")\nwandb.log_artifact(artifact)","metadata":{"execution":{"iopub.status.busy":"2023-11-02T20:41:26.372625Z","iopub.execute_input":"2023-11-02T20:41:26.372892Z","iopub.status.idle":"2023-11-02T20:41:32.625211Z","shell.execute_reply.started":"2023-11-02T20:41:26.372869Z","shell.execute_reply":"2023-11-02T20:41:32.624264Z"},"trusted":true},"execution_count":50,"outputs":[{"name":"stdout","text":"50/50 [==============================] - 1s 18ms/step\n              precision    recall  f1-score   support\n\n           N     0.9902    0.9936    0.9919      5296\n         SEB     0.9494    0.8502    0.8971       287\n         VEB     0.9579    0.9874    0.9725       715\n           F     0.9130    0.7975    0.8514        79\n\n    accuracy                         0.9840      6377\n   macro avg     0.9527    0.9072    0.9282      6377\nweighted avg     0.9838    0.9840    0.9837      6377\n\n50/50 [==============================] - 1s 16ms/step\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/kaggle/working)... Done. 0.0s\n","output_type":"stream"},{"name":"stdout","text":"              precision    recall  f1-score   support\n\n           N     0.9917    0.9938    0.9927      5296\n         SEB     0.9126    0.9094    0.9110       287\n         VEB     0.9776    0.9762    0.9769       715\n           F     0.9429    0.8354    0.8859        79\n\n    accuracy                         0.9860      6377\n   macro avg     0.9562    0.9287    0.9416      6377\nweighted avg     0.9860    0.9860    0.9860      6377\n\n","output_type":"stream"},{"execution_count":50,"output_type":"execute_result","data":{"text/plain":"<Artifact Full_Logs>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 800x800 with 1 Axes>","image/png":"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"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 800x800 with 1 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.python.framework.convert_to_constants import  convert_variables_to_constants_v2_as_graph\n\n\ndef get_flops(model):\n    concrete = tf.function(lambda inputs: model(inputs))\n    concrete_func = concrete.get_concrete_function(\n        [tf.TensorSpec([1, *inputs.shape[1:]]) for inputs in model.inputs])\n    frozen_func, graph_def = convert_variables_to_constants_v2_as_graph(concrete_func)\n    with tf.Graph().as_default() as graph:\n        tf.graph_util.import_graph_def(graph_def, name='')\n        run_meta = tf.compat.v1.RunMetadata()\n        opts = tf.compat.v1.profiler.ProfileOptionBuilder.float_operation()\n        flops = tf.compat.v1.profiler.profile(graph=graph, run_meta=run_meta, cmd=\"op\", options=opts)\n        return flops.total_float_ops\n\nmodel = dpm_sacc()\n\nprint(\"The FLOPs is:{}\".format(get_flops(model)) ,flush=True )\n\n","metadata":{"execution":{"iopub.status.busy":"2023-11-02T20:41:32.627293Z","iopub.execute_input":"2023-11-02T20:41:32.627617Z","iopub.status.idle":"2023-11-02T20:41:33.624981Z","shell.execute_reply.started":"2023-11-02T20:41:32.627591Z","shell.execute_reply":"2023-11-02T20:41:33.624025Z"},"trusted":true},"execution_count":51,"outputs":[{"name":"stdout","text":"\n=========================Options=============================\n-max_depth                  10000\n-min_bytes                  0\n-min_peak_bytes             0\n-min_residual_bytes         0\n-min_output_bytes           0\n-min_micros                 0\n-min_accelerator_micros     0\n-min_cpu_micros             0\n-min_params                 0\n-min_float_ops              1\n-min_occurrence             0\n-step                       -1\n-order_by                   float_ops\n-account_type_regexes       .*\n-start_name_regexes         .*\n-trim_name_regexes          \n-show_name_regexes          .*\n-hide_name_regexes          \n-account_displayed_op_only  true\n-select                     float_ops\n-output                     stdout:\n\n==================Model Analysis Report======================\n\nDoc:\nop: The nodes are operation kernel type, such as MatMul, Conv2D. Graph nodes belonging to the same type are aggregated together.\nflops: Number of float operations. Note: Please read the implementation for the math behind it.\n\nProfile:\nnode name | # float_ops\nConv2D                   73.92m float_ops (100.00%, 89.93%)\nDepthwiseConv2dNative    6.71m float_ops (10.07%, 8.16%)\nBiasAdd                  1.08m float_ops (1.90%, 1.32%)\nMatMul                   480.80k float_ops (0.59%, 0.58%)\nSoftmax                  2.58k float_ops (0.00%, 0.00%)\nMul                        512 float_ops (0.00%, 0.00%)\n\n======================End of Report==========================\nThe FLOPs is:82199552\n","output_type":"stream"}]},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2023-11-02T19:51:43.782194Z","iopub.execute_input":"2023-11-02T19:51:43.782571Z","iopub.status.idle":"2023-11-02T19:51:43.954100Z","shell.execute_reply.started":"2023-11-02T19:51:43.782538Z","shell.execute_reply":"2023-11-02T19:51:43.953189Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stdout","text":"Model: \"model_13\"\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to                     \n==================================================================================================\n inp1 (InputLayer)              [(None, 128, 128, 1  0           []                               \n                                )]                                                                \n                                                                                                  \n conv2d_96 (Conv2D)             (None, 64, 64, 32)   320         ['inp1[0][0]']                   \n                                                                                                  \n conv2d_97 (Conv2D)             (None, 64, 64, 32)   832         ['inp1[0][0]']                   \n                                                                                                  \n batch_normalization_166 (Batch  (None, 64, 64, 32)  128         ['conv2d_96[0][0]']              \n Normalization)                                                                                   \n                                                                                                  \n batch_normalization_167 (Batch  (None, 64, 64, 32)  128         ['conv2d_97[0][0]']              \n Normalization)                                                                                   \n                                                                                                  \n re_lu_166 (ReLU)               (None, 64, 64, 32)   0           ['batch_normalization_166[0][0]']\n                                                                                                  \n re_lu_167 (ReLU)               (None, 64, 64, 32)   0           ['batch_normalization_167[0][0]']\n                                                                                                  \n concatenate_26 (Concatenate)   (None, 64, 64, 64)   0           ['re_lu_166[0][0]',              \n                                                                  're_lu_167[0][0]']              \n                                                                                                  \n depthwise_conv2d_70 (Depthwise  (None, 64, 64, 64)  640         ['concatenate_26[0][0]']         \n Conv2D)                                                                                          \n                                                                                                  \n batch_normalization_168 (Batch  (None, 64, 64, 64)  256         ['depthwise_conv2d_70[0][0]']    \n Normalization)                                                                                   \n                                                                                                  \n re_lu_168 (ReLU)               (None, 64, 64, 64)   0           ['batch_normalization_168[0][0]']\n                                                                                                  \n conv2d_98 (Conv2D)             (None, 64, 64, 64)   4160        ['re_lu_168[0][0]']              \n                                                                                                  \n batch_normalization_169 (Batch  (None, 64, 64, 64)  256         ['conv2d_98[0][0]']              \n Normalization)                                                                                   \n                                                                                                  \n re_lu_169 (ReLU)               (None, 64, 64, 64)   0           ['batch_normalization_169[0][0]']\n                                                                                                  \n depthwise_conv2d_71 (Depthwise  (None, 32, 32, 64)  640         ['re_lu_169[0][0]']              \n Conv2D)                                                                                          \n                                                                                                  \n batch_normalization_170 (Batch  (None, 32, 32, 64)  256         ['depthwise_conv2d_71[0][0]']    \n Normalization)                                                                                   \n                                                                                                  \n re_lu_170 (ReLU)               (None, 32, 32, 64)   0           ['batch_normalization_170[0][0]']\n                                                                                                  \n conv2d_99 (Conv2D)             (None, 32, 32, 128)  8320        ['re_lu_170[0][0]']              \n                                                                                                  \n batch_normalization_171 (Batch  (None, 32, 32, 128)  512        ['conv2d_99[0][0]']              \n Normalization)                                                                                   \n                                                                                                  \n re_lu_171 (ReLU)               (None, 32, 32, 128)  0           ['batch_normalization_171[0][0]']\n                                                                                                  \n dropout_52 (Dropout)           (None, 32, 32, 128)  0           ['re_lu_171[0][0]']              \n                                                                                                  \n depthwise_conv2d_72 (Depthwise  (None, 16, 16, 128)  1280       ['dropout_52[0][0]']             \n Conv2D)                                                                                          \n                                                                                                  \n batch_normalization_172 (Batch  (None, 16, 16, 128)  512        ['depthwise_conv2d_72[0][0]']    \n Normalization)                                                                                   \n                                                                                                  \n re_lu_172 (ReLU)               (None, 16, 16, 128)  0           ['batch_normalization_172[0][0]']\n                                                                                                  \n conv2d_100 (Conv2D)            (None, 16, 16, 128)  16512       ['re_lu_172[0][0]']              \n                                                                                                  \n batch_normalization_173 (Batch  (None, 16, 16, 128)  512        ['conv2d_100[0][0]']             \n Normalization)                                                                                   \n                                                                                                  \n re_lu_173 (ReLU)               (None, 16, 16, 128)  0           ['batch_normalization_173[0][0]']\n                                                                                                  \n depthwise_conv2d_73 (Depthwise  (None, 8, 8, 128)   1280        ['re_lu_173[0][0]']              \n Conv2D)                                                                                          \n                                                                                                  \n batch_normalization_174 (Batch  (None, 8, 8, 128)   512         ['depthwise_conv2d_73[0][0]']    \n Normalization)                                                                                   \n                                                                                                  \n re_lu_174 (ReLU)               (None, 8, 8, 128)    0           ['batch_normalization_174[0][0]']\n                                                                                                  \n conv2d_101 (Conv2D)            (None, 8, 8, 256)    33024       ['re_lu_174[0][0]']              \n                                                                                                  \n batch_normalization_175 (Batch  (None, 8, 8, 256)   1024        ['conv2d_101[0][0]']             \n Normalization)                                                                                   \n                                                                                                  \n re_lu_175 (ReLU)               (None, 8, 8, 256)    0           ['batch_normalization_175[0][0]']\n                                                                                                  \n dropout_53 (Dropout)           (None, 8, 8, 256)    0           ['re_lu_175[0][0]']              \n                                                                                                  \n depthwise_conv2d_74 (Depthwise  (None, 4, 4, 256)   2560        ['dropout_53[0][0]']             \n Conv2D)                                                                                          \n                                                                                                  \n batch_normalization_176 (Batch  (None, 4, 4, 256)   1024        ['depthwise_conv2d_74[0][0]']    \n Normalization)                                                                                   \n                                                                                                  \n re_lu_176 (ReLU)               (None, 4, 4, 256)    0           ['batch_normalization_176[0][0]']\n                                                                                                  \n conv2d_102 (Conv2D)            (None, 4, 4, 256)    65792       ['re_lu_176[0][0]']              \n                                                                                                  \n batch_normalization_177 (Batch  (None, 4, 4, 256)   1024        ['conv2d_102[0][0]']             \n Normalization)                                                                                   \n                                                                                                  \n re_lu_177 (ReLU)               (None, 4, 4, 256)    0           ['batch_normalization_177[0][0]']\n                                                                                                  \n inp2 (InputLayer)              [(None, 2)]          0           []                               \n                                                                                                  \n global_max_pooling2d_13 (Globa  (None, 256)         0           ['re_lu_177[0][0]']              \n lMaxPooling2D)                                                                                   \n                                                                                                  \n metadata_feature_dense_1 (Dens  (None, 8)           24          ['inp2[0][0]']                   \n e)                                                                                               \n                                                                                                  \n dropout_54 (Dropout)           (None, 256)          0           ['global_max_pooling2d_13[0][0]']\n                                                                                                  \n concatenate_27 (Concatenate)   (None, 10)           0           ['metadata_feature_dense_1[0][0]'\n                                                                 , 'inp2[0][0]']                  \n                                                                                                  \n sacc_layer_13 (SACCLayer)      (None, 256)          199936      ['dropout_54[0][0]',             \n                                                                  'concatenate_27[0][0]']         \n                                                                                                  \n combine_feature_dense_1 (Dense  (None, 128)         32896       ['sacc_layer_13[0][0]']          \n )                                                                                                \n                                                                                                  \n combine_feature_dense_2 (Dense  (None, 64)          8256        ['combine_feature_dense_1[0][0]']\n )                                                                                                \n                                                                                                  \n dropout_55 (Dropout)           (None, 64)           0           ['combine_feature_dense_2[0][0]']\n                                                                                                  \n target5 (Dense)                (None, 4)            260         ['dropout_55[0][0]']             \n                                                                                                  \n==================================================================================================\nTotal params: 382,876\nTrainable params: 379,804\nNon-trainable params: 3,072\n__________________________________________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"tf.keras.utils.plot_model(\n    dpm_sacc(),\n    to_file='model.png',\n    show_shapes=True,\n    show_dtype=False,\n    show_layer_names=False,\n    rankdir='TB',\n    expand_nested=False,\n    dpi=96,\n    layer_range=None,\n    show_layer_activations=True,\n    show_trainable=True\n)\n\nrun_.finish()","metadata":{"execution":{"iopub.status.busy":"2023-11-02T19:17:34.598171Z","iopub.execute_input":"2023-11-02T19:17:34.598540Z","iopub.status.idle":"2023-11-02T19:17:44.785309Z","shell.execute_reply.started":"2023-11-02T19:17:34.598507Z","shell.execute_reply":"2023-11-02T19:17:44.784594Z"},"trusted":true},"execution_count":31,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>▁▄▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇████████████████</td></tr><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss</td><td>█▅▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>███████▇▇▇▇▇▇▆▆▆▆▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>precision</td><td>▁▄▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇██████████████</td></tr><tr><td>sensitivity</td><td>▁▄▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇█████████████████</td></tr><tr><td>val_accuracy</td><td>▂█▁▆▂▇▁▇▁██▆▅██▃▇▁▇▁▁█▂██▁███▂██████████</td></tr><tr><td>val_loss</td><td>▂▁▃▁▂▁▄▁▆▁▁▁▁▁▁▃▁█▁▇▆▁▅▁▁▆▁▁▁▃▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_precision</td><td>▂█▁▇▂▇▁▇▁██▇▅██▃▇▁▇▁▁█▂██▁███▂██████████</td></tr><tr><td>val_sensitivity</td><td>▁█▁▅▂▇▁▇▁██▆▄██▂▆▁▇▁▁█▂██▁███▂██████████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>0.99373</td></tr><tr><td>best_epoch</td><td>33</td></tr><tr><td>best_val_loss</td><td>0.09012</td></tr><tr><td>epoch</td><td>39</td></tr><tr><td>loss</td><td>0.03994</td></tr><tr><td>lr</td><td>2e-05</td></tr><tr><td>precision</td><td>0.98805</td></tr><tr><td>sensitivity</td><td>0.98686</td></tr><tr><td>val_accuracy</td><td>0.98878</td></tr><tr><td>val_loss</td><td>0.0957</td></tr><tr><td>val_precision</td><td>0.97818</td></tr><tr><td>val_sensitivity</td><td>0.97691</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">neat-spaceship-206</strong> at: <a href='https://wandb.ai/shreya-srivas02/ECG_BEAT_CLASSIFICATION_PAPER_COMMENTS/runs/srpxd3lg' target=\"_blank\">https://wandb.ai/shreya-srivas02/ECG_BEAT_CLASSIFICATION_PAPER_COMMENTS/runs/srpxd3lg</a><br/>Synced 6 W&B file(s), 9 media file(s), 61 artifact file(s) and 1 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>/root/wandb/run-20231102_184635-srpxd3lg/logs</code>"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}